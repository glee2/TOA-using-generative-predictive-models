{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "sys.path.append(\"/share/tml_package\")\n",
    "from tml import utils\n",
    "from scipy import io\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data import TechDataset, CVSampler\n",
    "from model import Encoder_SEQ, Decoder_SEQ, SEQ2SEQ\n",
    "from train_utils import run_epoch, EarlyStopping, perf_eval\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_SOS = '<SOS>'\n",
    "TOKEN_EOS = '<EOS>'\n",
    "TOKEN_PAD = '<PAD>'\n",
    "tokens = [TOKEN_SOS, TOKEN_EOS, TOKEN_PAD]\n",
    "regex = re.compile(\"[0-9a-zA-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home2/glee/Tech_Gen/data/\"\n",
    "rawdata = pd.read_csv(os.path.join(data_root, \"collection_final.csv\"))\n",
    "rawdata_dropna = rawdata.dropna(axis=0, subset=['main ipc', 'sub ipc'])[['number','main ipc', 'sub ipc']]\n",
    "cols_year = ['<1976']+list(np.arange(1976,2018).astype(str))\n",
    "n_TC = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipc, num = np.unique(rawdata_dropna['main ipc'].apply(lambda x: x.split(' ')[0]), return_counts=True)\n",
    "ipc_vocab_size = pd.concat([pd.Series(ipc[np.argsort(num)[::-1]]), pd.Series(num[np.argsort(num)[::-1]])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A61K</td>\n",
       "      <td>44658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C07K</td>\n",
       "      <td>7686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01N</td>\n",
       "      <td>6265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C12N</td>\n",
       "      <td>2174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A61L</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A61F</td>\n",
       "      <td>1841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A23L</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>G01N</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A23K</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C07D</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C07C</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A61Q</td>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C07H</td>\n",
       "      <td>573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C12Q</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C12P</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A61B</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>C11D</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A23G</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A01K</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C08G</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1\n",
       "0   A61K  44658\n",
       "1   C07K   7686\n",
       "2   A01N   6265\n",
       "3   C12N   2174\n",
       "4   A61L   2008\n",
       "5   A61F   1841\n",
       "6   A23L   1075\n",
       "7   G01N    820\n",
       "8   A23K    767\n",
       "9   C07D    692\n",
       "10  C07C    670\n",
       "11  A61Q    585\n",
       "12  C07H    573\n",
       "13  C12Q    531\n",
       "14  C12P    472\n",
       "15  A61B    468\n",
       "16  C11D    445\n",
       "17  A23G    406\n",
       "18  A01K    324\n",
       "19  C08G    292"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipc_vocab_size.iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ipc = \"A61K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ipcs = [x for x in pd.unique(rawdata_dropna['main ipc']) if target_ipc in x]\n",
    "rawdata_ipc = rawdata_dropna.loc[rawdata_dropna['main ipc'].isin(main_ipcs)]\n",
    "data = rawdata_ipc[['number']].copy(deep=True)\n",
    "data['main_ipc'] = rawdata_ipc['main ipc'].apply(lambda x: regex.findall(x)[0])\n",
    "data['sub_ipc'] = rawdata_ipc['sub ipc'].apply(lambda x: [regex.findall(xx)[0] for xx in x.split(';')])\n",
    "\n",
    "rawdata_tc = rawdata.loc[rawdata_ipc.index][['year']+cols_year]\n",
    "data['TC'+str(n_TC)] = rawdata_tc.apply(lambda x: x[np.arange(x['year']+1 if x['year']<2017 else 2017, x['year']+n_TC+1 if x['year']+n_TC<2018 else 2018).astype(str)].sum(), axis=1)\n",
    "\n",
    "data = data.set_index('number')\n",
    "# main_ipcs = [regex.findall(x)[0] for x in main_ipcs]\n",
    "main_ipcs = [target_ipc]\n",
    "sub_ipcs = list(np.unique(np.concatenate(list(data['sub_ipc'].values))))\n",
    "all_ipcs = list(np.union1d(main_ipcs, sub_ipcs))\n",
    "seq_len = data['sub_ipc'].apply(lambda x: len(x)).max() + 3\n",
    "\n",
    "vocab_w2i = {all_ipcs[i]: i for i in range(len(all_ipcs))}\n",
    "vocab_w2i.update({tokens[i]: len(all_ipcs)+i for i in range(len(tokens))})\n",
    "vocab_i2w = {i: all_ipcs[i] for i in range(len(all_ipcs))}\n",
    "vocab_i2w.update({len(all_ipcs)+i: tokens[i] for i in range(len(tokens))})\n",
    "vocab_size = len(vocab_w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(index=data.index)\n",
    "X_df['main'] = data['main_ipc'].apply(lambda x: vocab_w2i[x])\n",
    "X_df['sub'] = data['sub_ipc'].apply(lambda x: [vocab_w2i[xx] for xx in x])\n",
    "main_sub_combined = X_df.apply(lambda x: [x['main']]+x['sub'], axis=1)\n",
    "X_df['seq'] = main_sub_combined.apply(lambda x: np.concatenate([[vocab_w2i['<SOS>']]+x+[vocab_w2i['<EOS>']], np.zeros(seq_len-(len(x)+2))+vocab_w2i['<PAD>']]).astype(int))\n",
    "\n",
    "# xaxis = np.concatenate([np.tile([i], X_df['sub'].apply(lambda x: len(x)).values[i]) for i in range(len(X_df))])\n",
    "# X = np.zeros((len(self.data), len(self.all_ipcs))) # (#samples, #ipcs)\n",
    "# X[tuple(np.arange(len(X_df))), tuple(X_df['main'].values)] += 10\n",
    "# X[tuple(xaxis), tuple(np.concatenate(X_df['sub'].values))] += 1\n",
    "\n",
    "# X = np.zeros((len(data), seq_len, len(all_ipcs)+len(tokens)))\n",
    "# X[tuple(np.sort(np.tile(np.arange(len(X_df)), seq_len))), tuple(np.tile(np.arange(seq_len), len(X_df))), tuple(np.concatenate(X_df['seq'].values))] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_ipc</th>\n",
       "      <th>sub_ipc</th>\n",
       "      <th>TC3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9040668</th>\n",
       "      <td>A61K39/395</td>\n",
       "      <td>[C07K19/00, C07K14/765, C12P21/08, C07K16/46, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9040626</th>\n",
       "      <td>A61K8/02</td>\n",
       "      <td>[A61K9/70, B82Y5/00]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9040579</th>\n",
       "      <td>A61K31/216</td>\n",
       "      <td>[A61K31/122]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9040103</th>\n",
       "      <td>A61K36/00</td>\n",
       "      <td>[A61K36/53, A61K31/045]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9040102</th>\n",
       "      <td>A61K36/539</td>\n",
       "      <td>[A61K36/48, A61K36/00, A61K36/31]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3932610</th>\n",
       "      <td>A61K8/72</td>\n",
       "      <td>[A61K8/60, A61K8/81, A61K8/46, A61K8/92, A61K8...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3932607</th>\n",
       "      <td>A61K33/16</td>\n",
       "      <td>[A61K8/43, A61K8/44, A61K8/30, A61Q11/00]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3932603</th>\n",
       "      <td>A61K8/64</td>\n",
       "      <td>[A61K8/37, A61K8/46, A61K8/44, A61K8/96, A61K8...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3931398</th>\n",
       "      <td>A61K31/715</td>\n",
       "      <td>[A61K8/66, A61K38/43, A61K38/45, A61K8/30, A61...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3931396</th>\n",
       "      <td>A61K51/02</td>\n",
       "      <td>[A61K51/04]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44658 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           main_ipc                                            sub_ipc  TC3\n",
       "number                                                                     \n",
       "9040668  A61K39/395  [C07K19/00, C07K14/765, C12P21/08, C07K16/46, ...    0\n",
       "9040626    A61K8/02                               [A61K9/70, B82Y5/00]    0\n",
       "9040579  A61K31/216                                       [A61K31/122]    0\n",
       "9040103   A61K36/00                            [A61K36/53, A61K31/045]    0\n",
       "9040102  A61K36/539                  [A61K36/48, A61K36/00, A61K36/31]    0\n",
       "...             ...                                                ...  ...\n",
       "3932610    A61K8/72  [A61K8/60, A61K8/81, A61K8/46, A61K8/92, A61K8...    2\n",
       "3932607   A61K33/16          [A61K8/43, A61K8/44, A61K8/30, A61Q11/00]    0\n",
       "3932603    A61K8/64  [A61K8/37, A61K8/46, A61K8/44, A61K8/96, A61K8...    0\n",
       "3931398  A61K31/715  [A61K8/66, A61K38/43, A61K38/45, A61K8/30, A61...    0\n",
       "3931396   A61K51/02                                        [A61K51/04]    2\n",
       "\n",
       "[44658 rows x 3 columns]"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORM one-by-one\n",
      "29.3840069770813 sec Elapsed\n",
      "TRANSFORM as a whole\n",
      "28.631096363067627 sec Elapsed\n"
     ]
    }
   ],
   "source": [
    "print(\"TRANSFORM one-by-one\")\n",
    "tstart = time.time()\n",
    "tech_dataset = TechDataset(device=device, data_dir=data_root, do_transform=True, params={'target_ipc': 'A61K'})\n",
    "data_loader = DataLoader(tech_dataset, batch_size=batch_size)\n",
    "tend = time.time()\n",
    "print(f\"{tend-tstart} sec Elapsed\")\n",
    "\n",
    "print(\"TRANSFORM as a whole\")\n",
    "tstart = time.time()\n",
    "tech_dataset = TechDataset(device=device, data_dir=data_root, do_transform=False, params={'target_ipc': 'A61K'})\n",
    "data_loader = DataLoader(tech_dataset, batch_size=batch_size)\n",
    "tend = time.time()\n",
    "print(f\"{tend-tstart} sec Elapsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0036962032318115234 sec Elapsed\n"
     ]
    }
   ],
   "source": [
    "tstart = time.time()\n",
    "xx, yy = next(iter(data_loader))\n",
    "tend = time.time()\n",
    "print(f\"{tend-tstart} sec Elapsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder_SEQ(embedding_dim=128, vocab_size=vocab_size, hidden_dim=32, n_layers=1, device=device, padding_idx=tech_dataset.vocab_w2i['<PAD>'])\n",
    "enc = enc.to(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0181, -0.0901,  0.1597,  ...,  0.1031, -0.0934, -0.1351],\n",
       "         [-0.0181, -0.0901,  0.1597,  ...,  0.1031, -0.0934, -0.1351],\n",
       "         [-0.0181, -0.0901,  0.1597,  ...,  0.1031, -0.0934, -0.1351],\n",
       "         ...,\n",
       "         [-0.0181, -0.0901,  0.1597,  ...,  0.1031, -0.0934, -0.1351],\n",
       "         [-0.0181, -0.0901,  0.1597,  ...,  0.1031, -0.0934, -0.1351],\n",
       "         [-0.0181, -0.0901,  0.1597,  ...,  0.1031, -0.0934, -0.1351]]],\n",
       "       grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = Decoder_SEQ(embedding_dim=128, vocab_size=vocab_size, hidden_dim=32, n_layers=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_input = torch.from_numpy(np.tile(vocab_w2i[TOKEN_SOS], 32)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.initHidden(len(next_input)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "o,h = dec(next_input, hidden=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6032])"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = SEQ2SEQ(device=device, dataset=tech_dataset, enc=enc, dec=dec, max_len=tech_dataset.seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, zz = seq2seq(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([168, 32, 6032])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A01C1/06', 'A23K1/14', 'A61B5/055', 'A61K31/06', 'A61K31/133',\n",
       "       'A61K31/53', 'A61K31/713', 'A61K35/64', 'A61K36/63', 'A61K39/085',\n",
       "       'A61K47/40', 'A61K8/44', 'A61M5/20', 'A61P15/00', 'B01J19/10',\n",
       "       'B01J39/14', 'B05D7/00', 'B32B5/24', 'C01B15/037', 'C01B21/24',\n",
       "       'C03C3/15', 'C07B39/00', 'C07B59/00', 'C07C229/24', 'C07C239/00',\n",
       "       'C07C25/00', 'C07C275/10', 'C07C275/40', 'C07C43/315', 'C07C59/72',\n",
       "       'C07C69/28', 'C07D207/27', 'C07D241/24', 'C07D253/00',\n",
       "       'C07D261/20', 'C07D277/34', 'C07D295/32', 'C07D305/08',\n",
       "       'C07D487/18', 'C07F9/44', 'C07K1/113', 'C07K14/555', 'C08F2/50',\n",
       "       'C08F224/00', 'C08G65/332', 'C08G77/20', 'C08G77/38', 'C08G77/458',\n",
       "       'C08J3/03', 'C09C1/62', 'C09J133/02', 'C09K15/24', 'C12N1/20',\n",
       "       'C12N5/0775', 'C12P17/12', 'C13K5/00', 'C40B30/04', 'G01N21/80',\n",
       "       'G01N33/573'], dtype='<U10')"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([tech_dataset.vocab_i2w[i] for i in outputs[:,0,:].argmax(1).detach().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.7048, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(outputs.transpose(0,1).transpose(1,2), xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 168])"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues = xx.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(seq2seq.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_losses = []\n",
    "\n",
    "outputs, z = seq2seq(xx) # outputs shape: (seq_len, batch_size, vocab_size)\n",
    "preds = outputs.transpose(0,1).transpose(1,2) # preds shape: (batch_size, vocab_size, seq_len), regard seq_len as additional dimension\n",
    "trues = xx.clone()\n",
    "loss = loss_fn(preds, trues)\n",
    "batch_losses.append(loss.item())\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# if batch % 10 == 0 or batch == len(dataloader)-1:\n",
    "#     loss, current = loss.item(), batch*len(X)\n",
    "#     if batch == len(dataloader)-1:\n",
    "#         current = size\n",
    "#     print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\", end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------\n",
      "loss: 7.744429 [ 4800/44658]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19295/2158694661.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {ep+1}\\n\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Tech_Gen/src/train_utils.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(dataloader, model, loss_fn, mode, optimizer)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DL/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DL/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=10, verbose=True, path=\"../models/ES_checkpoint.ckpt\")\n",
    "for ep in range(max_epochs):\n",
    "    print(f\"Epoch {ep+1}\\n\"+str(\"-\"*30))\n",
    "    train_loss = run_epoch(data_loader, seq2seq, loss_fn, mode='train', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = CVSampler(tech_dataset, n_folds=1, test_ratio=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_idx = sampler.get_idx_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Samples\n",
      "Train: 25008, Validation: 6252, Test: 13398\n"
     ]
    }
   ],
   "source": [
    "print(f\"#Samples\\nTrain: {len(cv_idx[0]['train'])}, Validation: {len(cv_idx[0]['val'])}, Test: {len(cv_idx[0]['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data\n",
    "importlib.reload(data)\n",
    "from data import TechDataset, CVSampler\n",
    "\n",
    "import model\n",
    "importlib.reload(model)\n",
    "from model import Encoder_SEQ, Decoder_SEQ, SEQ2SEQ\n",
    "\n",
    "import train_utils\n",
    "importlib.reload(train_utils)\n",
    "from train_utils import run_epoch, EarlyStopping, perf_eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
