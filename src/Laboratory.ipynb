{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T06:18:01.860009Z",
     "start_time": "2023-02-07T06:18:01.843259Z"
    }
   },
   "outputs": [],
   "source": [
    "root_dir = '/home2/glee/dissertation/1_tech_gen_impact/Transformer/Tech_Gen/'\n",
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "sys.path.append(\"/share/tml_package\")\n",
    "from tml import utils\n",
    "from scipy import io\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import DataParallel as DP\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset, Dataset\n",
    "from accelerate import Accelerator\n",
    "import pytorch_model_summary\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import RandomSampler, TPESampler\n",
    "from optuna.integration import SkoptSampler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import matthews_corrcoef, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from data import TechDataset, CVSampler\n",
    "from models import Transformer\n",
    "from train_utils import run_epoch, EarlyStopping, perf_eval, objective_cv, build_model, train_model, validate_model\n",
    "from utils import token2class, DotDict\n",
    "\n",
    "from cleantext.sklearn import CleanTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T14:23:21.938234Z",
     "start_time": "2023-02-06T14:23:21.923246Z"
    }
   },
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    data_type=\"claim\",\n",
    "    target_ipc=None,\n",
    "    pred_type=None,\n",
    "    do_train=None,\n",
    "    do_tune=None,\n",
    "    n_folds=None,\n",
    "    max_epochs=None,\n",
    "    use_accelerator=None,\n",
    "    do_save=False,\n",
    "    light=True)\n",
    "\n",
    "config_file = \"configs_light.json\" if args.light else \"configs.json\"\n",
    "configs = DotDict().load(config_file)\n",
    "org_config_keys = {key: list(configs[key].keys()) for key in configs.keys()}\n",
    "\n",
    "instant_configs = {key: value for (key, value) in vars(args).items() if value is not None} # if any argument passed when main.py executed\n",
    "instant_configs_for_update = {configkey: {key: value for (key,value) in instant_configs.items() if key in org_config_keys[configkey]} for configkey in org_config_keys.keys()}\n",
    "for key, value in configs.items():\n",
    "    value.update(instant_configs_for_update[key])\n",
    "\n",
    "regex_ipc = re.compile('[A-Z](?![\\\\D])')\n",
    "if regex_ipc.match(configs.data.target_ipc) is None:\n",
    "    configs.data.update({\"target_ipc\": \"ALL\"})\n",
    "elif len(configs.data.target_ipc) > 5:\n",
    "    configs.data.update({\"target_ipc\": configs.data.target_ipc[:4]})\n",
    "\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "model_dir = os.path.join(root_dir, \"models\")\n",
    "\n",
    "if configs.train.use_accelerator:\n",
    "    accelerator = Accelerator()\n",
    "    device_ids = list(range(torch.cuda.device_count()))\n",
    "    device = accelerator.device\n",
    "\n",
    "    configs.train.update({\"accelerator\": accelerator})\n",
    "    # configs.model.update({\"use_accelerator\": configs.train.use_accelerator})\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        device_ids = list(range(torch.cuda.device_count()))\n",
    "        gpu_usages = [np.sum([float(usage.split(\"uses\")[-1].replace(\" \",\"\").replace(\"MB\",\"\")) for usage in torch.cuda.list_gpu_processes(id).split(\"GPU memory\") if not usage==\"\" and \"no processes are running\" not in usage]) for id in device_ids]\n",
    "        device_ids = np.argsort(gpu_usages)[:configs.train.n_gpus]\n",
    "        device_ids = list(map(lambda x: torch.device('cuda', x),list(device_ids)))\n",
    "        device = device_ids[0] # main device\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        device_ids = []\n",
    "\n",
    "configs.data.update({\"root_dir\": root_dir,\n",
    "                        \"data_dir\": data_dir,\n",
    "                        \"model_dir\": model_dir})\n",
    "configs.train.update({\"device\": device,\n",
    "                        \"device_ids\": device_ids,\n",
    "                        \"root_dir\": root_dir,\n",
    "                        \"data_dir\": data_dir,\n",
    "                        \"model_dir\": model_dir,\n",
    "                        \"early_stop_patience\": int(0.3*configs.train.max_epochs)})\n",
    "configs.model.update({\"device\": device,\n",
    "                        \"device_ids\": device_ids,\n",
    "                        \"n_directions\": 2 if configs.model.bidirec else 1,\n",
    "                        \"n_outputs\": 1 if configs.data.pred_type==\"regression\" else 2,\n",
    "                        \"use_accelerator\": configs.train.use_accelerator})\n",
    "\n",
    "## Set hyperparameters for model training (To be TUNED)\n",
    "if configs.train.do_train and configs.train.do_tune:\n",
    "    n_layers = configs.model.n_layers = None\n",
    "    d_embedding = configs.model.d_embedding = None\n",
    "    d_hidden = configs.model.d_hidden = None\n",
    "    d_latent = None\n",
    "    learning_rate = configs.train.learning_rate = None\n",
    "    batch_size = configs.train.batch_size = None\n",
    "    config_name = \"HPARAM_TUNING\"\n",
    "    final_model_path = None\n",
    "else:\n",
    "    n_layers = configs.model.n_layers\n",
    "    d_embedding = configs.model.d_embedding\n",
    "    d_hidden = configs.model.d_hidden\n",
    "    d_latent = configs.model.n_layers * configs.model.d_hidden * configs.model.n_directions\n",
    "    config_name = f\"{n_layers}layers_{d_embedding}emb_{d_hidden}hid_{configs.model.n_directions}direc_{np.round(configs.train.learning_rate,4)}lr_{configs.train.batch_size}batch_{configs.train.max_epochs}ep\"\n",
    "    final_model_path = os.path.join(model_dir, f\"[Final_model][{configs.data.target_ipc}]{config_name}.ckpt\")\n",
    "\n",
    "configs.model.update({\"d_latent\": d_latent})\n",
    "configs.train.update({\"config_name\": config_name,\n",
    "                        \"final_model_path\": final_model_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Dataset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data\n",
    "importlib.reload(data)\n",
    "from data import TechDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pickled dataset...\n",
      "Pickled dataset loaded\n",
      "2.9394 sec elapsed for loading patents for class [ALL]\n"
     ]
    }
   ],
   "source": [
    "tstart = time.time()\n",
    "dataset_config_name = \"-\".join([str(key)+\"=\"+str(value) for (key,value) in configs.data.items() if key in org_config_keys[\"data\"]])\n",
    "dataset_path = os.path.join(data_dir, \"pickled_dataset\", \"[tech_dataset]\"+dataset_config_name+\".pickle\")\n",
    "if os.path.exists(dataset_path) and args.do_save is False:\n",
    "    print(\"Load pickled dataset...\")\n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        tech_dataset = pickle.load(f)   # Load pickled dataset if dataset with same configuration already saved\n",
    "    print(\"Pickled dataset loaded\")\n",
    "else:\n",
    "    print(\"Make dataset...\")\n",
    "    tech_dataset = TechDataset(configs.data)\n",
    "#     with open(dataset_path, \"wb\") as f:\n",
    "#         tech_dataset.rawdata = None\n",
    "#         pickle.dump(tech_dataset, f)\n",
    "tend = time.time()\n",
    "print(f\"{np.round(tend-tstart,4)} sec elapsed for loading patents for class [{configs.data.target_ipc}]\")\n",
    "\n",
    "configs.model.update({\"vocabulary\": tech_dataset.vocab_w2i,\n",
    "                        \"vocabulary_rev\": tech_dataset.vocab_i2w,\n",
    "                        \"n_enc_vocab\": tech_dataset.vocab_size,\n",
    "                        \"n_dec_vocab\": tech_dataset.vocab_size,\n",
    "                        \"n_enc_seq\": tech_dataset.seq_len,\n",
    "                        \"n_dec_seq\": tech_dataset.seq_len,\n",
    "                        \"i_padding\": tech_dataset.vocab_w2i[\"<PAD>\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(dataset_path, \"wb\") as f:\n",
    "#     tech_dataset.rawdata = None\n",
    "#     pickle.dump(tech_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(368,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of sub ipcs\n",
    "np.unique(np.concatenate([[xx[:4] for xx in x] for x in tech_dataset.data['sub_ipc'].values])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5610880829015543"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average sub ipcs\n",
    "np.mean([len(x) for x in tech_dataset.data['sub_ipc'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs.model.n_enc_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Samples\n",
      "Train: 55847, Validation: 13962, Test: 29919\n"
     ]
    }
   ],
   "source": [
    "sampler = CVSampler(tech_dataset, n_folds=configs.train.n_folds, test_ratio=0.3, stratify=True)\n",
    "cv_idx = sampler.get_idx_dict()\n",
    "print(f\"#Samples\\nTrain: {len(cv_idx[0]['train'])}, Validation: {len(cv_idx[0]['val'])}, Test: {len(cv_idx[0]['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3-1: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if configs.train.do_tune:\n",
    "    configs.train.update({\"tuned_model_path\": os.path.join(model_dir,\"hparam_tuning\")})\n",
    "    optuna_obj = lambda trial: objective_cv(trial, dataset=tech_dataset, cv_idx=cv_idx, model_params=configs.model, train_params=configs.train)\n",
    "\n",
    "    opt_sampler = TPESampler()\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(optuna_obj, n_trials=configs.train.n_trials, gc_after_trial=True)\n",
    "    best_params = study.best_trial.params\n",
    "\n",
    "    print(f\"Best trial:\\n  CrossEntropyLoss: {study.best_trial.value}\\n  Params:\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"    {k}: {v}\")\n",
    "\n",
    "    configs.train.update({k: v for k,v in best_params.items() if k in configs.train.keys()})\n",
    "    configs.model.update({k: v for k,v in best_params.items() if k in configs.model.keys()})\n",
    "    config_name = f\"{configs.model.n_layers}layers_{configs.model.d_embedding}emb_{configs.model.d_hidden}hid_{configs.model.n_directions}direc_{np.round(configs.train.learning_rate, 4)}lr_{configs.train.batch_size}batch_{configs.train.max_epochs}ep\"\n",
    "    final_model_path = os.path.join(model_dir, f\"[Final_model][{configs.data.target_ipc}]{config_name}.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3-2: Dataset construction and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                   Parent Layers       Layer (type)            Input Shape         Param #     Tr. Param #\n",
      "==========================================================================================================================================================\n",
      "                                                             Transformer/Encoder        Embedding-1             [512, 132]          32,064          32,064\n",
      "                                                             Transformer/Encoder        Embedding-2             [512, 132]           4,256               0\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention           Linear-3         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention           Linear-4         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention           Linear-5         [512, 132, 32]           4,224           4,224\n",
      "   Transformer/Encoder/EncoderLayer/MultiHeadAttention/ScaledDotProductAttention          Dropout-6     [512, 4, 132, 132]               0               0\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention           Linear-7        [512, 132, 128]           4,128           4,128\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention          Dropout-8         [512, 132, 32]               0               0\n",
      "                                                Transformer/Encoder/EncoderLayer        LayerNorm-9         [512, 132, 32]              64              64\n",
      "                          Transformer/Encoder/EncoderLayer/PoswiseFeedForwardNet          Conv1d-10         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Encoder/EncoderLayer/PoswiseFeedForwardNet          Conv1d-11         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Encoder/EncoderLayer/PoswiseFeedForwardNet         Dropout-12         [512, 132, 32]               0               0\n",
      "                                                Transformer/Encoder/EncoderLayer       LayerNorm-13         [512, 132, 32]              64              64\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention          Linear-14         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention          Linear-15         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention          Linear-16         [512, 132, 32]           4,224           4,224\n",
      "   Transformer/Encoder/EncoderLayer/MultiHeadAttention/ScaledDotProductAttention         Dropout-17     [512, 4, 132, 132]               0               0\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention          Linear-18        [512, 132, 128]           4,128           4,128\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention         Dropout-19         [512, 132, 32]               0               0\n",
      "                                                Transformer/Encoder/EncoderLayer       LayerNorm-20         [512, 132, 32]              64              64\n",
      "                          Transformer/Encoder/EncoderLayer/PoswiseFeedForwardNet          Conv1d-21         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Encoder/EncoderLayer/PoswiseFeedForwardNet          Conv1d-22         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Encoder/EncoderLayer/PoswiseFeedForwardNet         Dropout-23         [512, 132, 32]               0               0\n",
      "                                                Transformer/Encoder/EncoderLayer       LayerNorm-24         [512, 132, 32]              64              64\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention          Linear-25         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention          Linear-26         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention          Linear-27         [512, 132, 32]           4,224           4,224\n",
      "   Transformer/Encoder/EncoderLayer/MultiHeadAttention/ScaledDotProductAttention         Dropout-28     [512, 4, 132, 132]               0               0\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention          Linear-29        [512, 132, 128]           4,128           4,128\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention         Dropout-30         [512, 132, 32]               0               0\n",
      "                                                Transformer/Encoder/EncoderLayer       LayerNorm-31         [512, 132, 32]              64              64\n",
      "                          Transformer/Encoder/EncoderLayer/PoswiseFeedForwardNet          Conv1d-32         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Encoder/EncoderLayer/PoswiseFeedForwardNet          Conv1d-33         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Encoder/EncoderLayer/PoswiseFeedForwardNet         Dropout-34         [512, 132, 32]               0               0\n",
      "                                                Transformer/Encoder/EncoderLayer       LayerNorm-35         [512, 132, 32]              64              64\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention          Linear-36         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention          Linear-37         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention          Linear-38         [512, 132, 32]           4,224           4,224\n",
      "   Transformer/Encoder/EncoderLayer/MultiHeadAttention/ScaledDotProductAttention         Dropout-39     [512, 4, 132, 132]               0               0\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention          Linear-40        [512, 132, 128]           4,128           4,128\n",
      "                             Transformer/Encoder/EncoderLayer/MultiHeadAttention         Dropout-41         [512, 132, 32]               0               0\n",
      "                                                Transformer/Encoder/EncoderLayer       LayerNorm-42         [512, 132, 32]              64              64\n",
      "                          Transformer/Encoder/EncoderLayer/PoswiseFeedForwardNet          Conv1d-43         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Encoder/EncoderLayer/PoswiseFeedForwardNet          Conv1d-44         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Encoder/EncoderLayer/PoswiseFeedForwardNet         Dropout-45         [512, 132, 32]               0               0\n",
      "                                                Transformer/Encoder/EncoderLayer       LayerNorm-46         [512, 132, 32]              64              64\n",
      "                                                             Transformer/Decoder       Embedding-47             [512, 132]          32,064          32,064\n",
      "                                                             Transformer/Decoder       Embedding-48             [512, 132]           4,256               0\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-49         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-50         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-51         [512, 132, 32]           4,224           4,224\n",
      "   Transformer/Decoder/DecoderLayer/MultiHeadAttention/ScaledDotProductAttention         Dropout-52     [512, 4, 132, 132]               0               0\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-53        [512, 132, 128]           4,128           4,128\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention         Dropout-54         [512, 132, 32]               0               0\n",
      "                                                Transformer/Decoder/DecoderLayer       LayerNorm-55         [512, 132, 32]              64              64\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-56         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-57         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-58         [512, 132, 32]           4,224           4,224\n",
      "   Transformer/Decoder/DecoderLayer/MultiHeadAttention/ScaledDotProductAttention         Dropout-59     [512, 4, 132, 132]               0               0\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-60        [512, 132, 128]           4,128           4,128\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention         Dropout-61         [512, 132, 32]               0               0\n",
      "                                                Transformer/Decoder/DecoderLayer       LayerNorm-62         [512, 132, 32]              64              64\n",
      "                          Transformer/Decoder/DecoderLayer/PoswiseFeedForwardNet          Conv1d-63         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Decoder/DecoderLayer/PoswiseFeedForwardNet          Conv1d-64         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Decoder/DecoderLayer/PoswiseFeedForwardNet         Dropout-65         [512, 132, 32]               0               0\n",
      "                                                Transformer/Decoder/DecoderLayer       LayerNorm-66         [512, 132, 32]              64              64\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-67         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-68         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-69         [512, 132, 32]           4,224           4,224\n",
      "   Transformer/Decoder/DecoderLayer/MultiHeadAttention/ScaledDotProductAttention         Dropout-70     [512, 4, 132, 132]               0               0\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-71        [512, 132, 128]           4,128           4,128\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention         Dropout-72         [512, 132, 32]               0               0\n",
      "                                                Transformer/Decoder/DecoderLayer       LayerNorm-73         [512, 132, 32]              64              64\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-74         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-75         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-76         [512, 132, 32]           4,224           4,224\n",
      "   Transformer/Decoder/DecoderLayer/MultiHeadAttention/ScaledDotProductAttention         Dropout-77     [512, 4, 132, 132]               0               0\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-78        [512, 132, 128]           4,128           4,128\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention         Dropout-79         [512, 132, 32]               0               0\n",
      "                                                Transformer/Decoder/DecoderLayer       LayerNorm-80         [512, 132, 32]              64              64\n",
      "                          Transformer/Decoder/DecoderLayer/PoswiseFeedForwardNet          Conv1d-81         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Decoder/DecoderLayer/PoswiseFeedForwardNet          Conv1d-82         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Decoder/DecoderLayer/PoswiseFeedForwardNet         Dropout-83         [512, 132, 32]               0               0\n",
      "                                                Transformer/Decoder/DecoderLayer       LayerNorm-84         [512, 132, 32]              64              64\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-85         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-86         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-87         [512, 132, 32]           4,224           4,224\n",
      "   Transformer/Decoder/DecoderLayer/MultiHeadAttention/ScaledDotProductAttention         Dropout-88     [512, 4, 132, 132]               0               0\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-89        [512, 132, 128]           4,128           4,128\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention         Dropout-90         [512, 132, 32]               0               0\n",
      "                                                Transformer/Decoder/DecoderLayer       LayerNorm-91         [512, 132, 32]              64              64\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-92         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-93         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-94         [512, 132, 32]           4,224           4,224\n",
      "   Transformer/Decoder/DecoderLayer/MultiHeadAttention/ScaledDotProductAttention         Dropout-95     [512, 4, 132, 132]               0               0\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention          Linear-96        [512, 132, 128]           4,128           4,128\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention         Dropout-97         [512, 132, 32]               0               0\n",
      "                                                Transformer/Decoder/DecoderLayer       LayerNorm-98         [512, 132, 32]              64              64\n",
      "                          Transformer/Decoder/DecoderLayer/PoswiseFeedForwardNet          Conv1d-99         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Decoder/DecoderLayer/PoswiseFeedForwardNet         Conv1d-100         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Decoder/DecoderLayer/PoswiseFeedForwardNet        Dropout-101         [512, 132, 32]               0               0\n",
      "                                                Transformer/Decoder/DecoderLayer      LayerNorm-102         [512, 132, 32]              64              64\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention         Linear-103         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention         Linear-104         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention         Linear-105         [512, 132, 32]           4,224           4,224\n",
      "   Transformer/Decoder/DecoderLayer/MultiHeadAttention/ScaledDotProductAttention        Dropout-106     [512, 4, 132, 132]               0               0\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention         Linear-107        [512, 132, 128]           4,128           4,128\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention        Dropout-108         [512, 132, 32]               0               0\n",
      "                                                Transformer/Decoder/DecoderLayer      LayerNorm-109         [512, 132, 32]              64              64\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention         Linear-110         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention         Linear-111         [512, 132, 32]           4,224           4,224\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention         Linear-112         [512, 132, 32]           4,224           4,224\n",
      "   Transformer/Decoder/DecoderLayer/MultiHeadAttention/ScaledDotProductAttention        Dropout-113     [512, 4, 132, 132]               0               0\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention         Linear-114        [512, 132, 128]           4,128           4,128\n",
      "                             Transformer/Decoder/DecoderLayer/MultiHeadAttention        Dropout-115         [512, 132, 32]               0               0\n",
      "                                                Transformer/Decoder/DecoderLayer      LayerNorm-116         [512, 132, 32]              64              64\n",
      "                          Transformer/Decoder/DecoderLayer/PoswiseFeedForwardNet         Conv1d-117         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Decoder/DecoderLayer/PoswiseFeedForwardNet         Conv1d-118         [512, 32, 132]           1,056           1,056\n",
      "                          Transformer/Decoder/DecoderLayer/PoswiseFeedForwardNet        Dropout-119         [512, 132, 32]               0               0\n",
      "                                                Transformer/Decoder/DecoderLayer      LayerNorm-120         [512, 132, 32]              64              64\n",
      "                                                             Transformer/Decoder         Linear-121         [512, 132, 32]          33,066          33,066\n",
      "==========================================================================================================================================================\n",
      "Total params: 325,482\n",
      "Trainable params: 316,970\n",
      "Non-trainable params: 8,512\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Construct datasets\n",
    "train_idx = cv_idx[0]['train']\n",
    "val_idx = cv_idx[0]['val']\n",
    "test_idx = cv_idx[0]['test']\n",
    "whole_idx = np.concatenate([train_idx, val_idx])\n",
    "\n",
    "train_dataset = Subset(tech_dataset, train_idx)\n",
    "val_dataset = Subset(tech_dataset, val_idx)\n",
    "test_dataset = Subset(tech_dataset, test_idx)\n",
    "whole_dataset = Subset(tech_dataset, whole_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=configs.train.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=configs.train.batch_size if len(val_idx)>configs.train.batch_size else len(val_idx), shuffle=True, num_workers=4, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=configs.train.batch_size if len(test_idx)>configs.train.batch_size else len(test_idx), shuffle=False, num_workers=4)\n",
    "whole_loader = DataLoader(whole_dataset, batch_size=configs.train.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "## Load best model or build model\n",
    "final_model = build_model(configs.model)\n",
    "x_input, _ = next(iter(train_loader))\n",
    "if re.search(\"^1.\", torch.__version__) is not None:\n",
    "    print(pytorch_model_summary.summary(final_model.module, torch.zeros(x_input.shape, device=device, dtype=torch.long), torch.zeros(x_input.shape, device=device, dtype=torch.long), show_input=True, max_depth=None, show_parent_layers=True))\n",
    "else:\n",
    "    print(\"INFO: pytorch-model-summary does not support PyTorch 2.0, so just print model structure\")\n",
    "    print(final_model)\n",
    "    torch._dynamo.config.verbose = True\n",
    "    torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:170% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:170% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Train model\n",
    "if configs.train.do_tune:\n",
    "    best_states = torch.load(os.path.join(configs.train.tuned_model_path,f\"[HPARAM_TUNING]{study.best_trial.number}trial.ckpt\"))\n",
    "    converted_states = OrderedDict()\n",
    "    for k, v in best_states.items():\n",
    "        if 'module' not in k:\n",
    "            k = 'module.'+k\n",
    "        else:\n",
    "            k = k.replace('features.module.', 'module.features.')\n",
    "        converted_states[k] = v\n",
    "    final_model.load_state_dict(converted_states)\n",
    "else:\n",
    "    final_model = train_model(final_model, train_loader, val_loader, configs.model, configs.train)\n",
    "torch.save(final_model.state_dict(), final_model_path) # Finalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3-3: Training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation on train dataset\n",
    "trues_recon_train, preds_recon_train = validate_model(final_model, whole_loader, configs.model)\n",
    "# trues_y_train, preds_y_train = validate_model(final_model, whole_loader, configs.model)\n",
    "eval_recon_train = perf_eval(\"TRAIN_SET\", trues_recon_train, preds_recon_train, configs=configs, pred_type='generative')\n",
    "# eval_y_train = perf_eval(\"TRAIN_SET\", trues_y_train, preds_y_train, pred_type=configs.data.pred_type)\n",
    "# if configs.data.pred_type == \"classification\":\n",
    "#     eval_y_train, confmat_y_train = eval_y_train\n",
    "\n",
    "## Evaluation on test dataset\n",
    "trues_recon_test, preds_recon_test = validate_model(final_model, test_loader, configs.model)\n",
    "# trues_y_test, preds_y_test = validate_model(final_model, test_loader, configs.model)\n",
    "eval_recon_test = perf_eval(\"TEST_SET\", trues_recon_test, preds_recon_test, configs=configs,  pred_type='generative')\n",
    "# eval_y_test = perf_eval(\"TEST_SET\", trues_y_test, preds_y_test, pred_type=configs.data.pred_type)\n",
    "# if configs.data.pred_type == \"classification\":\n",
    "#     eval_y_test, confmat_y_test = eval_y_test\n",
    "\n",
    "# eval_y_res = pd.concat([eval_y_train, eval_y_test], axis=0)\n",
    "eval_recon_res = pd.concat([eval_recon_train, eval_recon_test], axis=0)\n",
    "\n",
    "result_path = os.path.join(root_dir, \"results\")\n",
    "# with pd.ExcelWriter(os.path.join(result_path,f\"[RESULT][{args.target_ipc}]{train_param_name}.xlsx\")) as writer:\n",
    "#     eval_y_res.to_excel(writer, sheet_name=args.pred_type)\n",
    "    # eval_recon_train.to_excel(writer, sheet_name=\"Generative_TRAIN\")\n",
    "    # eval_recon_test.to_excel(writer, sheet_name=\"Generative_TEST\")\n",
    "\n",
    "print(\"Training is done!\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
