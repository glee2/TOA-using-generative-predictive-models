{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:59:40.407303Z",
     "start_time": "2025-06-08T19:59:31.803402Z"
    }
   },
   "outputs": [],
   "source": [
    "root_dir = '/home2/glee/dissertation/1_tech_gen_impact/class2class/Tech_Gen/'\n",
    "master_dir = '/home2/glee/dissertation/1_tech_gen_impact/master/Tech_Gen/'\n",
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "import uspto\n",
    "import json\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "import multiprocess as mp\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "sys.path.append(\"/share/tml_package\")\n",
    "from tml import utils\n",
    "from scipy import io\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import DataParallel as DP\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset, Dataset\n",
    "from accelerate import Accelerator\n",
    "import pytorch_model_summary\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import RandomSampler, TPESampler\n",
    "from optuna.integration import SkoptSampler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import matthews_corrcoef, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from data import TechDataset, CVSampler\n",
    "from models import Transformer, Predictor\n",
    "from train_utils import EarlyStopping, perf_eval, objective_cv, build_model, train_model, validate_model_mp\n",
    "from utils import token2class, DotDict, to_device\n",
    "\n",
    "from cleantext.sklearn import CleanTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:59:40.489064Z",
     "start_time": "2025-06-08T19:59:40.411747Z"
    }
   },
   "outputs": [],
   "source": [
    "# analysis_date = \"2023-05-04_1802\" # Seminconductor\n",
    "# analysis_date = \"2023-05-09_0331\" # AI\n",
    "analysis_date = \"2025-06-07_1732\" # AI, CPC\n",
    "args = argparse.Namespace(\n",
    "    do_eval = True,\n",
    "    do_save=False,\n",
    "    config_file=os.path.join(root_dir, \"configs\", \"USED_configs\", \"[CONFIGS]\"+analysis_date+\".json\"),\n",
    "    eval_train_set=False)\n",
    "\n",
    "project_data_dir = os.path.join(master_dir, \"data\")\n",
    "data_dir = os.path.join(\"/home2/glee/patent_data/data/\")\n",
    "model_dir = os.path.join(root_dir, \"models\")\n",
    "result_dir = os.path.join(root_dir, \"results\")\n",
    "config_dir = os.path.join(root_dir, \"configs\")\n",
    "\n",
    "## parse configuration file\n",
    "if args.config_file is not None:\n",
    "    config_file = args.config_file\n",
    "else:\n",
    "    config_file = os.path.join(config_dir, \"configs_light.json\") if args.light else os.path.join(config_dir, \"configs.json\")\n",
    "if args.do_eval: args.do_train = False\n",
    "configs = DotDict().load(config_file)\n",
    "org_config_keys = {key: list(configs[key].keys()) for key in configs.keys()}\n",
    "\n",
    "# parse command line arguments\n",
    "instant_configs = {key: value for (key, value) in vars(args).items() if value is not None} # if any argument passed when main.py executed\n",
    "instant_configs_for_update = {configkey: {key: value for (key,value) in instant_configs.items() if key in org_config_keys[configkey]} for configkey in org_config_keys.keys()}\n",
    "for key, value in configs.items():\n",
    "    value.update(instant_configs_for_update[key])\n",
    "\n",
    "## assign loss weights\n",
    "if configs.model.model_type == \"enc-pred-dec\":\n",
    "    configs.train.loss_weights[\"recon\"] = configs.train.loss_weights[\"recon\"] / sum(configs.train.loss_weights.values())\n",
    "    configs.train.loss_weights[\"y\"] = 1 - configs.train.loss_weights[\"recon\"]\n",
    "elif configs.model.model_type == \"enc-pred\":\n",
    "    configs.train.loss_weights = {\"recon\": 0, \"y\": 1}\n",
    "elif configs.model.model_type == \"enc-dec\":\n",
    "    configs.train.loss_weights = {\"recon\": 1, \"y\": 0}\n",
    "\n",
    "## assign devices\n",
    "if configs.train.use_accelerator:\n",
    "    accelerator = Accelerator()\n",
    "    device_ids = list(range(torch.cuda.device_count()))\n",
    "    device = accelerator.device\n",
    "    configs.train.update({\"accelerator\": accelerator})\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        device_ids = list(range(torch.cuda.device_count()))\n",
    "        gpu_usages = [np.sum([float(usage.split(\"uses\")[-1].replace(\" \",\"\").replace(\"MB\",\"\")) for usage in torch.cuda.list_gpu_processes(id).split(\"GPU memory\") if not usage==\"\" and \"no processes are running\" not in usage]) for id in device_ids]\n",
    "        device_ids = np.argsort(gpu_usages)[:configs.train.n_gpus]\n",
    "        device_ids = list(map(lambda x: torch.device('cuda', x),list(device_ids)))\n",
    "        device = device_ids[0] # main device\n",
    "        torch.cuda.set_device(device)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        device_ids = []\n",
    "\n",
    "## extract configurations for dataset\n",
    "config_period = \"[\"+\"-\".join([str(year) for year in configs.data.target_period])+\"]\"\n",
    "config_area = str(configs.data.target_area).replace(\"\\'\",\"\").replace(\" \",\"\")\n",
    "config_keywords = str(configs.data.target_keywords).replace(\"\\'\",\"\").replace(\" \",\"\")\n",
    "config_sampling_ratio = \"[\"+str(configs.data.sampling_ratio)+\"sampling\"+\"]\" if configs.data.sampling_ratio < 1 else \"\"\n",
    "\n",
    "## update configurations\n",
    "configs.data.update({\"root_dir\": root_dir,\n",
    "                        \"data_dir\": data_dir,\n",
    "                        \"model_dir\": model_dir,\n",
    "                        \"result_dir\": result_dir,\n",
    "                        \"pretrained_enc\": configs.model.pretrained_enc,\n",
    "                        \"pretrained_dec\": configs.model.pretrained_dec,\n",
    "                        \"data_nrows\": None,\n",
    "                        \"data_file\": \"collection_\" + \"\".join([config_keywords, config_area, config_period, config_sampling_ratio]) + \".csv\"})\n",
    "configs.train.update({\"device\": device,\n",
    "                        \"device_ids\": device_ids,\n",
    "                        \"root_dir\": root_dir,\n",
    "                        \"data_dir\": data_dir,\n",
    "                        \"model_dir\": model_dir,\n",
    "                        \"use_keywords\": configs.data.use_keywords,\n",
    "                        \"class_system\": configs.data.class_system,\n",
    "                        \"curr_ep\": 1,\n",
    "                        \"early_stop_patience\": int(0.3*configs.train.max_epochs)})\n",
    "configs.model.update({\"device\": device,\n",
    "                        \"device_ids\": device_ids,\n",
    "                        \"n_directions\": 2 if configs.model.bidirec else 1,\n",
    "                        \"use_accelerator\": configs.train.use_accelerator,\n",
    "                        \"model_dir\": model_dir})\n",
    "\n",
    "## Set hyperparameters for model training (To be TUNED)\n",
    "if configs.train.do_train and configs.train.do_tune:\n",
    "    n_layers = configs.model.n_layers = None\n",
    "    d_embedding = configs.model.d_embedding = None\n",
    "    d_enc_hidden = configs.model.d_enc_hidden = None\n",
    "    d_pred_hidden = configs.model.d_pred_hidden = None\n",
    "    learning_rate = configs.train.learning_rate = None\n",
    "    batch_size = configs.train.batch_size = None\n",
    "    config_name = \"HPARAM_TUNING\"\n",
    "    final_model_path = None\n",
    "else:\n",
    "    n_layers = configs.model.n_layers\n",
    "    d_embedding = configs.model.d_embedding\n",
    "    d_enc_hidden = configs.model.d_enc_hidden\n",
    "    d_pred_hidden = configs.model.d_pred_hidden\n",
    "    d_latent = configs.model.d_latent\n",
    "\n",
    "    key_components = {\"data\": [\"class_level\", \"class_system\", \"max_seq_len_class\", \"max_seq_len_claim\", \"vocab_size\"], \"model\": [\"n_layers\", \"d_hidden\", \"d_pred_hidden\", \"d_latent\", \"d_embedding\", \"d_ff\", \"n_head\", \"d_head\"], \"train\": [\"learning_rate\", \"batch_size\", \"max_epochs\", \"curr_ep\"]}\n",
    "    model_config_name_prefix = \"\".join([config_keywords, config_area, config_period, config_sampling_ratio]) + \"data\"\n",
    "    model_config_name = \"\" + model_config_name_prefix\n",
    "    model_config_name += f\"[{configs.data.class_system}]system\"\n",
    "    for key in [\"model\", \"train\"]:\n",
    "        for component in key_components[key]:\n",
    "            model_config_name += f\"[{str(configs[key][component])}]{component}\"\n",
    "    final_model_path = os.path.join(model_dir, f\"[MODEL]{model_config_name}.ckpt\")\n",
    "\n",
    "# configs.train.update({\"model_config_name\": model_config_name, \"final_model_path\": final_model_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:13:56.393152Z",
     "start_time": "2025-06-08T19:59:40.670960Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pickled dataset...\n",
      "Pickled dataset loaded\n"
     ]
    }
   ],
   "source": [
    "''' PART 2: Dataset setting '''\n",
    "tstart = time.time()\n",
    "dataset_config_name = \"\".join([config_keywords, config_area, config_period, config_sampling_ratio]) + \"data\"\n",
    "for component in key_components[\"data\"]:\n",
    "    dataset_config_name += f\"[{str(configs.data[component])}]{component}\"\n",
    "dataset_path = os.path.join(project_data_dir, \"pickled_dataset\", \"[DATASET]\"+dataset_config_name+\".pickle\")\n",
    "\n",
    "if os.path.exists(dataset_path) and args.do_save is False:\n",
    "    print(\"Load pickled dataset...\")\n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        tech_dataset = pickle.load(f)   # Load pickled dataset if dataset with same configuration already saved\n",
    "        if tech_dataset.pretrained_enc != configs.data.pretrained_enc or tech_dataset.pretrained_dec != configs.data.pretrained_dec:\n",
    "            tech_dataset.pretrained_enc = configs.data.pretrained_enc\n",
    "            tech_dataset.pretrained_dec = configs.data.pretrained_dec\n",
    "            tech_dataset.tokenizers = tech_dataset.get_tokenizers()\n",
    "        for tk in tech_dataset.tokenizers.values():\n",
    "            if \"vocab_size\" not in dir(tk):\n",
    "                tk.vocab_size = tk.get_vocab_size()\n",
    "        tech_dataset.use_keywords = configs.data.use_keywords\n",
    "        ## load saved rawdata\n",
    "        if tech_dataset.rawdata is None:\n",
    "            tech_dataset.rawdata = pd.read_csv(os.path.join(data_dir, configs.data.data_file), low_memory=False)\n",
    "    print(\"Pickled dataset loaded\")\n",
    "else:\n",
    "    print(\"Make dataset...\")\n",
    "    if args.debug:\n",
    "        configs.data.update({\"data_nrows\": 1000})\n",
    "        dataset_path += \".debug\"\n",
    "    tech_dataset = TechDataset(configs.data)\n",
    "    if not args.debug:\n",
    "        rawdata_for_save = copy.deepcopy(tech_dataset.rawdata)\n",
    "        with open(dataset_path, \"wb\") as f:\n",
    "            tech_dataset.rawdata = None\n",
    "            pickle.dump(tech_dataset, f)\n",
    "        tech_dataset.rawdata = rawdata_for_save\n",
    "tend = time.time()\n",
    "# print(f\"{np.round(tend-tstart,4)} sec elapsed for loading patents for class [{configs.data.target_area}]\")\n",
    "\n",
    "configs.model.update({\"tokenizers\": tech_dataset.tokenizers,\n",
    "                    \"n_enc_seq_claim\": tech_dataset.max_seq_len_claim,\n",
    "                    \"n_dec_seq_claim\": tech_dataset.max_seq_len_claim,\n",
    "                    \"n_enc_seq_class\": tech_dataset.max_seq_len_class,\n",
    "                    \"n_dec_seq_class\": tech_dataset.max_seq_len_class,\n",
    "                    \"n_outputs\": 1 if configs.data.pred_type==\"regression\" else tech_dataset.n_outputs,\n",
    "                    \"i_padding\": tech_dataset.tokenizers[\"class_enc\"].pad_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:13:56.513825Z",
     "start_time": "2025-06-08T20:13:56.459149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133654"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:13:56.964472Z",
     "start_time": "2025-06-08T20:13:56.517692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home2/glee/dissertation/1_tech_gen_impact/class2class/Tech_Gen/models/[MODEL][uspto_AI][2006-2015]data[CPC]system[2]n_layers[32]d_hidden[8]d_pred_hidden[32]d_latent[128]d_embedding[16]d_ff[2]n_head[16]d_head[0.0015]learning_rate[512]batch_size[30]max_epochs[1]curr_ep.ckpt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:13:57.699791Z",
     "start_time": "2025-06-08T20:13:56.969584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded\n"
     ]
    }
   ],
   "source": [
    "final_model = build_model(configs.model, tokenizers=tech_dataset.tokenizers)\n",
    "final_model_finder = final_model_path.split(\"[MODEL]\")[-1].split(\"max_epochs\")[0]+\"max_epochs\"\n",
    "matched_ckpts = [f for f in os.listdir(model_dir) if final_model_finder in f]\n",
    "latest_ckpt_index = np.argmax([int(f.split(\"curr_ep\")[0].split(\"[\")[-1].replace(\"]\",\"\")) for f in matched_ckpts])\n",
    "final_model_path = os.path.join(model_dir, matched_ckpts[latest_ckpt_index])\n",
    "if os.path.exists(final_model_path):\n",
    "    best_states = torch.load(final_model_path, map_location=device)\n",
    "else:\n",
    "    raise Exception(\"Model need to be trained first\")\n",
    "\n",
    "has_module_prefix = any(k.startswith(\"module.\") for k in best_states.keys())\n",
    "if has_module_prefix:\n",
    "    stripped = {}\n",
    "    for k, v in best_states.items():\n",
    "        new_key = k[len(\"module.\"):] if k.startswith(\"module.\") else k\n",
    "        stripped[new_key] = v\n",
    "    best_states = stripped\n",
    "final_model.load_state_dict(best_states)\n",
    "\n",
    "del best_states\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Model successfully loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:13:57.715002Z",
     "start_time": "2025-06-08T20:13:57.708027Z"
    }
   },
   "outputs": [],
   "source": [
    "global final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:13:58.669834Z",
     "start_time": "2025-06-08T20:13:57.721275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1.5M paramaters\n"
     ]
    }
   ],
   "source": [
    " if re.search(\"^1.\", torch.__version__) is not None:\n",
    "        model_size = sum(t.numel() for t in final_model.parameters())\n",
    "        print(f\"Model size: {model_size/1000**2:.1f}M paramaters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:14:42.012930Z",
     "start_time": "2025-06-08T20:13:58.672035Z"
    }
   },
   "outputs": [],
   "source": [
    "result_path = os.path.join(root_dir, \"results\")\n",
    "\n",
    "used_train_data = pd.read_excel(os.path.join(result_dir, \"[DATASET]\"+analysis_date+\".xlsx\"), sheet_name=\"TRAIN_dataset\")\n",
    "train_idx = tech_dataset.data.index.astype(int).get_indexer(pd.Index(used_train_data[\"patent_number\"]))\n",
    "train_dataset = Subset(tech_dataset, train_idx)\n",
    "\n",
    "used_test_data = pd.read_excel(os.path.join(result_dir, \"[DATASET]\"+analysis_date+\".xlsx\"), sheet_name=\"TEST_dataset\")\n",
    "test_idx = tech_dataset.data.index.astype(int).get_indexer(pd.Index(used_test_data[\"patent_number\"]))\n",
    "test_dataset = Subset(tech_dataset, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T08:17:48.461478Z",
     "start_time": "2025-06-08T08:17:46.287569Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:14:42.017927Z",
     "start_time": "2025-06-08T20:14:42.015117Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization -> 오래 걸리고 결과 이상함, 일단 보류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:14:42.090800Z",
     "start_time": "2025-06-08T20:14:42.019688Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zs, ys, preds = [], [], []\n",
    "newzs = []\n",
    "for batch_data in tqdm(train_loader):\n",
    "    batch_data = to_device(batch_data, final_model.device)\n",
    "    y = batch_data[\"targets\"].cpu().detach().numpy()\n",
    "    \n",
    "    enc_outputs, z, mu, logvar = final_model.encode(batch_data[\"text_inputs\"])\n",
    "    pred_outputs = final_model.predictor(z)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    zs.append(z.cpu().detach().numpy())\n",
    "    ys.append(y)\n",
    "    preds.append(pred_outputs.argmax(1).cpu().detach().numpy())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "                                                \n",
    "zs = np.concatenate(zs)\n",
    "ys = np.concatenate(ys)\n",
    "preds = np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T09:29:26.198074Z",
     "start_time": "2025-06-08T09:29:26.187573Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "col_years = [\"<1976\"] + np.arange(1976,2022).astype(str).tolist()\n",
    "# latest_year = datetime.datetime.now().year - 1\n",
    "latest_year = 2022\n",
    "n_TC = configs.data.n_TC\n",
    "\n",
    "visualize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:14:43.257517Z",
     "start_time": "2025-06-08T20:14:42.093822Z"
    }
   },
   "outputs": [],
   "source": [
    "ref_config_period = \"[2006-2020]\"\n",
    "ref_data_file = \"collection_\" + \"\".join([config_keywords, config_area, ref_config_period, config_sampling_ratio]) + \".csv\"\n",
    "ref_configs = copy.deepcopy(configs)\n",
    "ref_configs.data.update({\"target_period\": ref_config_period, \"data_file\": ref_data_file})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:39:14.074805Z",
     "start_time": "2025-06-08T20:14:43.259469Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer is trained and saved\n"
     ]
    }
   ],
   "source": [
    "ref_dataset = TechDataset(ref_configs.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T17:40:15.894032Z",
     "start_time": "2025-06-08T17:40:15.884653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data.TechDataset at 0x7f9d04d0e370>"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:39:19.413440Z",
     "start_time": "2025-06-08T20:39:14.078190Z"
    }
   },
   "outputs": [],
   "source": [
    "used_rawdata = tech_dataset.rawdata.set_index(\"patent_number\")\n",
    "total_data = pd.concat([tech_dataset.data, ref_dataset.data], axis=0)\n",
    "total_rawdata = pd.concat([tech_dataset.rawdata.set_index(\"patent_number\"), ref_dataset.rawdata.set_index(\"patent_number\")], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T17:41:18.938147Z",
     "start_time": "2025-06-08T17:41:18.901919Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_number</th>\n",
       "      <th>main_class</th>\n",
       "      <th>sub_class</th>\n",
       "      <th>patent_classes</th>\n",
       "      <th>claims</th>\n",
       "      <th>TC5</th>\n",
       "      <th>TC5_digitized</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patent_number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6983309</th>\n",
       "      <td>6983309</td>\n",
       "      <td>[G06Q010, H04L063, H04L063, G06Q020, G06Q020, ...</td>\n",
       "      <td>[Y10S707, Y10S707, H04L051]</td>\n",
       "      <td>[G06Q010, H04L063, H04L063, G06Q020, G06Q020, ...</td>\n",
       "      <td>1. An electronic apparatus comprising: a trans...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6982420</th>\n",
       "      <td>6982420</td>\n",
       "      <td>[H01J037, G06V020]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[H01J037, G06V020]</td>\n",
       "      <td>1. A sample observation method comprising a st...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983071</th>\n",
       "      <td>6983071</td>\n",
       "      <td>[G06V030, G06V030, G06V010, G06V010]</td>\n",
       "      <td>[G06V030]</td>\n",
       "      <td>[G06V030, G06V030, G06V010, G06V010, G06V030]</td>\n",
       "      <td>1. A character segmentation device for removin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6982717</th>\n",
       "      <td>6982717</td>\n",
       "      <td>[G06T005]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[G06T005]</td>\n",
       "      <td>1. A game apparatus comprising: an image gener...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983073</th>\n",
       "      <td>6983073</td>\n",
       "      <td>[G06T005, G06T005, G06T005]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[G06T005, G06T005, G06T005]</td>\n",
       "      <td>1. A method for recovering an image defined as...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10521802</th>\n",
       "      <td>10521802</td>\n",
       "      <td>[G06Q050, G06Q010, G06F040, G06F040, G06Q030, ...</td>\n",
       "      <td>[G06F016]</td>\n",
       "      <td>[G06Q050, G06Q010, G06F040, G06F040, G06Q030, ...</td>\n",
       "      <td>1. A computer-implemented method for reporting...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10517556</th>\n",
       "      <td>10517556</td>\n",
       "      <td>[A61B006, A61B006, A61B006, A61B006, A61B006, ...</td>\n",
       "      <td>[G01R033, A61B005, G01R033, G01R033, A61B005, ...</td>\n",
       "      <td>[A61B006, A61B006, A61B006, A61B006, A61B006, ...</td>\n",
       "      <td>1. A computer-implemented method for increasin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10523658</th>\n",
       "      <td>10523658</td>\n",
       "      <td>[H04L009, H04L009, H04L063, H04L067, H04L063, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[H04L009, H04L009, H04L063, H04L067, H04L063, ...</td>\n",
       "      <td>1. A method comprising: establishing a first s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10523420</th>\n",
       "      <td>10523420</td>\n",
       "      <td>[H04L009, H04L009, H04L009, H04B010, H04B010]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[H04L009, H04L009, H04L009, H04B010, H04B010]</td>\n",
       "      <td>1. An apparatus, comprising: a memory to store...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10523929</th>\n",
       "      <td>10523929</td>\n",
       "      <td>[G06T017, H04N013, G06T007, G06T015, H04N021, ...</td>\n",
       "      <td>[H04N013, H04N013]</td>\n",
       "      <td>[G06T017, H04N013, G06T007, G06T015, H04N021, ...</td>\n",
       "      <td>1. A system comprising: a non-transitory memor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>392404 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               patent_number  \\\n",
       "patent_number                  \n",
       "6983309              6983309   \n",
       "6982420              6982420   \n",
       "6983071              6983071   \n",
       "6982717              6982717   \n",
       "6983073              6983073   \n",
       "...                      ...   \n",
       "10521802            10521802   \n",
       "10517556            10517556   \n",
       "10523658            10523658   \n",
       "10523420            10523420   \n",
       "10523929            10523929   \n",
       "\n",
       "                                                      main_class  \\\n",
       "patent_number                                                      \n",
       "6983309        [G06Q010, H04L063, H04L063, G06Q020, G06Q020, ...   \n",
       "6982420                                       [H01J037, G06V020]   \n",
       "6983071                     [G06V030, G06V030, G06V010, G06V010]   \n",
       "6982717                                                [G06T005]   \n",
       "6983073                              [G06T005, G06T005, G06T005]   \n",
       "...                                                          ...   \n",
       "10521802       [G06Q050, G06Q010, G06F040, G06F040, G06Q030, ...   \n",
       "10517556       [A61B006, A61B006, A61B006, A61B006, A61B006, ...   \n",
       "10523658       [H04L009, H04L009, H04L063, H04L067, H04L063, ...   \n",
       "10523420           [H04L009, H04L009, H04L009, H04B010, H04B010]   \n",
       "10523929       [G06T017, H04N013, G06T007, G06T015, H04N021, ...   \n",
       "\n",
       "                                                       sub_class  \\\n",
       "patent_number                                                      \n",
       "6983309                              [Y10S707, Y10S707, H04L051]   \n",
       "6982420                                                       []   \n",
       "6983071                                                [G06V030]   \n",
       "6982717                                                       []   \n",
       "6983073                                                       []   \n",
       "...                                                          ...   \n",
       "10521802                                               [G06F016]   \n",
       "10517556       [G01R033, A61B005, G01R033, G01R033, A61B005, ...   \n",
       "10523658                                                      []   \n",
       "10523420                                                      []   \n",
       "10523929                                      [H04N013, H04N013]   \n",
       "\n",
       "                                                  patent_classes  \\\n",
       "patent_number                                                      \n",
       "6983309        [G06Q010, H04L063, H04L063, G06Q020, G06Q020, ...   \n",
       "6982420                                       [H01J037, G06V020]   \n",
       "6983071            [G06V030, G06V030, G06V010, G06V010, G06V030]   \n",
       "6982717                                                [G06T005]   \n",
       "6983073                              [G06T005, G06T005, G06T005]   \n",
       "...                                                          ...   \n",
       "10521802       [G06Q050, G06Q010, G06F040, G06F040, G06Q030, ...   \n",
       "10517556       [A61B006, A61B006, A61B006, A61B006, A61B006, ...   \n",
       "10523658       [H04L009, H04L009, H04L063, H04L067, H04L063, ...   \n",
       "10523420           [H04L009, H04L009, H04L009, H04B010, H04B010]   \n",
       "10523929       [G06T017, H04N013, G06T007, G06T015, H04N021, ...   \n",
       "\n",
       "                                                          claims  TC5  \\\n",
       "patent_number                                                           \n",
       "6983309        1. An electronic apparatus comprising: a trans...    8   \n",
       "6982420        1. A sample observation method comprising a st...    1   \n",
       "6983071        1. A character segmentation device for removin...    0   \n",
       "6982717        1. A game apparatus comprising: an image gener...    0   \n",
       "6983073        1. A method for recovering an image defined as...    1   \n",
       "...                                                          ...  ...   \n",
       "10521802       1. A computer-implemented method for reporting...    2   \n",
       "10517556       1. A computer-implemented method for increasin...    0   \n",
       "10523658       1. A method comprising: establishing a first s...    0   \n",
       "10523420       1. An apparatus, comprising: a memory to store...   11   \n",
       "10523929       1. A system comprising: a non-transitory memor...    0   \n",
       "\n",
       "               TC5_digitized  class  \n",
       "patent_number                        \n",
       "6983309                    0   1939  \n",
       "6982420                    0   2157  \n",
       "6983071                    0   1958  \n",
       "6982717                    0   1948  \n",
       "6983073                    0   1948  \n",
       "...                      ...    ...  \n",
       "10521802                   0   2539  \n",
       "10517556                   0    240  \n",
       "10523658                   0   2983  \n",
       "10523420                   1   2983  \n",
       "10523929                   0   2550  \n",
       "\n",
       "[392404 rows x 8 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:39:19.439472Z",
     "start_time": "2025-06-08T20:39:19.415777Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133654, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_dataset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:39:19.457400Z",
     "start_time": "2025-06-08T20:39:19.442880Z"
    }
   },
   "outputs": [],
   "source": [
    "L1_criterion = tech_dataset.data[\"TC5\"].quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:39:19.527106Z",
     "start_time": "2025-06-08T20:39:19.459481Z"
    }
   },
   "outputs": [],
   "source": [
    "# used_test_data_TC = used_test_data[(used_test_data[\"TC5\"]>0) & (used_test_data[\"TC5\"]<L1_criterion)].reset_index()\n",
    "used_test_data_TC = used_test_data[used_test_data[\"TC5\"]!=0].reset_index()\n",
    "used_test_index_TC = tech_dataset.data.index.get_indexer(pd.Index(used_test_data_TC[\"patent_number\"].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:21:40.717710Z",
     "start_time": "2025-06-09T02:21:40.677718Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate_reliability(model=None, idx=None, L1_threshold=0.5, n_iter=30, step_size=40):\n",
    "    cnt_nonexist = 0\n",
    "    cnt_noFC = 0\n",
    "    cnt_diverge = 0\n",
    "    cnt_same_ipcs = 0\n",
    "    cnt_diff_ipcs = 0\n",
    "    \n",
    "    input_class = torch.tensor(tech_dataset.tokenizers[\"class_enc\"].encode(tech_dataset.X_class[used_test_index_TC][idx])).unsqueeze(0)\n",
    "    input_claim = tech_dataset.tokenize(tech_dataset.tokenizers[\"claim_enc\"], tech_dataset.X_claim[used_test_index_TC][idx])\n",
    "    input_claim = {k: v.unsqueeze(0) for k, v in input_claim.items()}\n",
    "    batch_input = {\"class\": torch.tensor(input_class), \"claim\": input_claim}\n",
    "    input_inf = to_device(batch_input, model.device)\n",
    "\n",
    "    output_class = torch.tensor(tech_dataset.tokenizers[\"class_dec\"].encode(tech_dataset.X_class[used_test_index_TC][idx])).unsqueeze(0)\n",
    "    batch_output = {\"text_outputs\": torch.tensor(output_class)}\n",
    "    output_inf = to_device(batch_output, model.device)\n",
    "\n",
    "    enc_outputs, z, mu, logvar = model.encode(input_inf)\n",
    "    org_z = copy.deepcopy(z.view(1,-1).cpu().detach().numpy())\n",
    "    pred_outputs = model.predict(z)\n",
    "    org_y = copy.deepcopy(pred_outputs.argmax(1).cpu().detach().numpy())\n",
    "    dec_inputs = None\n",
    "\n",
    "    if used_test_data_TC.iloc[idx][\"TC5\"] > 0:\n",
    "        forward_refs = used_rawdata.loc[used_test_data_TC.iloc[idx][\"patent_number\"]][\"forward_refs\"].split(\";\")\n",
    "        ref_info = total_data.loc[[ref for ref in forward_refs if ref in total_data.index]]\n",
    "        if len(ref_info) == 0:\n",
    "            cnt_nonexist += 1\n",
    "            return (cnt_nonexist, cnt_noFC, cnt_diverge, cnt_same_ipcs, cnt_diff_ipcs), None\n",
    "        else:\n",
    "            ref_ipcs = ref_info[\"patent_classes\"].apply(lambda x: set(x))\n",
    "            ref_FCs = ref_info[\"TC\"+str(n_TC)]\n",
    "\n",
    "            tokenizer = tech_dataset.tokenizers[\"class_dec\"]\n",
    "            \n",
    "            org_text = tokenizer.decode_batch(input_class.cpu().detach().numpy())[0]\n",
    "            org_text = org_text[org_text.index(tokenizer.sos_token)+1:org_text.index(tokenizer.eos_token)]\n",
    "            if set(org_text)==set(np.concatenate(ref_ipcs.apply(lambda x: list(x)).values)):\n",
    "                cnt_same_ipcs += 1\n",
    "\n",
    "            inclusions = [None, None, None, None]\n",
    "            higher_impacts = [None, None, None, None]\n",
    "            similar_refs_out = [None, None, None, None]\n",
    "            unsimilar_refs_out = [None, None, None, None]\n",
    "            optimised = False\n",
    "            for i in range(n_iter):\n",
    "                pred_outputs = model.predict(z)\n",
    "                z.retain_grad()\n",
    "                FC_estimated = np.round(np.exp(pred_outputs[0,1].item()), 4) # estimated forward citations\n",
    "                FC_estimated_inv = np.round(np.exp(pred_outputs[0,0].item()), 4)\n",
    "                \n",
    "                L1_error = (1-torch.exp(pred_outputs[0,1]))\n",
    "                L1_error.backward(retain_graph=True)\n",
    "\n",
    "                grad_for_update = (step_size * z.grad)\n",
    "                z_ = z - grad_for_update\n",
    "\n",
    "                z.grad.zero_()\n",
    "                dec_outputs = model.decode(z_, enc_outputs, dec_inputs=None)\n",
    "                dec_outputs = dec_outputs.argmax(-1)\n",
    "\n",
    "                tokenizer = tech_dataset.tokenizers[\"class_dec\"]\n",
    "                gen_text = tokenizer.decode_batch(dec_outputs.cpu().detach().numpy())[0]\n",
    "                if tokenizer.eos_token in gen_text:\n",
    "                    gen_text = gen_text[gen_text.index(tokenizer.sos_token)+1:gen_text.index(tokenizer.eos_token)]\n",
    "                else:\n",
    "                    gen_text = gen_text[gen_text.index(tokenizer.sos_token)+1:]\n",
    "                if gen_text != []:\n",
    "                    gen_text = [gen_text[0]] + list(np.array(gen_text[1:])[np.unique(gen_text[1:], return_index=True)[1]])                \n",
    "                    gen_text = set(gen_text)\n",
    "                else: continue\n",
    "                \n",
    "                if FC_estimated>=L1_threshold:\n",
    "                    optimised = True\n",
    "    \n",
    "                    gen_text_breakdown = breakdown(gen_text)\n",
    "                    ref_ipcs_breakdown = (ref_ipcs.apply(lambda x: breakdown(x)[0]), ref_ipcs.apply(lambda x: breakdown(x)[1]), ref_ipcs.apply(lambda x: breakdown(x)[2]), ref_ipcs)\n",
    "            \n",
    "                    for i in range(4):\n",
    "                        if inclusions[i] is not None: continue\n",
    "                        temp_gen_text = gen_text_breakdown[i]\n",
    "                        temp_ref_ipcs = ref_ipcs_breakdown[i]\n",
    "                    \n",
    "                        hit_index = temp_ref_ipcs.apply(lambda x: 1 if set(x)==set(temp_gen_text) else 0)==1\n",
    "                        similar_refs = temp_ref_ipcs[hit_index].index\n",
    "                        similar_refs_out[i] = similar_refs\n",
    "                        unsimilar_refs = temp_ref_ipcs[~hit_index].index\n",
    "                        unsimilar_refs_out[i] = unsimilar_refs\n",
    "                        if len(similar_refs) == 0:\n",
    "                            inclusions[i] = 0\n",
    "                            higher_impacts[i] = None\n",
    "                        elif len(unsimilar_refs) == 0:\n",
    "                            inclusions[i] = 1\n",
    "                            similar_mean_FC = np.mean(ref_FCs.loc[similar_refs])\n",
    "                            if similar_mean_FC <= 0:\n",
    "                                higher_impacts[i] = 0\n",
    "                            else:\n",
    "                                higher_impacts[i] = 1\n",
    "                        else:\n",
    "                            inclusions[i] = 1\n",
    "                            similar_mean_FC = np.mean(ref_FCs.loc[similar_refs])\n",
    "                            unsimilar_mean_FC = np.mean(ref_FCs.loc[unsimilar_refs])\n",
    "                            if similar_mean_FC >= unsimilar_mean_FC:\n",
    "                                if similar_mean_FC <= 0:\n",
    "                                    higher_impacts[i] = None\n",
    "                                else:\n",
    "                                    higher_impacts[i] = 1\n",
    "                            else:\n",
    "                                higher_impacts[i] = 0\n",
    "                    if None not in inclusions:\n",
    "                        break\n",
    "                z = z_\n",
    "                \n",
    "            if optimised:\n",
    "                cnt_diff_ipcs += 1\n",
    "                return (cnt_nonexist, cnt_noFC, cnt_diverge, cnt_same_ipcs, cnt_diff_ipcs), {\"index\": idx, \"patent_id\": used_test_data_TC.iloc[idx][\"patent_number\"], \n",
    "                         \"org_text\": org_text, \"gen_text\": gen_text, \"ref_ipcs\": ref_ipcs, \"ref_FCs\": ref_FCs,\n",
    "                         \"inclusions\": inclusions, \"higher_impacts\": higher_impacts, \n",
    "                         \"FC_estimated\": FC_estimated,\n",
    "                         \"similar_refs\": similar_refs_out, \"unsimilar_refs\": unsimilar_refs_out}\n",
    "            else:\n",
    "                cnt_diverge += 1\n",
    "                return (cnt_nonexist, cnt_noFC, cnt_diverge, cnt_same_ipcs, cnt_diff_ipcs), None\n",
    "    else:\n",
    "        pass\n",
    "        cnt_noFC += 1\n",
    "        return (cnt_nonexist, cnt_noFC, cnt_diverge, cnt_same_ipcs, cnt_diff_ipcs), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_reliability_batch(\n",
    "    model,                     # nn.Module\n",
    "    batch_idxs: List[int],     # 예: [0,1,2,3,...,B-1]\n",
    "    L1_threshold=0.5,\n",
    "    n_iter=30,\n",
    "    step_size=40\n",
    "):\n",
    "    # 1) 토크나이저로 한 번에 batch tokenization\n",
    "    class_seqs = [ tech_dataset.X_class[i] for i in batch_idxs ]\n",
    "    claim_seqs = [ tech_dataset.X_claim[i] for i in batch_idxs ]\n",
    "    input_classes = torch.tensor(\n",
    "        tech_dataset.tokenizers[\"class_enc\"].encode_batch(class_seqs)\n",
    "    ).to(model.device)               # (B, seq_len)\n",
    "    input_claims = tech_dataset.tokenize_batch(\n",
    "        tech_dataset.tokenizers[\"claim_enc\"], claim_seqs\n",
    "    )                                # dict of (B, ...)\n",
    "    input_inf = to_device(\n",
    "        {\"class\": input_classes, \"claim\": input_claims},\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    # 2) 한 번만 encode → z: (B, D)\n",
    "    enc_outputs, z, mu, logvar = model.encode(input_inf)\n",
    "\n",
    "    # 3) B x n_iter 만큼의 gradient step을 벡터화\n",
    "    z = z.detach().requires_grad_()\n",
    "    for _ in range(n_iter):\n",
    "        preds = model.predict(z)    # (B, 2)  예시로 binary FC 예측\n",
    "        prob1 = torch.exp(preds[:,1])   # (B,)\n",
    "        mask = prob1 >= L1_threshold\n",
    "\n",
    "        if mask.all(): break\n",
    "        # L1 loss = 1 - prob1, backward all at once\n",
    "        loss = (1 - prob1).sum()\n",
    "        loss.backward(retain_graph=True)\n",
    "        grad = z.grad                  # (B, D)\n",
    "        z = (z - grad * step_size).detach().requires_grad_()\n",
    "\n",
    "    # 4) 최종 z에 대해 한 번에 decode\n",
    "    #    model.decode_batch 이라면 훨씬 빠르겠지만, 없으면 for-loop × B\n",
    "    dec_seqs = model.decode(z, enc_outputs)  \n",
    "    dec_tokens = dec_seqs.argmax(-1).cpu().numpy()  # (B, seq_len)\n",
    "\n",
    "    # 5) 나머지 비교·집계\n",
    "    #    - ref_info 조회, breakdown, inclusion 체크 등도 batch로 묶어서 처리\n",
    "    #    - Pandas apply 대신 prefetch, pre-split, numpy/vector 연산 이용\n",
    "\n",
    "    return batch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:22:08.138009Z",
     "start_time": "2025-06-09T02:22:08.131147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Batch 활용 테스트\n",
    "model = final_model.module if torch.cuda.is_available() else final_model\n",
    "cnt_nonexist, cnt_noFC, cnt_diverge, cnt_same_ipcs, cnt_diff_ipcs = 0, 0, 0, 0, 0\n",
    "dict_out = {\"index\": [], \"patent_id\": [], \"org_text\": [], \"gen_text\": [], \"ref_ipcs\": [], \"ref_FCs\": [],\n",
    "            \"inclusions\": [], \"higher_impacts\": [], \"FC_estimated\": [], \"similar_refs\": [], \"unsimilar_refs\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:23:32.728296Z",
     "start_time": "2025-06-09T02:23:32.719675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 77950,  13897,  26645, ..., 104772, 122736, 128059])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_test_index_TC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:24:08.245221Z",
     "start_time": "2025-06-09T02:24:08.242287Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_idxs = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:28:58.101349Z",
     "start_time": "2025-06-09T02:28:58.094475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizers.Tokenizer"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_dataset.tokenizers[\"claim_enc\"].__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:28:53.254485Z",
     "start_time": "2025-06-09T02:28:53.249578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tokenizers'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(tech_dataset.tokenizers[\"claim_enc\"].__class__).split(\"\\'\")[1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:33:39.632522Z",
     "start_time": "2025-06-09T02:33:39.603293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patent_number\n",
       "7268723                                   [G01S013, G01S013]\n",
       "7508332    [H03M001, G01K007, G01K007, H03M001, H03M001, ...\n",
       "8095229    [G05B019, G06Q050, G06Q010, G16Z099, Y02P090, ...\n",
       "8825655    [G06F016, G06F016, G06F016, G06V030, G06K009, ...\n",
       "8616308                          [B62D055, B62D055, Y10S901]\n",
       "Name: patent_classes, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_dataset.X_class[used_test_index_TC][batch_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:51:34.500456Z",
     "start_time": "2025-06-09T02:51:34.350647Z"
    }
   },
   "outputs": [],
   "source": [
    "# class_seqs = [ tech_dataset.X_class[used_test_index_TC][i] for i in batch_idxs ]\n",
    "# claim_seqs = [ tech_dataset.X_claim[used_test_index_TC][i] for i in batch_idxs ]\n",
    "class_seqs = tech_dataset.X_class[used_test_index_TC][batch_idxs]\n",
    "claim_seqs = tech_dataset.X_claim[used_test_index_TC][batch_idxs]\n",
    "input_classes = torch.tensor(\n",
    "    tech_dataset.tokenizers[\"class_enc\"].encode_batch(class_seqs)\n",
    ").to(model.device)               # (B, seq_len)\n",
    "\n",
    "tokenized_claim = tech_dataset.tokenizers[\"claim_enc\"].encode_batch(claim_seqs)\n",
    "tokenized_dict_claim = {\"input_ids\": torch.tensor([tokenized.ids for tokenized in tokenized_claim], dtype=torch.long),\n",
    "                           \"attention_mask\": torch.tensor([tokenized.attention_mask for tokenized in tokenized_claim], dtype=torch.long)}\n",
    "input_claims = tokenized_dict_claim\n",
    "# input_claims = {k: v.unsqueeze(0) for k, v in tokenized_dict_claim.items()}\n",
    "\n",
    "input_inf = to_device(\n",
    "    {\"class\": input_classes, \"claim\": input_claims},\n",
    "    model.device\n",
    ")\n",
    "\n",
    "output_class = torch.tensor(\n",
    "    tech_dataset.tokenizers[\"class_enc\"].encode_batch(class_seqs)\n",
    ").to(model.device)               # (B, seq_len)\n",
    "batch_output = {\"text_outputs\": torch.tensor(output_class)}\n",
    "output_inf = to_device(batch_output, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:51:54.536243Z",
     "start_time": "2025-06-09T02:51:54.388613Z"
    }
   },
   "outputs": [],
   "source": [
    "enc_outputs, z, mu, logvar = model.encode(input_inf)\n",
    "org_z = copy.deepcopy(z.view(1,-1).cpu().detach().numpy())\n",
    "pred_outputs = model.predict(z)\n",
    "org_y = copy.deepcopy(pred_outputs.argmax(1).cpu().detach().numpy())\n",
    "dec_inputs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_nonexist = 0\n",
    "cnt_noFC = 0\n",
    "cnt_diverge = 0\n",
    "cnt_same_ipcs = 0\n",
    "cnt_diff_ipcs = 0\n",
    "\n",
    "input_class = torch.tensor(tech_dataset.tokenizers[\"class_enc\"].encode(tech_dataset.X_class[used_test_index_TC][idx])).unsqueeze(0)\n",
    "input_claim = tech_dataset.tokenize(tech_dataset.tokenizers[\"claim_enc\"], tech_dataset.X_claim[used_test_index_TC][idx])\n",
    "input_claim = {k: v.unsqueeze(0) for k, v in input_claim.items()}\n",
    "batch_input = {\"class\": torch.tensor(input_class), \"claim\": input_claim}\n",
    "input_inf = to_device(batch_input, model.device)\n",
    "\n",
    "output_class = torch.tensor(tech_dataset.tokenizers[\"class_dec\"].encode(tech_dataset.X_class[used_test_index_TC][idx])).unsqueeze(0)\n",
    "batch_output = {\"text_outputs\": torch.tensor(output_class)}\n",
    "output_inf = to_device(batch_output, model.device)\n",
    "\n",
    "enc_outputs, z, mu, logvar = model.encode(input_inf)\n",
    "org_z = copy.deepcopy(z.view(1,-1).cpu().detach().numpy())\n",
    "pred_outputs = model.predict(z)\n",
    "org_y = copy.deepcopy(pred_outputs.argmax(1).cpu().detach().numpy())\n",
    "dec_inputs = None\n",
    "\n",
    "if used_test_data_TC.iloc[idx][\"TC5\"] > 0:\n",
    "    forward_refs = used_rawdata.loc[used_test_data_TC.iloc[idx][\"patent_number\"]][\"forward_refs\"].split(\";\")\n",
    "    ref_info = total_data.loc[[ref for ref in forward_refs if ref in total_data.index]]\n",
    "    if len(ref_info) == 0:\n",
    "        cnt_nonexist += 1\n",
    "        return (cnt_nonexist, cnt_noFC, cnt_diverge, cnt_same_ipcs, cnt_diff_ipcs), None\n",
    "    else:\n",
    "        ref_ipcs = ref_info[\"patent_classes\"].apply(lambda x: set(x))\n",
    "        ref_FCs = ref_info[\"TC\"+str(n_TC)]\n",
    "\n",
    "        tokenizer = tech_dataset.tokenizers[\"class_dec\"]\n",
    "\n",
    "        org_text = tokenizer.decode_batch(input_class.cpu().detach().numpy())[0]\n",
    "        org_text = org_text[org_text.index(tokenizer.sos_token)+1:org_text.index(tokenizer.eos_token)]\n",
    "        if set(org_text)==set(np.concatenate(ref_ipcs.apply(lambda x: list(x)).values)):\n",
    "            cnt_same_ipcs += 1\n",
    "\n",
    "        inclusions = [None, None, None, None]\n",
    "        higher_impacts = [None, None, None, None]\n",
    "        similar_refs_out = [None, None, None, None]\n",
    "        unsimilar_refs_out = [None, None, None, None]\n",
    "        optimised = False\n",
    "        for i in range(n_iter):\n",
    "            pred_outputs = model.predict(z)\n",
    "            z.retain_grad()\n",
    "            FC_estimated = np.round(np.exp(pred_outputs[0,1].item()), 4) # estimated forward citations\n",
    "            FC_estimated_inv = np.round(np.exp(pred_outputs[0,0].item()), 4)\n",
    "\n",
    "            L1_error = (1-torch.exp(pred_outputs[0,1]))\n",
    "            L1_error.backward(retain_graph=True)\n",
    "\n",
    "            grad_for_update = (step_size * z.grad)\n",
    "            z_ = z - grad_for_update\n",
    "\n",
    "            z.grad.zero_()\n",
    "            dec_outputs = model.decode(z_, enc_outputs, dec_inputs=None)\n",
    "            dec_outputs = dec_outputs.argmax(-1)\n",
    "\n",
    "            tokenizer = tech_dataset.tokenizers[\"class_dec\"]\n",
    "            gen_text = tokenizer.decode_batch(dec_outputs.cpu().detach().numpy())[0]\n",
    "            if tokenizer.eos_token in gen_text:\n",
    "                gen_text = gen_text[gen_text.index(tokenizer.sos_token)+1:gen_text.index(tokenizer.eos_token)]\n",
    "            else:\n",
    "                gen_text = gen_text[gen_text.index(tokenizer.sos_token)+1:]\n",
    "            if gen_text != []:\n",
    "                gen_text = [gen_text[0]] + list(np.array(gen_text[1:])[np.unique(gen_text[1:], return_index=True)[1]])                \n",
    "                gen_text = set(gen_text)\n",
    "            else: continue\n",
    "\n",
    "            if FC_estimated>=L1_threshold:\n",
    "                optimised = True\n",
    "\n",
    "                gen_text_breakdown = breakdown(gen_text)\n",
    "                ref_ipcs_breakdown = (ref_ipcs.apply(lambda x: breakdown(x)[0]), ref_ipcs.apply(lambda x: breakdown(x)[1]), ref_ipcs.apply(lambda x: breakdown(x)[2]), ref_ipcs)\n",
    "\n",
    "                for i in range(4):\n",
    "                    if inclusions[i] is not None: continue\n",
    "                    temp_gen_text = gen_text_breakdown[i]\n",
    "                    temp_ref_ipcs = ref_ipcs_breakdown[i]\n",
    "\n",
    "                    hit_index = temp_ref_ipcs.apply(lambda x: 1 if set(x)==set(temp_gen_text) else 0)==1\n",
    "                    similar_refs = temp_ref_ipcs[hit_index].index\n",
    "                    similar_refs_out[i] = similar_refs\n",
    "                    unsimilar_refs = temp_ref_ipcs[~hit_index].index\n",
    "                    unsimilar_refs_out[i] = unsimilar_refs\n",
    "                    if len(similar_refs) == 0:\n",
    "                        inclusions[i] = 0\n",
    "                        higher_impacts[i] = None\n",
    "                    elif len(unsimilar_refs) == 0:\n",
    "                        inclusions[i] = 1\n",
    "                        similar_mean_FC = np.mean(ref_FCs.loc[similar_refs])\n",
    "                        if similar_mean_FC <= 0:\n",
    "                            higher_impacts[i] = 0\n",
    "                        else:\n",
    "                            higher_impacts[i] = 1\n",
    "                    else:\n",
    "                        inclusions[i] = 1\n",
    "                        similar_mean_FC = np.mean(ref_FCs.loc[similar_refs])\n",
    "                        unsimilar_mean_FC = np.mean(ref_FCs.loc[unsimilar_refs])\n",
    "                        if similar_mean_FC >= unsimilar_mean_FC:\n",
    "                            if similar_mean_FC <= 0:\n",
    "                                higher_impacts[i] = None\n",
    "                            else:\n",
    "                                higher_impacts[i] = 1\n",
    "                        else:\n",
    "                            higher_impacts[i] = 0\n",
    "                if None not in inclusions:\n",
    "                    break\n",
    "            z = z_\n",
    "\n",
    "        if optimised:\n",
    "            cnt_diff_ipcs += 1\n",
    "            return (cnt_nonexist, cnt_noFC, cnt_diverge, cnt_same_ipcs, cnt_diff_ipcs), {\"index\": idx, \"patent_id\": used_test_data_TC.iloc[idx][\"patent_number\"], \n",
    "                     \"org_text\": org_text, \"gen_text\": gen_text, \"ref_ipcs\": ref_ipcs, \"ref_FCs\": ref_FCs,\n",
    "                     \"inclusions\": inclusions, \"higher_impacts\": higher_impacts, \n",
    "                     \"FC_estimated\": FC_estimated,\n",
    "                     \"similar_refs\": similar_refs_out, \"unsimilar_refs\": unsimilar_refs_out}\n",
    "        else:\n",
    "            cnt_diverge += 1\n",
    "            return (cnt_nonexist, cnt_noFC, cnt_diverge, cnt_same_ipcs, cnt_diff_ipcs), None\n",
    "else:\n",
    "    pass\n",
    "    cnt_noFC += 1\n",
    "    return (cnt_nonexist, cnt_noFC, cnt_diverge, cnt_same_ipcs, cnt_diff_ipcs), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load computed dict_out (validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:53:31.974959Z",
     "start_time": "2025-06-08T19:53:31.965512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-07_1732_thre0.6\n"
     ]
    }
   ],
   "source": [
    "load_dict_out = False\n",
    "save_dict_out = True\n",
    "L1_threshold = 0.6 # or None\n",
    "n_iter = 30\n",
    "step_size = 40\n",
    "analysis_config = analysis_date + \"_thre\" + str(L1_threshold)\n",
    "print(analysis_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-08T19:53:33.672Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 925/9920 [05:32<1:01:13,  2.45it/s]"
     ]
    }
   ],
   "source": [
    "if load_dict_out:\n",
    "    with open(\"../results/validation/\"+analysis_config+\"/dict_out.pickle\", \"rb\") as f:\n",
    "        dict_out = pickle.load(f)\n",
    "    print(\"dict_out loaded\")\n",
    "else:\n",
    "    model = final_model.module if torch.cuda.is_available() else final_model\n",
    "    cnt_nonexist, cnt_noFC, cnt_diverge, cnt_same_ipcs, cnt_diff_ipcs = 0, 0, 0, 0, 0\n",
    "    dict_out = {\"index\": [], \"patent_id\": [], \"org_text\": [], \"gen_text\": [], \"ref_ipcs\": [], \"ref_FCs\": [],\n",
    "                \"inclusions\": [], \"higher_impacts\": [], \"FC_estimated\": [], \"similar_refs\": [], \"unsimilar_refs\": []}\n",
    "    for idx in tqdm(range(len(used_test_index_TC))):\n",
    "#     for idx in tqdm(range(500)):\n",
    "        cnts, results = validate_reliability(model=model, idx=idx, L1_threshold=L1_threshold, n_iter=n_iter, step_size=step_size)\n",
    "        cnt_nonexist += cnts[0]\n",
    "        cnt_noFC += cnts[1]\n",
    "        cnt_diverge += cnts[2]\n",
    "        cnt_same_ipcs += cnts[3]\n",
    "        cnt_diff_ipcs += cnts[4]\n",
    "        if results is not None:\n",
    "            for k,v in results.items():\n",
    "                dict_out[k].append(v)\n",
    "    for k, v in dict_out.items():\n",
    "        dict_out[k] = np.array(v)\n",
    "    dict_out[\"cnts\"] = {\"cnt_nonexist\": cnt_nonexist, \"cnt_noFC\": cnt_noFC, \n",
    "                \"cnt_diverge\": cnt_diverge, \"cnt_same_ipcs\": cnt_same_ipcs, \"cnt_diff_ipcs\": cnt_diff_ipcs}\n",
    "    \n",
    "    if save_dict_out:\n",
    "        with open(\"../results/validation/\"+analysis_config+\"/dict_out.pickle\", \"wb\") as f:\n",
    "            pickle.dump(dict_out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T18:35:52.821969Z",
     "start_time": "2025-06-08T18:35:52.810826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH\n"
     ]
    }
   ],
   "source": [
    "print(\"FINISH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T18:50:08.083710Z",
     "start_time": "2025-06-08T18:50:08.062564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cnt_nonexist': 2983,\n",
       " 'cnt_noFC': 0,\n",
       " 'cnt_diverge': 12,\n",
       " 'cnt_same_ipcs': 618,\n",
       " 'cnt_diff_ipcs': 6925}"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_out[\"cnts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:12:57.794453Z",
     "start_time": "2025-06-08T19:12:57.716223Z"
    }
   },
   "outputs": [],
   "source": [
    "ipcs_comparison = pd.concat([pd.Series(dict_out[\"org_text\"]).apply(lambda x: set(x)), pd.Series(dict_out[\"gen_text\"]).apply(lambda x: set(x))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:12:58.392565Z",
     "start_time": "2025-06-08T19:12:58.299916Z"
    }
   },
   "outputs": [],
   "source": [
    "n_differentiated = ipcs_comparison.apply(lambda x: 1 if x[0]!=x[1] else 0, axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:12:58.942222Z",
     "start_time": "2025-06-08T19:12:58.932072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8990613718411552"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_differentiated / len(ipcs_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:13:00.628763Z",
     "start_time": "2025-06-08T19:13:00.600370Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#total: 9920\n",
      "#valid data: 6925\n",
      "#nonexist data: 2983\n",
      "#no forward citations: 0\n",
      "#diverged: 12\n",
      "#same ipcs: 618\n",
      "\n",
      "\n",
      "for level 2, Ratio generated IPCs are included in citing patents: 0.3534\n",
      "for level 2, Ratio generated IPCs have higher impact than other citing patents: 0.5359\n",
      "\n",
      "\n",
      "for level 3, Ratio generated IPCs are included in citing patents: 0.1544\n",
      "for level 3, Ratio generated IPCs have higher impact than other citing patents: 0.5068\n",
      "\n",
      "\n",
      "for level 4, Ratio generated IPCs are included in citing patents: 0.055\n",
      "for level 4, Ratio generated IPCs have higher impact than other citing patents: 0.4797\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"#total:\",len(used_test_index_TC))\n",
    "print(\"#valid data:\",len(dict_out[\"index\"]))\n",
    "print(\"#nonexist data:\",dict_out[\"cnts\"][\"cnt_nonexist\"])\n",
    "print(\"#no forward citations:\",dict_out[\"cnts\"][\"cnt_noFC\"])\n",
    "print(\"#diverged:\",dict_out[\"cnts\"][\"cnt_diverge\"])\n",
    "print(\"#same ipcs:\",dict_out[\"cnts\"][\"cnt_same_ipcs\"])\n",
    "print(\"\\n\")\n",
    "for i in range(1,4):\n",
    "    inclusions = np.array(dict_out[\"inclusions\"])[:,i]\n",
    "    ratio_included = np.round(len(inclusions[inclusions==1])/len(inclusions), 4)\n",
    "    higher_impacts = np.array(dict_out[\"higher_impacts\"])[:,i][np.array(dict_out[\"higher_impacts\"])[:,i] != None]\n",
    "    ratio_higher_impact = np.round(len(higher_impacts[higher_impacts==1])/len(higher_impacts), 4)\n",
    "    print(f\"for level {i+1}, Ratio generated IPCs are included in citing patents: {ratio_included}\")\n",
    "    print(f\"for level {i+1}, Ratio generated IPCs have higher impact than other citing patents: {ratio_higher_impact}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:13:20.980421Z",
     "start_time": "2025-06-08T19:13:20.971019Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_inclusions = np.array(dict_out[\"inclusions\"])[np.array(dict_out[\"inclusions\"])[:,-1]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:13:26.255783Z",
     "start_time": "2025-06-08T19:13:26.241158Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for level 1, Hit ratio when level 4 is not hit: 0.5122\n",
      "for level 2, Hit ratio when level 4 is not hit: 0.6843\n",
      "for level 3, Hit ratio when level 4 is not hit: 0.8949\n"
     ]
    }
   ],
   "source": [
    "temp_inclusions = np.array(dict_out[\"inclusions\"])[np.array(dict_out[\"inclusions\"])[:,-1]==0]\n",
    "for i in range(3):\n",
    "    temp_ratio = len(temp_inclusions[temp_inclusions[:,i]==0]) / len(temp_inclusions)\n",
    "    print(f\"for level {i+1}, Hit ratio when level 4 is not hit: {np.round(temp_ratio,4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:13:27.066048Z",
     "start_time": "2025-06-08T19:13:27.056025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133654, 8)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:13:42.190306Z",
     "start_time": "2025-06-08T19:13:29.969281Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/glee/.conda/envs/DL/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3702: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home2/glee/.conda/envs/DL/lib/python3.9/site-packages/numpy/core/_methods.py:253: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of forward citations (hit)\n",
      " count    1055.000000\n",
      "mean       19.375355\n",
      "std        52.841582\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         3.000000\n",
      "75%        11.000000\n",
      "max       843.000000\n",
      "dtype: float64 \n",
      "\n",
      "Distribution of forward citations (not hit)\n",
      " count    4775.000000\n",
      "mean       21.712251\n",
      "std        42.760047\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         4.000000\n",
      "75%        19.000000\n",
      "max       481.000000\n",
      "dtype: float64 \n",
      "\n",
      "Distribution of mean forward citations (hit)\n",
      " count    381.000000\n",
      "mean      10.089861\n",
      "std       24.950830\n",
      "min        0.000000\n",
      "25%        1.000000\n",
      "50%        3.000000\n",
      "75%        8.000000\n",
      "max      233.000000\n",
      "dtype: float64 \n",
      "\n",
      "Distribution of mean forward citations (not hit)\n",
      " count    381.000000\n",
      "mean       9.367657\n",
      "std       18.130869\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        3.250000\n",
      "75%       11.000000\n",
      "max      137.148148\n",
      "dtype: float64 \n",
      "\n",
      "Distribution of difference of forward citations\n",
      " count    381.000000\n",
      "mean       0.722204\n",
      "std       20.721175\n",
      "min     -106.000000\n",
      "25%       -3.700000\n",
      "50%        0.000000\n",
      "75%        3.166667\n",
      "max      232.500000\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "whole_patent_classes = tech_dataset.data[\"patent_classes\"].apply(lambda x: set(x))\n",
    "\n",
    "hit_similar_FCs, hit_unsimilar_FCs, hit_diff_FCs = [], [], []\n",
    "hit_similar_FCs_mean, hit_unsimilar_FCs_mean = [], []\n",
    "hit_similar_FCs_rank = []\n",
    "whole_FC_ttest = {\"statistic\": [], \"pvalue\": []}\n",
    "whole_FCs_diff = []\n",
    "hit_samples_index = dict_out[\"inclusions\"][:,-1]==1\n",
    "\n",
    "hit_patent_ids = dict_out[\"patent_id\"][hit_samples_index]\n",
    "hit_similar_refs = dict_out[\"similar_refs\"][hit_samples_index][:,-1]\n",
    "hit_unsimilar_refs = dict_out[\"unsimilar_refs\"][hit_samples_index][:,-1]\n",
    "\n",
    "for i in range(len(hit_patent_ids)):\n",
    "    hit_FCs = dict_out[\"ref_FCs\"][hit_samples_index][i]\n",
    "    hit_FCs = hit_FCs.loc[~hit_FCs.index.duplicated(keep=\"first\")]\n",
    "    hit_similar_FC = hit_FCs.loc[hit_similar_refs[i].drop_duplicates()]\n",
    "    hit_similar_FC_rank = hit_FCs.rank(pct=True).loc[hit_similar_refs[i].drop_duplicates()]\n",
    "    hit_unsimilar_FC = hit_FCs.loc[hit_unsimilar_refs[i].drop_duplicates()]\n",
    "    if len(hit_unsimilar_FC)>0:\n",
    "        hit_diff_FC = hit_similar_FC.mean() - hit_unsimilar_FC.mean()\n",
    "    else:\n",
    "        hit_diff_FC = hit_similar_FC.mean()\n",
    "    \n",
    "    org_whole_FC = tech_dataset.data.loc[whole_patent_classes[whole_patent_classes==set(dict_out[\"org_text\"][hit_samples_index][i])].index][\"TC5\"]\n",
    "    gen_whole_FC = tech_dataset.data.loc[whole_patent_classes[whole_patent_classes==set(dict_out[\"gen_text\"][hit_samples_index][i])].index][\"TC5\"]\n",
    "    if len(org_whole_FC)>0:\n",
    "        whole_FC_diff = gen_whole_FC.mean() - org_whole_FC.mean()\n",
    "    else:\n",
    "        whole_FC_diff = gen_whole_FC.mean()\n",
    "    \n",
    "    hit_similar_FCs.append(hit_similar_FC)\n",
    "    hit_similar_FCs_rank.append(hit_similar_FC_rank)\n",
    "    hit_similar_FCs_mean.append(hit_similar_FC.mean())\n",
    "    hit_unsimilar_FCs.append(hit_unsimilar_FC)\n",
    "    if len(hit_unsimilar_FC)>0:\n",
    "        hit_unsimilar_FCs_mean.append(hit_unsimilar_FC.mean())\n",
    "    else:\n",
    "        hit_unsimilar_FCs_mean.append(0)\n",
    "    hit_diff_FCs.append(hit_diff_FC)\n",
    "    \n",
    "    ttest_res = ttest_ind(gen_whole_FC, org_whole_FC, equal_var=False)\n",
    "    \n",
    "    whole_FC_ttest[\"statistic\"].append(ttest_res.statistic)\n",
    "    whole_FC_ttest[\"pvalue\"].append(ttest_res.pvalue)\n",
    "    if set(dict_out[\"org_text\"][hit_samples_index][i]) != set(dict_out[\"gen_text\"][hit_samples_index][i]):\n",
    "        whole_FCs_diff.append(whole_FC_diff)\n",
    "    \n",
    "hit_similar_FCs = np.concatenate(hit_similar_FCs)\n",
    "hit_similar_FCs_rank = np.concatenate(hit_similar_FCs_rank)\n",
    "hit_similar_FCs_mean = np.array(hit_similar_FCs_mean)\n",
    "hit_unsimilar_FCs = np.concatenate(hit_unsimilar_FCs)\n",
    "hit_unsimilar_FCs_mean = np.array(hit_unsimilar_FCs_mean)\n",
    "hit_diff_FCs = np.array(hit_diff_FCs)\n",
    "\n",
    "print(\"Distribution of forward citations (hit)\\n\",pd.Series(hit_similar_FCs).describe(),\"\\n\")\n",
    "print(\"Distribution of forward citations (not hit)\\n\",pd.Series(hit_unsimilar_FCs).describe(),\"\\n\")\n",
    "print(\"Distribution of mean forward citations (hit)\\n\",pd.Series(hit_similar_FCs_mean).describe(),\"\\n\")\n",
    "print(\"Distribution of mean forward citations (not hit)\\n\",pd.Series(hit_unsimilar_FCs_mean).describe(),\"\\n\")\n",
    "print(\"Distribution of difference of forward citations\\n\",pd.Series(hit_diff_FCs).describe(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T16:53:54.675989Z",
     "start_time": "2025-06-08T16:53:54.665441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8108343, 8032383, 8024799, 8819826, 8355610, 7010095, 8977643,\n",
       "       7242681, 7085753, 7076481, 7181459, 8059265, 8571618, 8849670,\n",
       "       7185000, 7062083, 7243093, 7401069, 8886540, 8797295, 7440981])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_out[\"patent_id\"][hit_samples_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:38:04.885308Z",
     "start_time": "2025-06-08T19:38:04.868057Z"
    }
   },
   "outputs": [],
   "source": [
    "cnt_same, cnt_diff = 0, 0\n",
    "ranks_same, ranks_diff = [], []\n",
    "value_same, value_diff = [], []\n",
    "org_patents_same, org_patents_diff = [], []\n",
    "cols_val = [\"patent_id\", \"org_ipcs\", \"org_FC\", \"gen_ipcs\", \"is_same\", \n",
    "            \"forward_ref\", \"ref_ipcs\", \"ref_FC\", \"ref_FC_rank\"]\n",
    "df_val = pd.DataFrame(columns=cols_val)\n",
    "hit_similar_refs = dict_out[\"similar_refs\"][hit_samples_index][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:38:43.663943Z",
     "start_time": "2025-06-08T19:38:43.641988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patent_number\n",
       "10236013    {G10L013, H04W004, G06F003, H04M001, G10L025, ...\n",
       "8135700                                    {G06F016, Y10S715}\n",
       "8135700                                    {G06F016, Y10S715}\n",
       "9959028     {E21B044, G06F003, E21B047, E21B049, H04L067, ...\n",
       "10236011    {H04R001, G10L013, H04W004, G06F003, H04M001, ...\n",
       "10236012    {H04R001, G10L013, H04W004, G06F003, H04M001, ...\n",
       "10311887    {H04R001, G10L013, G06F003, H04W004, H04M001, ...\n",
       "9509269                           {G11B020, G11B027, H03G003}\n",
       "10133460                 {E21B047, H04L067, G06F003, H04L069}\n",
       "10391361                 {G06F016, A63B022, A63B024, A63B071}\n",
       "10410649    {H04R001, G10L013, H04W004, G06F003, H04M001, ...\n",
       "10297265    {H04R001, G10L013, G06F003, H04W004, G10L025, ...\n",
       "8135736                                    {G06F016, Y10S715}\n",
       "8135736                                    {G06F016, Y10S715}\n",
       "10416666        {B64C039, G06F003, H04L065, G05D001, G06Q010}\n",
       "9576050                                             {G06F016}\n",
       "Name: patent_classes, dtype: object"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_out[\"ref_ipcs\"][hit_samples_index][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:38:57.234572Z",
     "start_time": "2025-06-08T19:38:57.222288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7542816"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_out[\"patent_id\"][hit_samples_index][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:39:18.156027Z",
     "start_time": "2025-06-08T19:39:18.146222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['6983309', '6982420', '6983071', '6982717', '6983073', '6983051',\n",
       "       '6983229', '6983242', '6983320', '6982659',\n",
       "       ...\n",
       "       '9226105', '9223299', '9226051', '9225639', '9225549', '9223620',\n",
       "       '9223405', '9225764', '9226068', '9223929'],\n",
       "      dtype='object', name='patent_number', length=133654)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_dataset.data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:51:35.677585Z",
     "start_time": "2025-06-08T19:51:35.667075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381,)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_out[\"patent_id\"][hit_samples_index].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:40:46.883799Z",
     "start_time": "2025-06-08T19:40:41.886033Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [00:04<00:00, 76.79it/s] \n"
     ]
    }
   ],
   "source": [
    "cnt_same, cnt_diff = 0, 0\n",
    "ranks_same, ranks_diff = [], []\n",
    "value_same, value_diff = [], []\n",
    "org_patents_same, org_patents_diff = [], []\n",
    "cols_val = [\"patent_id\", \"org_ipcs\", \"org_FC\", \"gen_ipcs\", \"is_same\", \n",
    "            \"forward_ref\", \"ref_ipcs\", \"ref_FC\", \"ref_FC_rank\"]\n",
    "df_val = pd.DataFrame(columns=cols_val)\n",
    "hit_similar_refs = dict_out[\"similar_refs\"][hit_samples_index][:,-1]\n",
    "\n",
    "for i in tqdm(range(len(dict_out[\"ref_ipcs\"][hit_samples_index]))):\n",
    "    pid = dict_out[\"patent_id\"][hit_samples_index][i]\n",
    "    pid = str(pid)\n",
    "    org_FC = tech_dataset.data.loc[pid][\"TC5\"]\n",
    "    orgs = set(dict_out[\"org_text\"][dict_out[\"inclusions\"][:,-1]==1][i])\n",
    "    gens = set(dict_out[\"gen_text\"][dict_out[\"inclusions\"][:,-1]==1][i])\n",
    "    is_same = 1 if orgs==gens or orgs.union(gens)==orgs else 0\n",
    "    hit_FCs = dict_out[\"ref_FCs\"][hit_samples_index][i]\n",
    "    hit_FCs = hit_FCs.loc[~hit_FCs.index.duplicated(keep=\"first\")]\n",
    "    hit_similar_FC = hit_FCs.loc[hit_similar_refs[i][hit_similar_refs[i].duplicated()]]\n",
    "#     hit_similar_FC = hit_similar_FC.drop_duplicates()\n",
    "    hit_similar_FC_rank = hit_FCs.rank(pct=True).loc[hit_similar_refs[i][hit_similar_refs[i].duplicated()]]\n",
    "#     hit_similar_FC_rank = hit_similar_FC_rank.drop_duplicates()\n",
    "    \n",
    "    for ref in hit_similar_refs[i].drop_duplicates():\n",
    "        ref_ipcs = dict_out[\"ref_ipcs\"][hit_samples_index][i].loc[ref]\n",
    "        if isinstance(ref_ipcs, pd.Series): ref_ipcs = ref_ipcs[0]\n",
    "        hit_similar_FC = hit_FCs.loc[ref]\n",
    "        if isinstance(hit_similar_FC, pd.Series): hit_similar_FC = hit_similar_FC[0]\n",
    "        hit_similar_FC_rank = hit_FCs.rank(pct=True).loc[ref]\n",
    "        if isinstance(hit_similar_FC_rank, pd.Series): hit_similar_FC_rank = hit_similar_FC_rank[0]\n",
    "        \n",
    "        df_container = pd.DataFrame([[pid, orgs, org_FC, gens, is_same, ref, ref_ipcs, hit_similar_FC, hit_similar_FC_rank]], columns=cols_val)\n",
    "        df_val = pd.concat([df_val, df_container])\n",
    "\n",
    "df_val = df_val.set_index(\"patent_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:40:47.919735Z",
     "start_time": "2025-06-08T19:40:47.854921Z"
    }
   },
   "outputs": [],
   "source": [
    "df_val.loc[:,\"ref_FC_new\"] = df_val.apply(lambda x: str(x[\"ref_FC\"])+\" (\"+str(np.round(x[\"ref_FC_rank\"],2))+\")\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:40:48.563643Z",
     "start_time": "2025-06-08T19:40:48.526423Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_ipcs</th>\n",
       "      <th>org_FC</th>\n",
       "      <th>gen_ipcs</th>\n",
       "      <th>is_same</th>\n",
       "      <th>forward_ref</th>\n",
       "      <th>ref_ipcs</th>\n",
       "      <th>ref_FC</th>\n",
       "      <th>ref_FC_rank</th>\n",
       "      <th>ref_FC_new</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patent_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7542816</th>\n",
       "      <td>{Y10S707, G06F016, G11B027}</td>\n",
       "      <td>18</td>\n",
       "      <td>{G06F016}</td>\n",
       "      <td>1</td>\n",
       "      <td>9576050</td>\n",
       "      <td>{G06F016}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>6 (0.82)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8108343</th>\n",
       "      <td>{G06F016}</td>\n",
       "      <td>11</td>\n",
       "      <td>{G06F016}</td>\n",
       "      <td>1</td>\n",
       "      <td>10331657</td>\n",
       "      <td>{G06F016}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2 (0.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8032383</th>\n",
       "      <td>{G10L015}</td>\n",
       "      <td>51</td>\n",
       "      <td>{G10L015}</td>\n",
       "      <td>1</td>\n",
       "      <td>9858925</td>\n",
       "      <td>{G10L015}</td>\n",
       "      <td>35</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>35 (0.82)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8032383</th>\n",
       "      <td>{G10L015}</td>\n",
       "      <td>51</td>\n",
       "      <td>{G10L015}</td>\n",
       "      <td>1</td>\n",
       "      <td>10395654</td>\n",
       "      <td>{G10L015}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>3 (0.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8032383</th>\n",
       "      <td>{G10L015}</td>\n",
       "      <td>51</td>\n",
       "      <td>{G10L015}</td>\n",
       "      <td>1</td>\n",
       "      <td>10079014</td>\n",
       "      <td>{G10L015}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107407</td>\n",
       "      <td>0 (0.11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7818341</th>\n",
       "      <td>{G06F016}</td>\n",
       "      <td>4</td>\n",
       "      <td>{G06F016}</td>\n",
       "      <td>1</td>\n",
       "      <td>9135357</td>\n",
       "      <td>{G06F016}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0 (0.75)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7287033</th>\n",
       "      <td>{Y10S707, G06F016}</td>\n",
       "      <td>13</td>\n",
       "      <td>{Y10S707, G06F016}</td>\n",
       "      <td>1</td>\n",
       "      <td>8065308</td>\n",
       "      <td>{Y10S707, G06F016}</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7 (1.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7287033</th>\n",
       "      <td>{Y10S707, G06F016}</td>\n",
       "      <td>13</td>\n",
       "      <td>{Y10S707, G06F016}</td>\n",
       "      <td>1</td>\n",
       "      <td>8489597</td>\n",
       "      <td>{Y10S707, G06F016}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0 (0.2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7287033</th>\n",
       "      <td>{Y10S707, G06F016}</td>\n",
       "      <td>13</td>\n",
       "      <td>{Y10S707, G06F016}</td>\n",
       "      <td>1</td>\n",
       "      <td>7472140</td>\n",
       "      <td>{Y10S707, G06F016}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>2 (0.6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7496557</th>\n",
       "      <td>{Y10S707, G06F016}</td>\n",
       "      <td>3</td>\n",
       "      <td>{Y10S707, G06F016}</td>\n",
       "      <td>1</td>\n",
       "      <td>8041705</td>\n",
       "      <td>{Y10S707, G06F016}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1 (1.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1055 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              org_ipcs org_FC            gen_ipcs is_same  \\\n",
       "patent_id                                                                   \n",
       "7542816    {Y10S707, G06F016, G11B027}     18           {G06F016}       1   \n",
       "8108343                      {G06F016}     11           {G06F016}       1   \n",
       "8032383                      {G10L015}     51           {G10L015}       1   \n",
       "8032383                      {G10L015}     51           {G10L015}       1   \n",
       "8032383                      {G10L015}     51           {G10L015}       1   \n",
       "...                                ...    ...                 ...     ...   \n",
       "7818341                      {G06F016}      4           {G06F016}       1   \n",
       "7287033             {Y10S707, G06F016}     13  {Y10S707, G06F016}       1   \n",
       "7287033             {Y10S707, G06F016}     13  {Y10S707, G06F016}       1   \n",
       "7287033             {Y10S707, G06F016}     13  {Y10S707, G06F016}       1   \n",
       "7496557             {Y10S707, G06F016}      3  {Y10S707, G06F016}       1   \n",
       "\n",
       "          forward_ref            ref_ipcs ref_FC  ref_FC_rank ref_FC_new  \n",
       "patent_id                                                                 \n",
       "7542816       9576050           {G06F016}      6     0.821429   6 (0.82)  \n",
       "8108343      10331657           {G06F016}      2     0.500000    2 (0.5)  \n",
       "8032383       9858925           {G10L015}     35     0.822222  35 (0.82)  \n",
       "8032383      10395654           {G10L015}      3     0.400000    3 (0.4)  \n",
       "8032383      10079014           {G10L015}      0     0.107407   0 (0.11)  \n",
       "...               ...                 ...    ...          ...        ...  \n",
       "7818341       9135357           {G06F016}      0     0.750000   0 (0.75)  \n",
       "7287033       8065308  {Y10S707, G06F016}      7     1.000000    7 (1.0)  \n",
       "7287033       8489597  {Y10S707, G06F016}      0     0.200000    0 (0.2)  \n",
       "7287033       7472140  {Y10S707, G06F016}      2     0.600000    2 (0.6)  \n",
       "7496557       8041705  {Y10S707, G06F016}      1     1.000000    1 (1.0)  \n",
       "\n",
       "[1055 rows x 9 columns]"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:40:59.454242Z",
     "start_time": "2025-06-08T19:40:59.421881Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total hit: 1055\n",
      "same: 1012\n",
      "diff: 43\n",
      "over L1 criterion: 7\n",
      "ratio: 0.16279069767441862\n"
     ]
    }
   ],
   "source": [
    "L1_criterion = tech_dataset.data[\"TC5\"].quantile(0.9)\n",
    "print(\"total hit:\", len(df_val))\n",
    "print(\"same:\",len(df_val[df_val[\"is_same\"]==1]))\n",
    "print(\"diff:\",len(df_val[df_val[\"is_same\"]==0]))\n",
    "print(\"over L1 criterion:\", len(df_val[df_val[\"is_same\"]==0][df_val[df_val[\"is_same\"]==0][\"ref_FC\"]>=L1_criterion]))\n",
    "print(\"ratio:\",len(df_val[df_val[\"is_same\"]==0][df_val[df_val[\"is_same\"]==0][\"ref_FC\"]>=L1_criterion]) / len(df_val[df_val[\"is_same\"]==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:41:01.454329Z",
     "start_time": "2025-06-08T19:41:01.419642Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_ipcs</th>\n",
       "      <th>org_FC</th>\n",
       "      <th>gen_ipcs</th>\n",
       "      <th>forward_ref</th>\n",
       "      <th>ref_ipcs</th>\n",
       "      <th>ref_FC_new</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patent_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8355862</th>\n",
       "      <td>{G01S019, G06F003, G01S005, G01C021, H04M001}</td>\n",
       "      <td>71</td>\n",
       "      <td>{G01C021, G06F003, G06F016}</td>\n",
       "      <td>9200915</td>\n",
       "      <td>{G01C021, G06F003, G06F016}</td>\n",
       "      <td>41 (1.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7003522</th>\n",
       "      <td>{Y10S707, G06F016}</td>\n",
       "      <td>36</td>\n",
       "      <td>{G06F040, G06F016}</td>\n",
       "      <td>9582608</td>\n",
       "      <td>{G06F040, G06F016}</td>\n",
       "      <td>15 (0.78)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8260914</th>\n",
       "      <td>{H04L061, H04L063}</td>\n",
       "      <td>132</td>\n",
       "      <td>{G06F016, H04L061, H04L063}</td>\n",
       "      <td>9306969</td>\n",
       "      <td>{G06F016, H04L061, H04L063}</td>\n",
       "      <td>16 (0.53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7328155</th>\n",
       "      <td>{G10L015}</td>\n",
       "      <td>29</td>\n",
       "      <td>{G01C021, G10L015}</td>\n",
       "      <td>8219399</td>\n",
       "      <td>{G01C021, G10L015}</td>\n",
       "      <td>29 (0.75)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7769756</th>\n",
       "      <td>{H04N021, H04N019, H04H060, H04L065}</td>\n",
       "      <td>16</td>\n",
       "      <td>{H04H060, H04N021, G06Q030}</td>\n",
       "      <td>8966525</td>\n",
       "      <td>{H04H060, H04N021, G06Q030}</td>\n",
       "      <td>37 (1.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7290266</th>\n",
       "      <td>{G06N005, G06F009, G06F021}</td>\n",
       "      <td>24</td>\n",
       "      <td>{H04L063, G06F021}</td>\n",
       "      <td>8925101</td>\n",
       "      <td>{H04L063, G06F021}</td>\n",
       "      <td>22 (0.61)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7290266</th>\n",
       "      <td>{G06N005, G06F009, G06F021}</td>\n",
       "      <td>24</td>\n",
       "      <td>{H04L063, G06F021}</td>\n",
       "      <td>8544003</td>\n",
       "      <td>{H04L063, G06F021}</td>\n",
       "      <td>40 (0.96)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                org_ipcs org_FC  \\\n",
       "patent_id                                                         \n",
       "8355862    {G01S019, G06F003, G01S005, G01C021, H04M001}     71   \n",
       "7003522                               {Y10S707, G06F016}     36   \n",
       "8260914                               {H04L061, H04L063}    132   \n",
       "7328155                                        {G10L015}     29   \n",
       "7769756             {H04N021, H04N019, H04H060, H04L065}     16   \n",
       "7290266                      {G06N005, G06F009, G06F021}     24   \n",
       "7290266                      {G06N005, G06F009, G06F021}     24   \n",
       "\n",
       "                              gen_ipcs forward_ref  \\\n",
       "patent_id                                            \n",
       "8355862    {G01C021, G06F003, G06F016}     9200915   \n",
       "7003522             {G06F040, G06F016}     9582608   \n",
       "8260914    {G06F016, H04L061, H04L063}     9306969   \n",
       "7328155             {G01C021, G10L015}     8219399   \n",
       "7769756    {H04H060, H04N021, G06Q030}     8966525   \n",
       "7290266             {H04L063, G06F021}     8925101   \n",
       "7290266             {H04L063, G06F021}     8544003   \n",
       "\n",
       "                              ref_ipcs ref_FC_new  \n",
       "patent_id                                          \n",
       "8355862    {G01C021, G06F003, G06F016}   41 (1.0)  \n",
       "7003522             {G06F040, G06F016}  15 (0.78)  \n",
       "8260914    {G06F016, H04L061, H04L063}  16 (0.53)  \n",
       "7328155             {G01C021, G10L015}  29 (0.75)  \n",
       "7769756    {H04H060, H04N021, G06Q030}   37 (1.0)  \n",
       "7290266             {H04L063, G06F021}  22 (0.61)  \n",
       "7290266             {H04L063, G06F021}  40 (0.96)  "
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 인용 IPC 중 생성 IPC와 동일한 게 있으면서, 입력 IPC와 생성 IPC가 다르고, 피인용수가 전체 데이터셋의 L1 기준 이상인 샘플\n",
    "df_val[df_val[\"is_same\"]==0][df_val[df_val[\"is_same\"]==0][\"ref_FC\"]>=L1_criterion].drop(labels=[\"is_same\", \"ref_FC\", \"ref_FC_rank\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:44:33.769061Z",
     "start_time": "2025-06-08T19:44:33.741980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patent_id\n",
       "8621259    8621259\n",
       "7188306    7188306\n",
       "7917869    7917869\n",
       "7613602    7613602\n",
       "7328155    7328155\n",
       "7769756    7769756\n",
       "7290266    7290266\n",
       "dtype: object"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 인용 IPC 중 생성 IPC와 동일하면서, 입력 IPC와는 다르고, 피인용수가 입력 특허보다 많은 샘플\n",
    "df_val[df_val[\"is_same\"]==0].drop(labels=[\"is_same\", \"ref_FC\", \"ref_FC_rank\"], axis=1).apply(lambda x: x.name if int(x[\"org_FC\"]) <= int(x[\"ref_FC_new\"].split(\"(\")[0]) else np.nan, axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T19:45:32.362069Z",
     "start_time": "2025-06-08T19:45:32.345801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org_ipcs                {Y10S707, G06F040}\n",
       "org_FC                                   3\n",
       "gen_ipcs       {Y10S707, G06F040, G06F016}\n",
       "is_same                                  0\n",
       "forward_ref                        8327260\n",
       "ref_ipcs       {G06F040, Y10S707, G06F016}\n",
       "ref_FC                                  12\n",
       "ref_FC_rank                            1.0\n",
       "ref_FC_new                        12 (1.0)\n",
       "Name: 7188306, dtype: object"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.loc[\"7188306\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## **5.1 in-depth 사례를 4.3으로 옮기기 위해, iteration 과정 추출 (231018)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "7600135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T22:40:13.087672Z",
     "start_time": "2023-10-18T22:40:13.084562Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = dict_out[\"index\"][np.where(dict_out[\"patent_id\"]==\"7600135\")[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T22:50:58.749356Z",
     "start_time": "2023-10-18T22:50:58.735930Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tech_dataset.data.iloc[used_test_index_TC[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T22:51:59.839020Z",
     "start_time": "2023-10-18T22:51:59.836084Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = final_model.module if torch.cuda.is_available() else final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T23:38:53.770068Z",
     "start_time": "2023-10-18T23:38:53.756504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_class = torch.tensor(tech_dataset.tokenizers[\"class_enc\"].encode(tech_dataset.X_class[used_test_index_TC][idx])).unsqueeze(0)\n",
    "input_claim = tech_dataset.tokenize(tech_dataset.tokenizers[\"claim_enc\"], tech_dataset.X_claim[used_test_index_TC][idx])\n",
    "input_claim = {k: v.unsqueeze(0) for k, v in input_claim.items()}\n",
    "batch_input = {\"class\": torch.tensor(input_class), \"claim\": input_claim}\n",
    "input_inf = to_device(batch_input, model.device)\n",
    "\n",
    "output_class = torch.tensor(tech_dataset.tokenizers[\"class_dec\"].encode(tech_dataset.X_class[used_test_index_TC][idx])).unsqueeze(0)\n",
    "batch_output = {\"text_outputs\": torch.tensor(output_class)}\n",
    "output_inf = to_device(batch_output, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T00:07:41.675436Z",
     "start_time": "2023-10-19T00:07:41.656636Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "forward_refs = used_rawdata.loc[used_test_data_TC.iloc[idx][\"number\"]][\"forward_refs\"].split(\";\")\n",
    "ref_info = total_data.loc[[ref for ref in forward_refs if ref in total_data.index]]\n",
    "if len(ref_info) == 0:\n",
    "    pass\n",
    "else:\n",
    "    ref_ipcs = ref_info[\"ipcs\"].apply(lambda x: set(x))\n",
    "    ref_FCs = ref_info[\"TC\"+str(n_TC)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T00:56:54.728714Z",
     "start_time": "2023-10-19T00:56:54.725676Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = dict_out[\"index\"][np.where(dict_out[\"patent_id\"]==\"7636945\")[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T00:56:55.776493Z",
     "start_time": "2023-10-19T00:56:55.771025Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tech_dataset.data.iloc[used_test_index_TC[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "7636945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T00:57:04.341623Z",
     "start_time": "2023-10-19T00:57:04.338653Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = final_model.module if torch.cuda.is_available() else final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T00:57:05.404740Z",
     "start_time": "2023-10-19T00:57:05.370008Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_class = torch.tensor(tech_dataset.tokenizers[\"class_enc\"].encode(tech_dataset.X_class[used_test_index_TC][idx])).unsqueeze(0)\n",
    "input_claim = tech_dataset.tokenize(tech_dataset.tokenizers[\"claim_enc\"], tech_dataset.X_claim[used_test_index_TC][idx])\n",
    "input_claim = {k: v.unsqueeze(0) for k, v in input_claim.items()}\n",
    "batch_input = {\"class\": torch.tensor(input_class), \"claim\": input_claim}\n",
    "input_inf = to_device(batch_input, model.device)\n",
    "\n",
    "output_class = torch.tensor(tech_dataset.tokenizers[\"class_dec\"].encode(tech_dataset.X_class[used_test_index_TC][idx])).unsqueeze(0)\n",
    "batch_output = {\"text_outputs\": torch.tensor(output_class)}\n",
    "output_inf = to_device(batch_output, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T00:57:06.298221Z",
     "start_time": "2023-10-19T00:57:06.279315Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "forward_refs = used_rawdata.loc[used_test_data_TC.iloc[idx][\"number\"]][\"forward_refs\"].split(\";\")\n",
    "ref_info = total_data.loc[[ref for ref in forward_refs if ref in total_data.index]]\n",
    "if len(ref_info) == 0:\n",
    "    pass\n",
    "else:\n",
    "    ref_ipcs = ref_info[\"ipcs\"].apply(lambda x: set(x))\n",
    "    ref_FCs = ref_info[\"TC\"+str(n_TC)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T00:57:08.679064Z",
     "start_time": "2023-10-19T00:57:07.385613Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "while 1:\n",
    "    i = 0\n",
    "    enc_outputs, z, mu, logvar = model.encode(input_inf)\n",
    "    pred_outputs = model.predict(z)\n",
    "\n",
    "    dec_outputs = model.decode(z, enc_outputs, dec_inputs=None)\n",
    "    dec_outputs = dec_outputs.argmax(-1)\n",
    "\n",
    "    tokenizer = tech_dataset.tokenizers[\"class_dec\"]\n",
    "    gen_text = tokenizer.decode_batch(dec_outputs.cpu().detach().numpy())[0]\n",
    "\n",
    "    if tokenizer.eos_token in gen_text:\n",
    "        gen_text = gen_text[gen_text.index(tokenizer.sos_token)+1:gen_text.index(tokenizer.eos_token)]\n",
    "    else:\n",
    "        gen_text = gen_text[gen_text.index(tokenizer.sos_token)+1:]\n",
    "    if gen_text != []:\n",
    "        gen_text = [gen_text[0]] + list(np.array(gen_text[1:])[np.unique(gen_text[1:], return_index=True)[1]])                \n",
    "        gen_text = set(gen_text)\n",
    "\n",
    "    FC_estimated = np.round(np.exp(pred_outputs[0,1].item()), 4)\n",
    "    if FC_estimated > 0.5: continue\n",
    "\n",
    "    print(f\"Iteration 0, Generated IPC {gen_text}, L1 prob {FC_estimated}\")\n",
    "\n",
    "    inclusions = [None, None, None, None]\n",
    "    higher_impacts = [None, None, None, None]\n",
    "    similar_refs_out = [None, None, None, None]\n",
    "    unsimilar_refs_out = [None, None, None, None]\n",
    "\n",
    "    optimised = False\n",
    "    for i in range(1,n_iter+1):\n",
    "        pred_outputs = model.predict(z)\n",
    "        z.retain_grad()\n",
    "        L1_error = (1-torch.exp(pred_outputs[0,1]))\n",
    "        L1_error.backward(retain_graph=True)\n",
    "        grad_for_update = (step_size * z.grad)\n",
    "        z_ = z - grad_for_update\n",
    "\n",
    "        z.grad.zero_()\n",
    "        dec_outputs = model.decode(z_, enc_outputs, dec_inputs=None)\n",
    "        dec_outputs = dec_outputs.argmax(-1)\n",
    "\n",
    "        pred_outputs_ = model.predict(z_)\n",
    "        FC_estimated = np.round(np.exp(pred_outputs_[0,1].item()), 4) # estimated forward citations\n",
    "        FC_estimated_inv = np.round(np.exp(pred_outputs_[0,0].item()), 4)\n",
    "\n",
    "        tokenizer = tech_dataset.tokenizers[\"class_dec\"]\n",
    "        gen_text = tokenizer.decode_batch(dec_outputs.cpu().detach().numpy())[0]\n",
    "        if tokenizer.eos_token in gen_text:\n",
    "            gen_text = gen_text[gen_text.index(tokenizer.sos_token)+1:gen_text.index(tokenizer.eos_token)]\n",
    "        else:\n",
    "            gen_text = gen_text[gen_text.index(tokenizer.sos_token)+1:]\n",
    "        if gen_text != []:\n",
    "            gen_text = [gen_text[0]] + list(np.array(gen_text[1:])[np.unique(gen_text[1:], return_index=True)[1]])                \n",
    "            gen_text = set(gen_text)\n",
    "        else: continue\n",
    "\n",
    "        print(f\"Iteration {i}, Generated IPC {gen_text}, L1 prob {FC_estimated}\")\n",
    "\n",
    "        z = z_\n",
    "\n",
    "        if FC_estimated>=L1_threshold:\n",
    "            optimised = True\n",
    "            \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:28:26.354821Z",
     "start_time": "2023-09-24T07:28:26.340945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_val[df_val[\"is_same\"]==0][df_val[df_val[\"is_same\"]==0][\"ref_FC\"]>=L1_criterion][\"ref_FC\"].astype(int).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:28:27.171911Z",
     "start_time": "2023-09-24T07:28:27.157811Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_val[df_val[\"is_same\"]==0][df_val[df_val[\"is_same\"]==0][\"ref_FC\"]>=L1_criterion][\"ref_FC_rank\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:28:27.770630Z",
     "start_time": "2023-09-24T07:28:27.767496Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ## 인용 IPC 중 생성 IPC와 동일한 게 있으면서, 입력 IPC와 생성 IPC가 다르고, 피인용수가 전체 데이터셋의 L1 기준인 9 이상인 샘플\n",
    "# df_val[df_val[\"is_same\"]==0][df_val[df_val[\"is_same\"]==0][\"ref_FC\"]>=L1_criterion]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Patent_matched 전체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:28:29.003791Z",
     "start_time": "2023-09-24T07:28:28.991733Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_val[\"ref_FC\"].astype(float).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:28:29.718088Z",
     "start_time": "2023-09-24T07:28:29.706339Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_val[\"ref_FC_rank\"].astype(float).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:28:30.835573Z",
     "start_time": "2023-09-24T07:28:30.596415Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_val[\"ref_FC\"].astype(float).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:28:31.361613Z",
     "start_time": "2023-09-24T07:28:31.228146Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_val[\"ref_FC_rank\"].astype(float).hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- IPC_original == IPC_identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:38:18.922291Z",
     "start_time": "2024-01-04T07:38:18.901879Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_val[df_val[\"is_same\"]==1][\"ref_FC\"].astype(int).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:38:19.932143Z",
     "start_time": "2024-01-04T07:38:19.911922Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_val[df_val[\"is_same\"]==1][\"ref_FC_rank\"].astype(float).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:38:29.020066Z",
     "start_time": "2024-01-04T07:38:25.776614Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_val[df_val[\"is_same\"]==1][\"ref_FC\"].astype(float).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:38:29.285527Z",
     "start_time": "2024-01-04T07:38:29.024108Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_val[df_val[\"is_same\"]==1][\"ref_FC_rank\"].astype(float).hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- IPC_original != IPC_identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:39:37.520726Z",
     "start_time": "2024-01-04T07:39:37.491080Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_val[df_val[\"is_same\"]==0][\"ref_FC\"].astype(int).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T07:39:40.211770Z",
     "start_time": "2024-01-04T07:39:40.191755Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_val[df_val[\"is_same\"]==0][\"ref_FC_rank\"].astype(float).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:28:40.237719Z",
     "start_time": "2023-09-24T07:28:40.097827Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_val[df_val[\"is_same\"]==0][\"ref_FC\"].astype(float).hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:28:40.993443Z",
     "start_time": "2023-09-24T07:28:40.854056Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_val[df_val[\"is_same\"]==0][\"ref_FC_rank\"].astype(float).hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:28:42.455169Z",
     "start_time": "2023-09-24T07:28:42.423257Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_val_adv = df_val[df_val.apply(lambda x: True if x[\"org_FC\"]<=x[\"ref_FC\"] else False, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T10:54:07.384394Z",
     "start_time": "2024-01-19T10:54:07.110856Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "L1_criterion = tech_dataset.data[\"TC5\"].quantile(0.9)\n",
    "print(\"total hit:\", len(df_val_adv))\n",
    "print(\"same:\",len(df_val_adv[df_val_adv[\"is_same\"]==1]))\n",
    "print(\"diff:\",len(df_val_adv[df_val_adv[\"is_same\"]==0]))\n",
    "print(\"over L1 criterion:\", len(df_val_adv[df_val_adv[\"is_same\"]==0][df_val_adv[df_val_adv[\"is_same\"]==0][\"ref_FC\"]>=L1_criterion]))\n",
    "print(\"ratio:\",len(df_val_adv[df_val_adv[\"is_same\"]==0][df_val_adv[df_val_adv[\"is_same\"]==0][\"ref_FC\"]>=L1_criterion]) / len(df_val_adv[df_val_adv[\"is_same\"]==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:28:45.339132Z",
     "start_time": "2023-09-24T07:28:45.309437Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 인용 IPC 중 생성 IPC와 동일한 게 있으면서, 입력 IPC와 생성 IPC가 다르고, 피인용수가 전체 데이터셋의 L1 기준인 9 이상이며, 인용 특허의 피인용 수가 입력 특허보다 많은 샘플\n",
    "df_val_adv[df_val_adv[\"is_same\"]==0][df_val_adv[df_val_adv[\"is_same\"]==0][\"ref_FC\"]>=L1_criterion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:28:47.074130Z",
     "start_time": "2023-09-24T07:28:47.067915Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T12:24:53.157012Z",
     "start_time": "2023-05-19T12:24:49.285906Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../results/validation/\"+analysis_config):\n",
    "    os.mkdir(\"../results/validation/\"+analysis_config)\n",
    "with open(\"../results/validation/\"+analysis_config+\"/dict_out.pickle\", \"wb\") as f:\n",
    "    pickle.dump(dict_out, f)\n",
    "with open(\"../results/validation/\"+analysis_config+\"/df_val.pickle\", \"wb\") as f:\n",
    "    pickle.dump(df_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:29:41.467362Z",
     "start_time": "2023-09-24T07:28:52.429225Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "whole_FC_ttest = {\"statistic\": [], \"pvalue\": []}\n",
    "whole_FCs_diff = []\n",
    "for i in tqdm(range(len(dict_out[\"patent_id\"]))):\n",
    "    org_whole_FC = tech_dataset.data.loc[whole_patent_classes[whole_patent_classes==set(dict_out[\"org_text\"][i])].index][\"TC5\"]\n",
    "    gen_whole_FC = tech_dataset.data.loc[whole_patent_classes[whole_patent_classes==set(dict_out[\"gen_text\"][i])].index][\"TC5\"]\n",
    "    \n",
    "    if len(org_whole_FC)>0 and len(gen_whole_FC)>0:\n",
    "        whole_FC_diff = gen_whole_FC.mean() - org_whole_FC.mean()\n",
    "    elif len(org_whole_FC)==0 and len(gen_whole_FC)>0:\n",
    "        whole_FC_diff = gen_whole_FC.mean()\n",
    "    elif len(org_whole_FC)>0 and len(gen_whole_FC)==0:\n",
    "        whole_FC_diff = org_whole_FC.mean()\n",
    "    else:\n",
    "        whole_FC_diff = 0.0\n",
    "    \n",
    "    ttest_res = ttest_ind(gen_whole_FC, org_whole_FC, equal_var=False)\n",
    "    \n",
    "    if set(dict_out[\"org_text\"][i]) != set(dict_out[\"gen_text\"][i]):    \n",
    "        whole_FC_ttest[\"statistic\"].append(ttest_res.statistic)\n",
    "        whole_FC_ttest[\"pvalue\"].append(ttest_res.pvalue)\n",
    "        if set(dict_out[\"org_text\"][i]) != set(dict_out[\"gen_text\"][i]):\n",
    "            whole_FCs_diff.append(whole_FC_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:32:42.709985Z",
     "start_time": "2023-09-24T07:32:42.700623Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.Series(whole_FCs_diff).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:32:43.985156Z",
     "start_time": "2023-09-24T07:32:43.971013Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.Series(whole_FC_ttest[\"statistic\"])[~pd.Series(whole_FC_ttest[\"statistic\"]).isna()].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T07:32:45.064054Z",
     "start_time": "2023-09-24T07:32:45.049637Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.Series(whole_FC_ttest[\"statistic\"]).loc[pd.Series(whole_FC_ttest[\"pvalue\"]).dropna()[pd.Series(whole_FC_ttest[\"pvalue\"]).dropna()<0.05].index].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
