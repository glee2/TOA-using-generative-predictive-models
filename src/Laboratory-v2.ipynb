{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T04:52:41.169609Z",
     "start_time": "2023-02-15T04:51:00.326000Z"
    }
   },
   "outputs": [],
   "source": [
    "root_dir = '/home2/glee/dissertation/1_tech_gen_impact/class2class/Tech_Gen/'\n",
    "master_dir = '/home2/glee/dissertation/1_tech_gen_impact/master/Tech_Gen/'\n",
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "import multiprocess as mp\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "sys.path.append(\"/share/tml_package\")\n",
    "from tml import utils\n",
    "from scipy import io\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import DataParallel as DP\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset, Dataset\n",
    "from accelerate import Accelerator\n",
    "import pytorch_model_summary\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import RandomSampler, TPESampler\n",
    "from optuna.integration import SkoptSampler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import matthews_corrcoef, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from data import TechDataset, CVSampler\n",
    "from models import Transformer, Predictor\n",
    "from train_utils import EarlyStopping, perf_eval, objective_cv, build_model, train_model, validate_model_mp\n",
    "from utils import token2class, DotDict, to_device\n",
    "\n",
    "from cleantext.sklearn import CleanTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T04:52:41.201542Z",
     "start_time": "2023-02-15T04:52:41.175879Z"
    }
   },
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    data_type=\"class+claim\",\n",
    "    data_file = \"collection_[H01L,H10][2017].csv\",\n",
    "    target_ipc=\"H01L\",\n",
    "    pred_type=\"classification\",\n",
    "    n_TC = 5,\n",
    "    use_pretrained_tokenizer=False,\n",
    "    do_train=None,\n",
    "    do_tune=None,\n",
    "    n_folds=None,\n",
    "    ipc_level=4,\n",
    "    batch_size=16,\n",
    "    max_epochs=50,\n",
    "    use_accelerator=None,\n",
    "    do_save=True,\n",
    "    n_gpus=4,\n",
    "    light=True,\n",
    "#     config_file=os.path.join(root_dir, \"configs\", \"USED_configs\", \"[CONFIGS]2023-04-11_00:39.json\"),\n",
    "    config_file=None,\n",
    "    eval_train_set=False)\n",
    "\n",
    "data_dir = os.path.join(master_dir, \"data\")\n",
    "model_dir = os.path.join(root_dir, \"models\")\n",
    "result_dir = os.path.join(root_dir, \"results\")\n",
    "config_dir = os.path.join(root_dir, \"configs\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "if args.config_file is not None:\n",
    "    config_file = args.config_file\n",
    "else:\n",
    "    config_file = os.path.join(config_dir, \"configs_light.json\") if args.light else os.path.join(config_dir, \"configs.json\")\n",
    "configs = DotDict().load(config_file)\n",
    "org_config_keys = {key: list(configs[key].keys()) for key in configs.keys()}\n",
    "\n",
    "instant_configs = {key: value for (key, value) in vars(args).items() if value is not None} # if any argument passed when main.py executed\n",
    "instant_configs_for_update = {configkey: {key: value for (key,value) in instant_configs.items() if key in org_config_keys[configkey]} for configkey in org_config_keys.keys()}\n",
    "for key, value in configs.items():\n",
    "    value.update(instant_configs_for_update[key])\n",
    "\n",
    "regex_ipc = re.compile('[A-Z](?![\\\\D])')\n",
    "if regex_ipc.match(configs.data.target_ipc) is None:\n",
    "    configs.data.update({\"target_ipc\": \"ALL\"})\n",
    "elif len(configs.data.target_ipc) > 5:\n",
    "    configs.data.update({\"target_ipc\": configs.data.target_ipc[:4]})\n",
    "\n",
    "if configs.model.model_type == \"enc-pred-dec\":\n",
    "    configs.train.loss_weights[\"recon\"] = configs.train.loss_weights[\"recon\"] / sum(configs.train.loss_weights.values())\n",
    "    configs.train.loss_weights[\"y\"] = 1 - configs.train.loss_weights[\"recon\"]\n",
    "elif configs.model.model_type == \"enc-pred\":\n",
    "    configs.train.loss_weights = {\"recon\": 0, \"y\": 1}\n",
    "elif configs.model.model_type == \"enc-dec\":\n",
    "    configs.train.loss_weights = {\"recon\": 1, \"y\": 0}\n",
    "\n",
    "if configs.train.use_accelerator:\n",
    "    accelerator = Accelerator()\n",
    "    device_ids = list(range(torch.cuda.device_count()))\n",
    "    device = accelerator.device\n",
    "    configs.train.update({\"accelerator\": accelerator})\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        device_ids = list(range(torch.cuda.device_count()))\n",
    "        gpu_usages = [np.sum([float(usage.split(\"uses\")[-1].replace(\" \",\"\").replace(\"MB\",\"\")) for usage in torch.cuda.list_gpu_processes(id).split(\"GPU memory\") if not usage==\"\" and \"no processes are running\" not in usage]) for id in device_ids]\n",
    "        device_ids = np.argsort(gpu_usages)[:configs.train.n_gpus]\n",
    "        device_ids = list(map(lambda x: torch.device('cuda', x),list(device_ids)))\n",
    "        device = device_ids[0] # main device\n",
    "        torch.cuda.set_device(device)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        device_ids = []\n",
    "\n",
    "configs.data.update({\"root_dir\": root_dir,\n",
    "                        \"data_dir\": data_dir,\n",
    "                        \"model_dir\": model_dir,\n",
    "                        \"result_dir\": result_dir,\n",
    "                        \"pretrained_enc\": configs.model.pretrained_enc,\n",
    "                        \"pretrained_dec\": configs.model.pretrained_dec,\n",
    "                        \"data_nrows\": None})\n",
    "configs.train.update({\"device\": device,\n",
    "                        \"device_ids\": device_ids,\n",
    "                        \"root_dir\": root_dir,\n",
    "                        \"data_dir\": data_dir,\n",
    "                        \"model_dir\": model_dir,\n",
    "                        \"use_keywords\": configs.data.use_keywords,\n",
    "                        \"early_stop_patience\": int(0.3*configs.train.max_epochs)})\n",
    "configs.model.update({\"device\": device,\n",
    "                        \"device_ids\": device_ids,\n",
    "                        \"n_directions\": 2 if configs.model.bidirec else 1,\n",
    "                        \"use_accelerator\": configs.train.use_accelerator})\n",
    "\n",
    "## Set hyperparameters for model training (To be TUNED)\n",
    "if configs.train.do_train and configs.train.do_tune:\n",
    "    n_layers = configs.model.n_layers = None\n",
    "    d_embedding = configs.model.d_embedding = None\n",
    "    d_enc_hidden = configs.model.d_enc_hidden = None\n",
    "    d_pred_hidden = configs.model.d_pred_hidden = None\n",
    "    d_latent = 64\n",
    "    learning_rate = configs.train.learning_rate = None\n",
    "    batch_size = configs.train.batch_size = None\n",
    "    config_name = \"HPARAM_TUNING\"\n",
    "    final_model_path = None\n",
    "else:\n",
    "    n_layers = configs.model.n_layers\n",
    "    d_embedding = configs.model.d_embedding\n",
    "    d_enc_hidden = configs.model.d_enc_hidden\n",
    "    d_pred_hidden = configs.model.d_pred_hidden\n",
    "    d_latent = configs.model.d_enc_hidden * configs.model.n_directions\n",
    "    d_latent = 64\n",
    "\n",
    "    configs.model.update({\"d_latent\": d_latent})\n",
    "\n",
    "    key_components = {\"data\": [\"target_ipc\", \"vocab_size\"], \"model\": [\"n_layers\", \"d_enc_hidden\", \"d_pred_hidden\", \"d_latent\", \"d_embedding\", \"d_ff\", \"n_head\", \"d_head\"], \"train\": [\"learning_rate\", \"batch_size\", \"max_epochs\"]}\n",
    "    config_name = \"\"\n",
    "    for key in key_components.keys():\n",
    "        for component in key_components[key]:\n",
    "            config_name += \"[\"+str(configs[key][component])+component+\"]\"\n",
    "    final_model_path = os.path.join(model_dir, f\"[Final_model]{config_name}.ckpt\")\n",
    "\n",
    "configs.train.update({\"config_name\": config_name,\n",
    "                        \"final_model_path\": final_model_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Dataset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T04:53:55.403544Z",
     "start_time": "2023-02-15T04:52:41.241617Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make dataset...\n",
      "\n",
      "\n",
      "\n",
      "Tokenizer is trained and saved\n",
      "62.5476 sec elapsed for loading patents for class [H01L]\n"
     ]
    }
   ],
   "source": [
    "tstart = time.time()\n",
    "org_config_keys_temp = copy.copy(org_config_keys[\"data\"])\n",
    "org_config_keys_temp.pop(org_config_keys_temp.index(\"use_pretrained_tokenizer\"))\n",
    "org_config_keys_temp.pop(org_config_keys_temp.index(\"data_file\"))\n",
    "dataset_config_name = \"-\".join([str(key)+\"=\"+str(value) for (key,value) in configs.data.items() if key in org_config_keys_temp])\n",
    "dataset_path = os.path.join(data_dir, \"pickled_dataset\", \"[tech_dataset]\"+dataset_config_name+\".pickle\")\n",
    "if os.path.exists(dataset_path) and args.do_save is False:\n",
    "    print(\"Load pickled dataset...\")\n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        tech_dataset = pickle.load(f)   # Load pickled dataset if dataset with same configuration already saved\n",
    "        if tech_dataset.pretrained_enc != configs.data.pretrained_enc or tech_dataset.pretrained_dec != configs.data.pretrained_dec:\n",
    "                tech_dataset.pretrained_enc = configs.data.pretrained_enc\n",
    "                tech_dataset.pretrained_dec = configs.data.pretrained_dec\n",
    "                tech_dataset.tokenizers = tech_dataset.get_tokenizers()\n",
    "        for tk in tech_dataset.tokenizers.values():\n",
    "            if \"vocab_size\" not in dir(tk):\n",
    "                tk.vocab_size = tk.get_vocab_size()\n",
    "    print(\"Pickled dataset loaded\")\n",
    "else:\n",
    "    print(\"Make dataset...\")\n",
    "    tech_dataset = TechDataset(configs.data)\n",
    "#     with open(dataset_path, \"wb\") as f:\n",
    "#         tech_dataset.rawdata = None\n",
    "#         pickle.dump(tech_dataset, f)\n",
    "tend = time.time()\n",
    "print(f\"{np.round(tend-tstart,4)} sec elapsed for loading patents for class [{configs.data.target_ipc}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>main_ipc</th>\n",
       "      <th>sub_ipc</th>\n",
       "      <th>ipcs</th>\n",
       "      <th>claims</th>\n",
       "      <th>TC5</th>\n",
       "      <th>TC5_digitized</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9853235</th>\n",
       "      <td>9853235</td>\n",
       "      <td>H01L51/52</td>\n",
       "      <td>[H01L27/32, H05B33/04]</td>\n",
       "      <td>[H01L51/52, H01L27/32, H05B33/04]</td>\n",
       "      <td>1. A display device comprising: a light emitti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9854199</th>\n",
       "      <td>9854199</td>\n",
       "      <td>H04N05/76</td>\n",
       "      <td>[G11B27/00, G11B27/024, G11B27/032, G11B27/034...</td>\n",
       "      <td>[H04N05/76, G11B27/00, G11B27/024, G11B27/032,...</td>\n",
       "      <td>1. A method for a digital video recorder, comp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9851599</th>\n",
       "      <td>9851599</td>\n",
       "      <td>G02F01/1335</td>\n",
       "      <td>[G02B05/20, G02F01/1343, G09G03/34, G09G03/36,...</td>\n",
       "      <td>[G02F01/1335, G02B05/20, G02F01/1343, G09G03/3...</td>\n",
       "      <td>1. A color display device for displaying an n-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9851864</th>\n",
       "      <td>9851864</td>\n",
       "      <td>G06F17/21</td>\n",
       "      <td>[G06F03/041, G06F03/0481, G06F03/0485, G06F03/...</td>\n",
       "      <td>[G06F17/21, G06F03/041, G06F03/0481, G06F03/04...</td>\n",
       "      <td>1. A method comprising: identifying content to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9852488</th>\n",
       "      <td>9852488</td>\n",
       "      <td>A63F09/24</td>\n",
       "      <td>[A63F13/00, G06F17/00, G06F19/00, G06Q50/34, G...</td>\n",
       "      <td>[A63F09/24, A63F13/00, G06F17/00, G06F19/00, G...</td>\n",
       "      <td>1. A computer implemented method of managing b...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9537605</th>\n",
       "      <td>9537605</td>\n",
       "      <td>H04K03/00</td>\n",
       "      <td>[H04B01/04]</td>\n",
       "      <td>[H04K03/00, H04B01/04]</td>\n",
       "      <td>1. An ultra-wideband, high-power, solid-state ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9538636</th>\n",
       "      <td>9538636</td>\n",
       "      <td>H05K01/02</td>\n",
       "      <td>[H05K01/03, H05K01/14, H05K01/18, H05K03/00, H...</td>\n",
       "      <td>[H05K01/02, H05K01/03, H05K01/14, H05K01/18, H...</td>\n",
       "      <td>1. An apparatus comprising: a substrate compri...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9536977</th>\n",
       "      <td>9536977</td>\n",
       "      <td>H01L29/66</td>\n",
       "      <td>[H01L21/332, H01L21/336, H01L21/8238, H01L29/739]</td>\n",
       "      <td>[H01L29/66, H01L21/332, H01L21/336, H01L21/823...</td>\n",
       "      <td>1. A semiconductor device comprising: a precur...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9534772</th>\n",
       "      <td>9534772</td>\n",
       "      <td>H01L33/62</td>\n",
       "      <td>[F21K99/00, F21V03/04, F21V05/04, F21V07/00, F...</td>\n",
       "      <td>[H01L33/62, F21K99/00, F21V03/04, F21V05/04, F...</td>\n",
       "      <td>1. A lighting apparatus comprising: a pluralit...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9532585</th>\n",
       "      <td>9532585</td>\n",
       "      <td>A61K35/60</td>\n",
       "      <td>[A23D09/007, A61K09/48, A61K36/534, A61K36/752]</td>\n",
       "      <td>[A61K35/60, A23D09/007, A61K09/48, A61K36/534,...</td>\n",
       "      <td>1. A capsule for delivering an odoriferous oil...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33875 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          number     main_ipc  \\\n",
       "number                          \n",
       "9853235  9853235    H01L51/52   \n",
       "9854199  9854199    H04N05/76   \n",
       "9851599  9851599  G02F01/1335   \n",
       "9851864  9851864    G06F17/21   \n",
       "9852488  9852488    A63F09/24   \n",
       "...          ...          ...   \n",
       "9537605  9537605    H04K03/00   \n",
       "9538636  9538636    H05K01/02   \n",
       "9536977  9536977    H01L29/66   \n",
       "9534772  9534772    H01L33/62   \n",
       "9532585  9532585    A61K35/60   \n",
       "\n",
       "                                                   sub_ipc  \\\n",
       "number                                                       \n",
       "9853235                             [H01L27/32, H05B33/04]   \n",
       "9854199  [G11B27/00, G11B27/024, G11B27/032, G11B27/034...   \n",
       "9851599  [G02B05/20, G02F01/1343, G09G03/34, G09G03/36,...   \n",
       "9851864  [G06F03/041, G06F03/0481, G06F03/0485, G06F03/...   \n",
       "9852488  [A63F13/00, G06F17/00, G06F19/00, G06Q50/34, G...   \n",
       "...                                                    ...   \n",
       "9537605                                        [H04B01/04]   \n",
       "9538636  [H05K01/03, H05K01/14, H05K01/18, H05K03/00, H...   \n",
       "9536977  [H01L21/332, H01L21/336, H01L21/8238, H01L29/739]   \n",
       "9534772  [F21K99/00, F21V03/04, F21V05/04, F21V07/00, F...   \n",
       "9532585    [A23D09/007, A61K09/48, A61K36/534, A61K36/752]   \n",
       "\n",
       "                                                      ipcs  \\\n",
       "number                                                       \n",
       "9853235                  [H01L51/52, H01L27/32, H05B33/04]   \n",
       "9854199  [H04N05/76, G11B27/00, G11B27/024, G11B27/032,...   \n",
       "9851599  [G02F01/1335, G02B05/20, G02F01/1343, G09G03/3...   \n",
       "9851864  [G06F17/21, G06F03/041, G06F03/0481, G06F03/04...   \n",
       "9852488  [A63F09/24, A63F13/00, G06F17/00, G06F19/00, G...   \n",
       "...                                                    ...   \n",
       "9537605                             [H04K03/00, H04B01/04]   \n",
       "9538636  [H05K01/02, H05K01/03, H05K01/14, H05K01/18, H...   \n",
       "9536977  [H01L29/66, H01L21/332, H01L21/336, H01L21/823...   \n",
       "9534772  [H01L33/62, F21K99/00, F21V03/04, F21V05/04, F...   \n",
       "9532585  [A61K35/60, A23D09/007, A61K09/48, A61K36/534,...   \n",
       "\n",
       "                                                    claims  TC5  \\\n",
       "number                                                            \n",
       "9853235  1. A display device comprising: a light emitti...    0   \n",
       "9854199  1. A method for a digital video recorder, comp...    0   \n",
       "9851599  1. A color display device for displaying an n-...    0   \n",
       "9851864  1. A method comprising: identifying content to...    0   \n",
       "9852488  1. A computer implemented method of managing b...    8   \n",
       "...                                                    ...  ...   \n",
       "9537605  1. An ultra-wideband, high-power, solid-state ...    1   \n",
       "9538636  1. An apparatus comprising: a substrate compri...    2   \n",
       "9536977  1. A semiconductor device comprising: a precur...    5   \n",
       "9534772  1. A lighting apparatus comprising: a pluralit...    6   \n",
       "9532585  1. A capsule for delivering an odoriferous oil...    0   \n",
       "\n",
       "         TC5_digitized  class  \n",
       "number                         \n",
       "9853235              0    106  \n",
       "9854199              0    109  \n",
       "9851599              0     95  \n",
       "9851864              0     99  \n",
       "9852488              1     13  \n",
       "...                ...    ...  \n",
       "9537605              0    109  \n",
       "9538636              0    110  \n",
       "9536977              1    106  \n",
       "9534772              1    106  \n",
       "9532585              0     11  \n",
       "\n",
       "[33875 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs.model.update({\"tokenizers\": tech_dataset.tokenizers,\n",
    "                    \"n_enc_seq_claim\": tech_dataset.max_seq_len_claim,\n",
    "                    \"n_dec_seq_claim\": tech_dataset.max_seq_len_claim,\n",
    "                    \"n_enc_seq_class\": tech_dataset.max_seq_len_class,\n",
    "                    \"n_dec_seq_class\": tech_dataset.max_seq_len_class,\n",
    "                    \"n_outputs\": 1 if configs.data.pred_type==\"regression\" else tech_dataset.n_outputs,\n",
    "                    \"i_padding\": tech_dataset.tokenizers[\"class_enc\"].token_to_id(\"<PAD>\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampler = CVSampler(tech_dataset, n_folds=configs.train.n_folds, test_ratio=0.1, stratify=True)\n",
    "cv_idx = sampler.get_idx_dict()\n",
    "print(f\"#Samples\\nTrain: {len(cv_idx[0]['train'])}, Validation: {len(cv_idx[0]['val'])}, Test: {len(cv_idx[0]['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3-2: Dataset construction and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Construct datasets\n",
    "train_idx = cv_idx[0]['train']\n",
    "val_idx = cv_idx[0]['val']\n",
    "test_idx = cv_idx[0]['test']\n",
    "whole_idx = np.concatenate([train_idx, val_idx])\n",
    "\n",
    "train_dataset = Subset(tech_dataset, train_idx)\n",
    "val_dataset = Subset(tech_dataset, val_idx)\n",
    "test_dataset = Subset(tech_dataset, test_idx)\n",
    "whole_dataset = Subset(tech_dataset, whole_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=configs.train.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=configs.train.batch_size if len(val_idx)>configs.train.batch_size else len(val_idx), shuffle=True, num_workers=4, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=configs.train.batch_size if len(test_idx)>configs.train.batch_size else len(test_idx), shuffle=False, num_workers=4)\n",
    "whole_loader = DataLoader(whole_dataset, batch_size=configs.train.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "## Load best model or build model\n",
    "final_model = build_model(configs.model, tokenizers=tech_dataset.tokenizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import models\n",
    "importlib.reload(models)\n",
    "from models import CLS2CLS, VCLS2CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VCLS2CLS(configs.model, tokenizers=tech_dataset.tokenizers).to(configs.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = to_device(next(iter(train_loader)), device=configs.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc_outputs, z, mu, logvar = model.encode(batch_data[\"text_inputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_outputs = model.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dec_outputs = model.decode(z, enc_outputs[\"class\"], batch_data[\"text_outputs\"], teach_force_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dec_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational 빼고 Attention 추가해서 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import models\n",
    "importlib.reload(models)\n",
    "from models import CLS2CLS, VCLS2CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLS2CLS(configs.model, tokenizers=tech_dataset.tokenizers).to(configs.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = to_device(next(iter(train_loader)), device=configs.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc_outputs, z, mu, logvar = model.encode(batch_data[\"text_inputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_outputs={}\n",
    "enc_outputs[\"claim\"], *_ = model.encoders[\"claim\"](**batch_data[\"text_inputs\"][\"claim\"])\n",
    "enc_outputs[\"class\"], hidden = model.encoders[\"class\"](batch_data[\"text_inputs\"][\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_outputs[\"class\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden.permute(1,0,2).contiguous().view(16, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.d_hidden * model.n_directions * model.n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.d_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pool(enc_outputs[\"claim\"]).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_outputs[\"class\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_outputs = model.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dec_outputs = model.decode(z, enc_outputs[\"class\"], batch_data[\"text_outputs\"], teach_force_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_anneal_function(step, func_type=\"logistic\", k=0.0025, x0=2500):\n",
    "    if func_type == 'logistic':\n",
    "        return float(1/(1+np.exp(-k*(step-x0))))\n",
    "    elif func_type == 'linear':\n",
    "        return min(1, step/x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_anneal_function(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(tech_dataset.X_class.apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_loader = DataLoader(whole_dataset, batch_size=len(whole_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholebatch = next(iter(whole_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictor parameter update 실험, z-sampling 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor(np.unique(tech_dataset.Y[whole_idx], return_counts=True)[1][::-1].copy()).to(device)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "from utils import KLDLoss\n",
    "from parallel import DataParallel\n",
    "model_params = configs.model\n",
    "loss_recon = torch.nn.CrossEntropyLoss(ignore_index=model_params['i_padding'])\n",
    "loss_y = torch.nn.MSELoss() if model_params['n_outputs']==1 else torch.nn.NLLLoss(weight=class_weights)\n",
    "loss_kld = KLDLoss()\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, final_model.module.parameters()), lr=configs.train['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in final_model.module.predictor.parameters():\n",
    "    print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(train_loader))\n",
    "batch_data = to_device(batch_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = final_model.module(batch_data[\"text_inputs\"], batch_data[\"text_outputs\"])\n",
    "preds_recon = outputs[\"dec_outputs\"].permute(0,2,1)\n",
    "trues_recon = batch_data[\"text_outputs\"]\n",
    "preds_y = outputs[\"pred_outputs\"]\n",
    "trues_y = batch_data[\"targets\"]\n",
    "preds_mu = outputs[\"mu\"]\n",
    "preds_logvar = outputs[\"logvar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_recon(preds_recon, trues_recon) + loss_kld(preds_mu, preds_logvar) + loss_y(preds_y, trues_y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_outputs, z, mu, logvar = final_model.module.encode(batch_data[\"text_inputs\"])\n",
    "pred_outputs = final_model.module.predictor(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z[:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in final_model.module.predictor.parameters():\n",
    "    print(p.sum())\n",
    "for p in final_model.module.predictor.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = final_model.module(batch_data[\"text_inputs\"], batch_data[\"text_outputs\"])\n",
    "preds_recon = outputs[\"dec_outputs\"].permute(0,2,1)\n",
    "trues_recon = batch_data[\"text_outputs\"]\n",
    "preds_y = outputs[\"pred_outputs\"]\n",
    "trues_y = batch_data[\"targets\"]\n",
    "preds_mu = outputs[\"mu\"]\n",
    "preds_logvar = outputs[\"logvar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_recon(preds_recon, trues_recon) + loss_kld(preds_mu, preds_logvar) + loss_y(preds_y, trues_y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_outputs, z, mu, logvar = final_model.module.encode(batch_data[\"text_inputs\"])\n",
    "pred_outputs = final_model.module.predictor(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z[:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in final_model.module.predictor.parameters():\n",
    "    print(p.sum())\n",
    "for p in final_model.module.predictor.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.module.freeze(module_name=\"predictor\")\n",
    "for p in final_model.module.predictor.parameters():\n",
    "    print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, final_model.module.parameters()), lr=configs.train['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = final_model.module(batch_data[\"text_inputs\"], batch_data[\"text_outputs\"])\n",
    "preds_recon = outputs[\"dec_outputs\"].permute(0,2,1)\n",
    "trues_recon = batch_data[\"text_outputs\"]\n",
    "preds_y = outputs[\"pred_outputs\"]\n",
    "trues_y = batch_data[\"targets\"]\n",
    "preds_mu = outputs[\"mu\"]\n",
    "preds_logvar = outputs[\"logvar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_recon(preds_recon, trues_recon) + loss_kld(preds_mu, preds_logvar) + loss_y(preds_y, trues_y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_outputs, z, mu, logvar = final_model.module.encode(batch_data[\"text_inputs\"])\n",
    "pred_outputs = final_model.module.predictor(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z[:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in final_model.module.predictor.parameters():\n",
    "    print(p.sum(), p.requires_grad)\n",
    "for p in final_model.module.predictor.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in final_model.module.predictor.parameters():\n",
    "    print(p.sum(), p.requires_grad)\n",
    "for p in final_model.module.predictor.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_model.module.freeze(module_name=\"predictor\", defreeze=True)\n",
    "for p in final_model.module.predictor.parameters():\n",
    "    print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, final_model.module.parameters()), lr=configs.train['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = final_model.module(batch_data[\"text_inputs\"], batch_data[\"text_outputs\"])\n",
    "preds_recon = outputs[\"dec_outputs\"].permute(0,2,1)\n",
    "trues_recon = batch_data[\"text_outputs\"]\n",
    "preds_y = outputs[\"pred_outputs\"]\n",
    "trues_y = batch_data[\"targets\"]\n",
    "preds_mu = outputs[\"mu\"]\n",
    "preds_logvar = outputs[\"logvar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_recon(preds_recon, trues_recon) + loss_kld(preds_mu, preds_logvar) + loss_y(preds_y, trues_y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_outputs, z, mu, logvar = final_model.module.encode(batch_data[\"text_inputs\"])\n",
    "pred_outputs = final_model.module.predictor(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z[:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in final_model.module.predictor.parameters():\n",
    "    print(p.sum(), p.requires_grad)\n",
    "for p in final_model.module.predictor.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3-3: Training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.eval_train_set:\n",
    "    ## Evaluation on train dataset\n",
    "    print(\"Validate model on train dataset\")\n",
    "    # trues_recon_train, preds_recon_train, trues_y_train, preds_y_train = validate_model(final_model, whole_loader, configs.model, configs.train)\n",
    "    val_res_train = validate_model_mp(final_model, whole_dataset, mp=mp, model_params=configs.model, train_params=configs.train)\n",
    "    trues_recon_train = np.concatenate([res[\"recon\"][\"true\"] for res in val_res_train.values()])\n",
    "    preds_recon_train = np.concatenate([res[\"recon\"][\"pred\"] for res in val_res_train.values()])\n",
    "    trues_y_train = np.concatenate([res[\"y\"][\"true\"] for res in val_res_train.values()])\n",
    "    preds_y_train = np.concatenate([res[\"y\"][\"pred\"] for res in val_res_train.values()])\n",
    "\n",
    "    eval_recon_train = perf_eval(\"TRAIN_SET\", trues_recon_train, preds_recon_train, configs=configs, pred_type='generative', tokenizer=final_model.module.tokenizer)\n",
    "    eval_recon_train = perf_eval(\"TRAIN_SET\", trues_recon_train, preds_recon_train, configs=configs, pred_type='generative', tokenizer=final_model.module.tokenizer)\n",
    "    eval_y_train = perf_eval(\"TRAIN_SET\", trues_y_train, preds_y_train, configs=configs, pred_type=configs.data.pred_type)\n",
    "    if configs.data.pred_type == \"classification\":\n",
    "        eval_y_train, confmat_y_train = eval_y_train\n",
    "else:\n",
    "    eval_recon_train = eval_y_train = None\n",
    "\n",
    "## Evaluation on test dataset\n",
    "print(\"Validate model on test dataset\")\n",
    "# trues_recon_test, preds_recon_test, trues_y_test, preds_y_test = validate_model(final_model, test_loader, configs.model, configs.train)\n",
    "val_res_test = validate_model_mp(final_model, test_dataset, mp=mp, batch_size=64, model_params=configs.model, train_params=configs.train)\n",
    "trues_recon_test = np.concatenate([res[\"recon\"][\"true\"] for res in val_res_test.values()])\n",
    "preds_recon_test = np.concatenate([res[\"recon\"][\"pred\"] for res in val_res_test.values()])\n",
    "trues_y_test = np.concatenate([res[\"y\"][\"true\"] for res in val_res_test.values()])\n",
    "preds_y_test = np.concatenate([res[\"y\"][\"pred\"] for res in val_res_test.values()])\n",
    "\n",
    "eval_recon_test = perf_eval(\"TEST_SET\", trues_recon_test, preds_recon_test, configs=configs,  pred_type='generative', tokenizer=final_model.module.tokenizer)\n",
    "eval_y_test = perf_eval(\"TEST_SET\", trues_y_test, preds_y_test, configs=configs, pred_type=configs.data.pred_type)\n",
    "if configs.data.pred_type == \"classification\":\n",
    "    eval_y_test, confmat_y_test = eval_y_test\n",
    "\n",
    "eval_recon_res = pd.concat([eval_recon_train, eval_recon_test], axis=0)\n",
    "eval_y_res = pd.concat([eval_y_train, eval_y_test], axis=0)\n",
    "if configs.data.pred_type == \"classification\":\n",
    "    confmat_y_res = pd.concat([confmat_y_train, confmat_y_test], axis=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
