{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T16:08:59.278713Z",
     "start_time": "2022-10-25T16:08:53.652670Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "sys.path.append(\"/share/tml_package\")\n",
    "# sys.path.append(\"/share/tml_package/tml\")\n",
    "sys.path.append(\"/share/uspto_pkg\")\n",
    "from tml import utils\n",
    "from scipy import io\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import json\n",
    "\n",
    "from data import TechDataset, CVSampler\n",
    "from models import Transformer, init_weights\n",
    "from train_utils import run_epoch, EarlyStopping, perf_eval\n",
    "from utils import token2class, DotDict\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import cleantext\n",
    "from cleantext.sklearn import CleanTransformer\n",
    "\n",
    "from collections.abc import Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_conf = Config({}).load('train_config.json')\n",
    "# model_conf = Config({}).load('model_config.json')\n",
    "configs = Config({}).load('configs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_dataset = TechDataset(data_dir=\"/home2/glee/Tech_Gen/data/\", params=train_conf)\n",
    "\n",
    "configs.model.update({'device': device,\n",
    "                  'n_enc_vocab': tech_dataset.vocab_size,\n",
    "                  'n_dec_vocab': tech_dataset.vocab_size,\n",
    "                  'n_enc_seq': tech_dataset.seq_len,\n",
    "                  'n_dec_seq': tech_dataset.seq_len,\n",
    "                  'i_pad': tech_dataset.vocab_w2i['<PAD>']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.model.update({'device': device,\n",
    "                  'n_enc_vocab': tech_dataset.vocab_size,\n",
    "                  'n_dec_vocab': tech_dataset.vocab_size,\n",
    "                  'n_enc_seq': tech_dataset.seq_len,\n",
    "                  'n_dec_seq': tech_dataset.seq_len,\n",
    "                  'i_pad': tech_dataset.vocab_w2i['<PAD>']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(tech_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = next(iter(data_loader))\n",
    "X, Y = X.to(device), Y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = tech_dataset.vocab_size\n",
    "d_hidn = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
    "    def cal_angle(position, i_hidn):\n",
    "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
    "    return sinusoid_table\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(i_pad)\n",
    "    pad_attn_mask = pad_attn_mask.unsqueeze(1).expand(batch_size, len_q, len_k)\n",
    "    return pad_attn_mask\n",
    "\n",
    "def get_attn_decoder_mask(seq):\n",
    "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n",
    "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = self.config.device\n",
    "        self.dropout = nn.Dropout(self.config.dropout)\n",
    "        self.scale = 1 / (self.config.d_head ** 0.5)\n",
    "        \n",
    "    def forward(self, Q, K, V, attn_mask):    \n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) # scores: (batch_size, n_head, n_q_seq, n_k_seq)\n",
    "        scores = scores.mul_(self.scale)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        \n",
    "        attn_prob = nn.Softmax(dim=-1)(scores) # attn_prob: (batch_size, n_head, n_q_seq, n_k_seq)\n",
    "        attn_prob = self.dropout(attn_prob)\n",
    "        \n",
    "        context = torch.matmul(attn_prob, V) # context: (batch_size, n_head, n_q_seq, d_v)\n",
    "        \n",
    "        return context, attn_prob\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = self.config.device\n",
    "        \n",
    "        self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head).to(self.device)\n",
    "        self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head).to(self.device)\n",
    "        self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head).to(self.device)\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\n",
    "        self.linear = nn.Linear(self.config.n_head*self.config.d_head, self.config.d_hidn).to(self.device)\n",
    "        self.dropout = nn.Dropout(self.config.dropout).to(self.device)\n",
    "        \n",
    "    def forward(self, X_Q, X_K, X_V, attn_mask):\n",
    "        batch_size = X_Q.size(0)\n",
    "\n",
    "        # Q: (batch_size, n_head, n_q_seq, d_head)\n",
    "        Q = self.W_Q(X_Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1, 2)\n",
    "        # K: (batch_size, n_head, n_k_seq, d_head)\n",
    "        K = self.W_K(X_K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1, 2)\n",
    "        # V: (batch_size, n_head, n_v_seq, d_head)\n",
    "        V = self.W_V(X_V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1, 2)\n",
    "\n",
    "        # attn_mask: (batch_size, n_head, n_q_seq, n_k_seq)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n",
    "        \n",
    "        # context: (batch_size, n_head, n_q_seq, d_head), attn_prob: (batch_size, n_head, n_q_seq, n_k_seq)\n",
    "        context, attn_prob = self.scaled_dot_attn(Q, K, V, attn_mask)\n",
    "        # context: (batch_size, n_q_seq, n_head*d_head)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head)\n",
    "        \n",
    "        # output: (batch_size, n_q_seq, d_hidn)\n",
    "        output = self.dropout(self.linear(context))\n",
    "        \n",
    "        return output, attn_prob\n",
    "    \n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = self.config.device\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1).to(self.device)\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1).to(self.device)\n",
    "        self.activation = F.relu\n",
    "        self.dropout = nn.Dropout(self.config.dropout).to(self.device)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = self.activation(self.conv1(inputs.transpose(1,2))) # output: (batch_size, d_ff, n_seq)\n",
    "        output = self.dropout(self.conv2(output).transpose(1,2)) # output: (batch_size, n_esq, d_hidn)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = self.config.device\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon).to(self.device)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon).to(self.device)\n",
    "        \n",
    "    def forward(self, inputs, attn_mask):\n",
    "        # attn_outputs: (batch_size, n_enc_seq, d_hidn), attn_prob: (batch_size, n_head, n_enc_seq, n_enc_seq)\n",
    "        attn_outputs, attn_prob = self.self_attn(inputs, inputs, inputs, attn_mask)\n",
    "        attn_outputs = self.layer_norm1(inputs + attn_outputs) # residual sum, layer normalization\n",
    "        \n",
    "        # ffn_outputs: (batch_size, n_enc_seq, d_hidn)\n",
    "        ffn_outputs = self.pos_ffn(attn_outputs)\n",
    "        ffn_outputs = self.layer_norm2(ffn_outputs + attn_outputs) # residual sum, layer normalization\n",
    "        \n",
    "        return ffn_outputs, attn_prob\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = self.config.device\n",
    "        \n",
    "        self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn).to(self.device)\n",
    "        sinusoid_table = torch.tensor(get_sinusoid_encoding_table(self.config.n_enc_seq + 1, self.config.d_hidn), dtype=torch.float64).to(self.device)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True).to(self.device)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = inputs.eq(self.config.i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "        \n",
    "        # outputs: (batch_size, n_enc_seq, d_hidn)\n",
    "        outputs = self.enc_emb(inputs) + self.pos_emb(positions)\n",
    "        outputs = outputs.to(dtype=torch.float32)\n",
    "        \n",
    "        # attn_mask: (batch_size, n_enc_seq, n_enc_seq)\n",
    "        attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)\n",
    "        \n",
    "        attn_probs = []\n",
    "        for layer in self.layers:\n",
    "            # outputs: (batch_size, n_enc_seq, d_hidn), attn_prob: (batch_size, n_head, n_enc_seq, n_enc_seq)\n",
    "            outputs, attn_prob = layer(outputs, attn_mask)\n",
    "            attn_probs.append(attn_prob)\n",
    "            \n",
    "        return outputs, attn_probs\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = self.config.device\n",
    "        \n",
    "        self.masked_self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon).to(self.device)\n",
    "        self.dec_enc_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon).to(self.device)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm3 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon).to(self.device)\n",
    "    \n",
    "    def forward(self, dec_inputs, enc_outputs, masked_self_attn_mask, dec_enc_attn_mask):\n",
    "        # masked_self_attn_outputs: (batch_size, n_dec_seq, d_hidn), masked_self_attn_prob: (batch_size, n_head, n_dec_seq, n_dec_seq)\n",
    "        masked_self_attn_outputs, masked_self_attn_prob = self.masked_self_attn(dec_inputs, dec_inputs, dec_inputs, masked_self_attn_mask)\n",
    "        masked_self_attn_outputs = self.layer_norm1(dec_inputs + masked_self_attn_outputs)\n",
    "        \n",
    "        # dec_enc_attn_outputs: (batch_size, n_dec_seq, d_hidn), dec_enc_attn_prob: (batch_size, n_head, n_dec_seq, n_enc_seq)\n",
    "        dec_enc_attn_outputs, dec_enc_attn_prob = self.dec_enc_attn(masked_self_attn_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_enc_attn_outputs = self.layer_norm2(masked_self_attn_outputs + dec_enc_attn_outputs)\n",
    "        \n",
    "        # ffn_outputs: (batch_size, n_dec_seq, d_hidn)\n",
    "        ffn_outputs = self.pos_ffn(dec_enc_attn_outputs)\n",
    "        ffn_outputs = self.layer_norm3(dec_enc_attn_outputs + ffn_outputs)\n",
    "        \n",
    "        return ffn_outputs, masked_self_attn_prob, dec_enc_attn_prob\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = self.config.device\n",
    "\n",
    "        self.dec_emb = nn.Embedding(self.config.n_dec_vocab, self.config.d_hidn).to(self.device)\n",
    "        sinusoid_table = torch.tensor(get_sinusoid_encoding_table(self.config.n_dec_seq + 1, self.config.d_hidn), dtype=torch.float64).to(self.device)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True).to(self.device)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "        \n",
    "        self.out = nn.Linear(self.config.d_hidn, self.config.n_dec_vocab).to(self.device)\n",
    "    \n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        positions = torch.arange(dec_inputs.size(1), device=dec_inputs.device, dtype=dec_inputs.dtype).expand(dec_inputs.size(0), dec_inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = dec_inputs.eq(self.config.i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "    \n",
    "        # dec_outputs: (batch_size, n_dec_seq, d_hidn)\n",
    "        dec_outputs = self.dec_emb(dec_inputs) + self.pos_emb(positions)\n",
    "        dec_outputs = dec_outputs.to(dtype=torch.float32)\n",
    "\n",
    "        # dec_attn_pad_mask: (batch_size, n_dec_seq, n_dec_seq)\n",
    "        dec_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs, self.config.i_pad)\n",
    "        # dec_attn_decoder_mask: (batch_size, n_dec_seq, n_dec_seq)\n",
    "        dec_attn_decoder_mask = get_attn_decoder_mask(dec_inputs)\n",
    "        # dec_self_attn_mask: (batch_size, n_dec_seq, n_dec_seq)\n",
    "        dec_self_attn_mask = torch.gt((dec_attn_pad_mask + dec_attn_decoder_mask), 0)\n",
    "        # dec_enc_attn_mask: (batch_size, n_dec_seq, n_enc_seq)\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs, self.config.i_pad)\n",
    "\n",
    "        masked_self_attn_probs, dec_enc_attn_probs = [], []\n",
    "        for layer in self.layers:\n",
    "            # dec_outputs: (batch_size, n_dec_seq, d_hidn), masked_self_attn_prob: (batch_size, n_dec_seq, n_dec_seq), dec_enc_attn_prob: (batch_size, n_dec_seq, n_enc_seq)\n",
    "            dec_outputs, masked_self_attn_prob, dec_enc_attn_prob = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            masked_self_attn_probs.append(masked_self_attn_prob)\n",
    "            dec_enc_attn_probs.append(dec_enc_attn_prob)\n",
    "            \n",
    "        # dec_outputs: (batch_size, n_dec_seq, n_dec_vocab)\n",
    "        dec_outputs = self.out(dec_outputs)\n",
    "\n",
    "        return dec_outputs, masked_self_attn_probs, dec_enc_attn_probs\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.encoder = Encoder(self.config)\n",
    "        self.decoder = Decoder(self.config)\n",
    "        \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        # enc_outputs: (batch_size, n_enc_seq, d_hidn)\n",
    "        enc_outputs, enc_self_attn_probs = self.encoder(enc_inputs)\n",
    "        # dec_outputs: (batch_size, n_dec_seq, d_hidn)\n",
    "        dec_outputs, dec_self_attn_probs, dec_enc_attn_probs = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "#         dec_outputs = nn.Softmax(dim=-1)(dec_outputs)\n",
    "        \n",
    "        return dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import init_weights, get_sinusoid_encoding_table, get_pad_mask, get_subsequent_mask\n",
    "from models import ScaledDotProductAttention, MultiHeadAttention, PoswiseFeedForwardNet, EncoderLayer, Encoder, DecoderLayer, Decoder, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Transformer(model_conf)\n",
    "init_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=train_conf.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = model_conf.i_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, (X, Y) in enumerate(data_loader):\n",
    "        src, trg = X.to(device), X.to(device)\n",
    "        y = Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_trg, *_ = model(src, trg[:,:-1]) # omit <eos> from target sequence\n",
    "        # output: (batch_size, n_dec_seq-1, n_dec_vocab)\n",
    "        output_dim = pred_trg.shape[-1]\n",
    "        pred_trg = pred_trg.contiguous().view(-1, output_dim) # output: (batch_size * (n_dec_seq-1))\n",
    "        true_trg = trg[:,1:].contiguous().view(-1) # omit <sos> from target sequence\n",
    "        \n",
    "        loss = criterion(pred_trg, true_trg)\n",
    "        loss.backward()        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, (X, Y) in enumerate(data_loader):\n",
    "        src, trg = X.to(device), X.to(device)\n",
    "        y = Y.to(device)\n",
    "        \n",
    "        pred_trg, *_ = model(src, trg[:,:-1]) # omit <eos> from target sequence\n",
    "        # output: (batch_size, n_dec_seq-1, n_dec_vocab)\n",
    "        output_dim = pred_trg.shape[-1]\n",
    "        pred_trg = pred_trg.contiguous().view(-1, output_dim) # output: (batch_size * (n_dec_seq-1))\n",
    "        true_trg = trg[:,1:].contiguous().view(-1) # omit <sos> from target sequence\n",
    "        \n",
    "        loss = criterion(pred_trg, true_trg)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "def epoch_time(start, end):\n",
    "    elapsed_time = end - start\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 5s\n",
      "\tTrain Loss: 0.711 | Train PPL: 2.036\n",
      "\tValidation Loss: 0.676 | Validation PPL: 1.965\n",
      "Epoch: 02 | Time: 1m 4s\n",
      "\tTrain Loss: 0.598 | Train PPL: 1.818\n",
      "\tValidation Loss: 0.748 | Validation PPL: 2.112\n",
      "Epoch: 03 | Time: 1m 5s\n",
      "\tTrain Loss: 0.518 | Train PPL: 1.678\n",
      "\tValidation Loss: 2.066 | Validation PPL: 7.896\n",
      "Epoch: 04 | Time: 1m 5s\n",
      "\tTrain Loss: 0.507 | Train PPL: 1.660\n",
      "\tValidation Loss: 0.868 | Validation PPL: 2.381\n",
      "Epoch: 05 | Time: 1m 3s\n",
      "\tTrain Loss: 0.497 | Train PPL: 1.644\n",
      "\tValidation Loss: 4.720 | Validation PPL: 112.145\n",
      "Epoch: 06 | Time: 1m 4s\n",
      "\tTrain Loss: 0.489 | Train PPL: 1.630\n",
      "\tValidation Loss: 1.000 | Validation PPL: 2.718\n",
      "Epoch: 07 | Time: 1m 3s\n",
      "\tTrain Loss: 0.486 | Train PPL: 1.626\n",
      "\tValidation Loss: 0.890 | Validation PPL: 2.436\n",
      "Epoch: 08 | Time: 1m 3s\n",
      "\tTrain Loss: 0.479 | Train PPL: 1.614\n",
      "\tValidation Loss: 6.335 | Validation PPL: 563.858\n",
      "Epoch: 09 | Time: 1m 3s\n",
      "\tTrain Loss: 0.480 | Train PPL: 1.617\n",
      "\tValidation Loss: 3.576 | Validation PPL: 35.725\n",
      "Epoch: 10 | Time: 1m 3s\n",
      "\tTrain Loss: 0.477 | Train PPL: 1.611\n",
      "\tValidation Loss: 5.168 | Validation PPL: 175.555\n",
      "Epoch: 11 | Time: 1m 4s\n",
      "\tTrain Loss: 0.475 | Train PPL: 1.608\n",
      "\tValidation Loss: 0.814 | Validation PPL: 2.257\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "clip = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for ep in range(100):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, data_loader, optimizer, criterion, clip)\n",
    "    valid_loss = evaluate(model, data_loader, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {ep + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [tech_dataset.vocab_w2i['<SOS>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_inputs = X[0].unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc_outputs, enc_self_attn_probs = transformer.encoder(enc_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_inputs = X[0].unsqueeze(0).to(device)\n",
    "enc_outputs, enc_self_attn_probs = transformer.encoder(enc_inputs)\n",
    "\n",
    "for i in range(10):\n",
    "    trg_tensor = torch.tensor(example).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        dec_outputs, dec_self_attn_probs, dec_enc_attn_probs = transformer.decoder(trg_tensor, enc_inputs, enc_outputs)\n",
    "        \n",
    "    pred_token = dec_outputs.argmax(2)[:,-1].item()\n",
    "    example.append(pred_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token2class(example, vocabulary=tech_dataset.vocab_i2w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
