{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T16:08:59.278713Z",
     "start_time": "2022-10-25T16:08:53.652670Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import glob\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "sys.path.append(\"/share/tml_package\")\n",
    "# sys.path.append(\"/share/tml_package/tml\")\n",
    "sys.path.append(\"/share/uspto_pkg\")\n",
    "from tml import utils\n",
    "from scipy import io\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data import TechDataset, CVSampler\n",
    "from model import Encoder_SEQ, SEQ2SEQ, Attention, AttnDecoder_SEQ\n",
    "from train_utils import run_epoch, EarlyStopping, perf_eval\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import cleantext\n",
    "from cleantext.sklearn import CleanTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T16:08:59.293835Z",
     "start_time": "2022-10-25T16:08:59.281715Z"
    }
   },
   "outputs": [],
   "source": [
    "import uspto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patent classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T06:40:54.782896Z",
     "start_time": "2022-10-26T06:40:54.768971Z"
    }
   },
   "outputs": [],
   "source": [
    "TOKEN_SOS = '<SOS>'\n",
    "TOKEN_EOS = '<EOS>'\n",
    "TOKEN_PAD = '<PAD>'\n",
    "tokens = [TOKEN_SOS, TOKEN_EOS, TOKEN_PAD]\n",
    "regex = re.compile(\"[0-9a-zA-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T06:40:56.259850Z",
     "start_time": "2022-10-26T06:40:55.552601Z"
    }
   },
   "outputs": [],
   "source": [
    "data_root = \"/home2/glee/Tech_Gen/data/\"\n",
    "rawdata = pd.read_csv(os.path.join(data_root, \"collection_final.csv\"))\n",
    "rawdata_dropna = rawdata.dropna(axis=0, subset=['main ipc', 'sub ipc'])[['number','main ipc', 'sub ipc']]\n",
    "cols_year = ['<1976']+list(np.arange(1976,2018).astype(str))\n",
    "n_TC = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T06:48:06.510676Z",
     "start_time": "2022-10-26T06:48:06.414135Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipc, num = np.unique(rawdata_dropna['main ipc'].apply(lambda x: x.split(' ')[0][:3]), return_counts=True)\n",
    "ipc_sample_size = pd.concat([pd.Series(ipc[np.argsort(num)[::-1]]), pd.Series(num[np.argsort(num)[::-1]])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T06:48:07.402390Z",
     "start_time": "2022-10-26T06:48:07.386704Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A61</td>\n",
       "      <td>50281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C07</td>\n",
       "      <td>9828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01</td>\n",
       "      <td>6801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C12</td>\n",
       "      <td>3215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A23</td>\n",
       "      <td>2866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>G01</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C08</td>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C11</td>\n",
       "      <td>467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B01</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A45</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C09</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A24</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C01</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>B32</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C02</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>B27</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>B05</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>B65</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>B29</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>D06</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1\n",
       "0   A61  50281\n",
       "1   C07   9828\n",
       "2   A01   6801\n",
       "3   C12   3215\n",
       "4   A23   2866\n",
       "5   G01    850\n",
       "6   C08    774\n",
       "7   C11    467\n",
       "8   B01    389\n",
       "9   A45    217\n",
       "10  C09    191\n",
       "11  A24    132\n",
       "12  C01    112\n",
       "13  B32    104\n",
       "14  C02     67\n",
       "15  B27     66\n",
       "16  B05     57\n",
       "17  B65     54\n",
       "18  B29     52\n",
       "19  D06     44"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipc_sample_size.iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T06:47:39.327716Z",
     "start_time": "2022-10-26T06:47:39.162788Z"
    }
   },
   "outputs": [],
   "source": [
    "ipc_sample_size.to_csv(os.path.join(data_root, \"sample_size_collection_final.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Level 1\")\n",
    "display(np.unique(rawdata_dropna['main ipc'].apply(lambda x: x.split(' ')[0][:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Level 2\")\n",
    "display(np.unique(rawdata_dropna['main ipc'].apply(lambda x: x.split(' ')[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Level 3\")\n",
    "display(np.unique(rawdata_dropna['main ipc'].apply(lambda x: x.replace(' ',''))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipc_vocab_size.iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_ipc = \"A61K\"\n",
    "target_ipc = \"G01N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ipcs = [x for x in pd.unique(rawdata_dropna['main ipc']) if target_ipc in x]\n",
    "rawdata_ipc = rawdata_dropna.loc[rawdata_dropna['main ipc'].isin(main_ipcs)]\n",
    "data = rawdata_ipc[['number']].copy(deep=True)\n",
    "data['main_ipc'] = rawdata_ipc['main ipc'].apply(lambda x: regex.findall(x)[0])\n",
    "data['sub_ipc'] = rawdata_ipc['sub ipc'].apply(lambda x: [regex.findall(xx)[0] for xx in x.split(';')])\n",
    "\n",
    "rawdata_tc = rawdata.loc[rawdata_ipc.index][['year']+cols_year]\n",
    "data['TC'+str(n_TC)] = rawdata_tc.apply(lambda x: x[np.arange(x['year']+1 if x['year']<2017 else 2017, x['year']+n_TC+1 if x['year']+n_TC<2018 else 2018).astype(str)].sum(), axis=1)\n",
    "\n",
    "data = data.set_index('number')\n",
    "# main_ipcs = [regex.findall(x)[0] for x in main_ipcs]\n",
    "main_ipcs = [target_ipc]\n",
    "sub_ipcs = list(np.unique(np.concatenate(list(data['sub_ipc'].values))))\n",
    "all_ipcs = list(np.union1d(main_ipcs, sub_ipcs))\n",
    "seq_len = data['sub_ipc'].apply(lambda x: len(x)).max() + 3\n",
    "\n",
    "vocab_w2i = {all_ipcs[i]: i for i in range(len(all_ipcs))}\n",
    "vocab_w2i.update({tokens[i]: len(all_ipcs)+i for i in range(len(tokens))})\n",
    "vocab_i2w = {i: all_ipcs[i] for i in range(len(all_ipcs))}\n",
    "vocab_i2w.update({len(all_ipcs)+i: tokens[i] for i in range(len(tokens))})\n",
    "vocab_size = len(vocab_w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = {'r': 1, 'y': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa['r'] = aa['r'] / sum(aa.values())\n",
    "aa['y'] = 1 - aa['r']\n",
    "display(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(\"[0-9a-zA-Z\\/]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['main_ipc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(rawdata_ipc['main ipc'].apply(lambda x: \"\".join(regex.findall(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_ipc['main ipc'].apply(lambda x: regex.findall(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = rawdata_ipc['sub ipc'].apply(lambda x: [\"\".join(regex.findall(xx)) for xx in x.split(';')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa.iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_ipc['main ipc'].apply(lambda x: \"\".join(regex.findall(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sub_ipc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_ipc['main ipc'].apply(lambda x: regex.findall(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ipcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_i2w.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(index=data.index)\n",
    "X_df['main'] = data['main_ipc'].apply(lambda x: vocab_w2i[x])\n",
    "X_df['sub'] = data['sub_ipc'].apply(lambda x: [vocab_w2i[xx] for xx in x])\n",
    "main_sub_combined = X_df.apply(lambda x: [x['main']]+x['sub'], axis=1)\n",
    "X_df['seq'] = main_sub_combined.apply(lambda x: np.concatenate([[vocab_w2i['<SOS>']]+x+[vocab_w2i['<EOS>']], np.zeros(seq_len-(len(x)+2))+vocab_w2i['<PAD>']]).astype(int))\n",
    "\n",
    "# xaxis = np.concatenate([np.tile([i], X_df['sub'].apply(lambda x: len(x)).values[i]) for i in range(len(X_df))])\n",
    "# X = np.zeros((len(self.data), len(self.all_ipcs))) # (#samples, #ipcs)\n",
    "# X[tuple(np.arange(len(X_df))), tuple(X_df['main'].values)] += 10\n",
    "# X[tuple(xaxis), tuple(np.concatenate(X_df['sub'].values))] += 1\n",
    "\n",
    "# X = np.zeros((len(data), seq_len, len(all_ipcs)+len(tokens)))\n",
    "# X[tuple(np.sort(np.tile(np.arange(len(X_df)), seq_len))), tuple(np.tile(np.arange(seq_len), len(X_df))), tuple(np.concatenate(X_df['seq'].values))] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ipc = \"A23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"TRANSFORM one-by-one\")\n",
    "tstart = time.time()\n",
    "tech_dataset = TechDataset(device=device, data_dir=data_root, do_transform=True, params={'target_ipc': target_ipc})\n",
    "data_loader = DataLoader(tech_dataset, batch_size=batch_size)\n",
    "tend = time.time()\n",
    "print(f\"{tend-tstart} sec Elapsed\")\n",
    "\n",
    "print(\"TRANSFORM as a whole\")\n",
    "tstart = time.time()\n",
    "tech_dataset = TechDataset(device=device, data_dir=data_root, do_transform=False, params={'target_ipc': target_ipc})\n",
    "data_loader = DataLoader(tech_dataset, batch_size=batch_size)\n",
    "tend = time.time()\n",
    "print(f\"{tend-tstart} sec Elapsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data\n",
    "importlib.reload(data)\n",
    "from data import CVSampler, TechDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tstart = time.time()\n",
    "xx, yy = next(iter(data_loader))\n",
    "tend = time.time()\n",
    "print(f\"{tend-tstart} sec Elapsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder_SEQ(embedding_dim=128, vocab_size=vocab_size, hidden_dim=32, n_layers=1, device=device, padding_idx=tech_dataset.vocab_w2i['<PAD>'])\n",
    "enc = enc.to(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = Decoder_SEQ(embedding_dim=128, vocab_size=vocab_size, hidden_dim=32, n_layers=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_input = torch.from_numpy(np.tile(vocab_w2i[TOKEN_SOS], 32)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.initHidden(len(next_input)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o,h = dec(next_input, hidden=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = SEQ2SEQ(device=device, dataset=tech_dataset, enc=enc, dec=dec, max_len=tech_dataset.seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, zz = seq2seq(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.unique([tech_dataset.vocab_i2w[i] for i in outputs[:,0,:].argmax(1).detach().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(outputs.transpose(0,1).transpose(1,2), xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues = xx.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(seq2seq.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_losses = []\n",
    "\n",
    "outputs, z = seq2seq(xx) # outputs shape: (seq_len, batch_size, vocab_size)\n",
    "preds = outputs.transpose(0,1).transpose(1,2) # preds shape: (batch_size, vocab_size, seq_len), regard seq_len as additional dimension\n",
    "trues = xx.clone()\n",
    "loss = loss_fn(preds, trues)\n",
    "batch_losses.append(loss.item())\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# if batch % 10 == 0 or batch == len(dataloader)-1:\n",
    "#     loss, current = loss.item(), batch*len(X)\n",
    "#     if batch == len(dataloader)-1:\n",
    "#         current = size\n",
    "#     print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\", end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(np.tile([0], 32)).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(np.tile(np.arange(30), (32,1))).squeeze(0).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=10, verbose=True, path=\"../models/ES_checkpoint.ckpt\")\n",
    "for ep in range(max_epochs):\n",
    "    print(f\"Epoch {ep+1}\\n\"+str(\"-\"*30))\n",
    "    train_loss = run_epoch(data_loader, seq2seq, loss_fn, mode='train', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = CVSampler(tech_dataset, n_folds=1, test_ratio=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_idx = sampler.get_idx_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"#Samples\\nTrain: {len(cv_idx[0]['train'])}, Validation: {len(cv_idx[0]['val'])}, Test: {len(cv_idx[0]['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data\n",
    "importlib.reload(data)\n",
    "from data import TechDataset, CVSampler\n",
    "\n",
    "import model\n",
    "importlib.reload(model)\n",
    "from model import Encoder_SEQ, Decoder_SEQ, SEQ2SEQ, Attention, AttnDecoder_SEQ\n",
    "\n",
    "import train_utils\n",
    "importlib.reload(train_utils)\n",
    "from train_utils import run_epoch, EarlyStopping, perf_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patent claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T16:09:02.711149Z",
     "start_time": "2022-10-25T16:09:01.541551Z"
    }
   },
   "outputs": [],
   "source": [
    "data_root = \"/home2/glee/Tech_Gen/data/\"\n",
    "rawdata = pd.read_csv(os.path.join(data_root, \"collection_final.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T16:09:02.759238Z",
     "start_time": "2022-10-25T16:09:02.714286Z"
    }
   },
   "outputs": [],
   "source": [
    "pns = rawdata['number'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:40:15.491909Z",
     "start_time": "2022-10-26T01:40:15.484693Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T06:14:09.672515Z",
     "start_time": "2022-10-26T06:14:07.197540Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 20.39it/s]\n"
     ]
    }
   ],
   "source": [
    "claims = []\n",
    "pns_with_claims = []\n",
    "for pn in tqdm(pns[:50]):\n",
    "    try:\n",
    "        p = uspto.Patent(pn)\n",
    "        claims.append(p.claims)\n",
    "        pns_with_claims.append(int(pn))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T06:14:11.334112Z",
     "start_time": "2022-10-26T06:14:11.155087Z"
    }
   },
   "outputs": [],
   "source": [
    "newdata = rawdata.set_index('number').loc[pns_with_claims].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T06:14:12.135075Z",
     "start_time": "2022-10-26T06:14:12.124284Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newdata = pd.concat([newdata, pd.Series(claims).rename('claims')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T06:15:41.830724Z",
     "start_time": "2022-10-26T06:15:41.815118Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_cleaning(text_list=None, claim_level=1, claim_separator=\"\\n\\n\\n\"):\n",
    "    if not isinstance(text_list, pd.core.series.Series): text_list = pd.Series(text_list)\n",
    "\n",
    "    basic_cleaner = CleanTransformer(\n",
    "                    lower=True, no_line_breaks=True, normalize_whitespace=True,\n",
    "                    no_punct=True, strip_lines=True,\n",
    "                    no_currency_symbols=True, replace_with_currency_symbol=\"\",\n",
    "                    no_numbers=True, replace_with_number=\"\",\n",
    "                    no_digits=True, replace_with_digit=\"\")\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Take the first claim\n",
    "    if claim_level == -1:\n",
    "        cleaned = text_list\n",
    "    else:\n",
    "        cleaned = text_list.apply(lambda x: \"\".join(x.split(claim_separator)[:claim_level]) if len(x.split(claim_separator))>=claim_level else x)\n",
    "    # Basic text cleaning\n",
    "    cleaned = basic_cleaner.transform(cleaned)\n",
    "    # Remove stopwords\n",
    "    cleaned = cleaned.apply(lambda claim: np.array([word for word in claim.split() if word not in stop_words]))\n",
    "    # Stemming\n",
    "    cleaned = cleaned.apply(lambda claim: [stemmer.stem(word) for word in claim])\n",
    "    # Remove duplicates and sorting\n",
    "    cleaned = cleaned.apply(lambda claim: list(np.array(claim)[np.sort(np.unique(claim, return_index=True)[1])]))\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T06:15:47.591149Z",
     "start_time": "2022-10-26T06:15:47.256500Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = text_cleaning(text_list=claims, claim_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T06:15:48.719376Z",
     "start_time": "2022-10-26T06:15:48.709744Z"
    }
   },
   "outputs": [],
   "source": [
    "newnew = pd.concat([newdata[['number']], pd.Series(a).rename('claim')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generate new sample from latent vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_type = 'sequence'\n",
    "n_folds = 1\n",
    "learning_rate = 5e-3\n",
    "batch_size = 1\n",
    "max_epochs = 2\n",
    "n_gpus = 1\n",
    "embedding_dim = 128\n",
    "hidden_dim = 32\n",
    "n_layers = 3\n",
    "bidirec = None\n",
    "\n",
    "data_dir = \"/home2/glee/Tech_Gen/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target_ipc = 'G01N'\n",
    "train_params = {'target_ipc': target_ipc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Load dataset...\")\n",
    "tstart = time.time()\n",
    "tech_dataset = TechDataset(device=device, data_dir=data_dir, do_transform=False, params=train_params)\n",
    "data_loader = DataLoader(tech_dataset, batch_size=batch_size)\n",
    "tend = time.time()\n",
    "print(f\"{np.round(tend-tstart,4)} sec elapsed for loading patents for class [{train_params['target_ipc']}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xs, ys = next(iter(data_loader))\n",
    "x = xs[0].unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One-directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bidirec = True\n",
    "# hidden_dim_enc = 2 if bidirec else 1\n",
    "hidden_dim_dec = hidden_dim\n",
    "n_directions = 2 if bidirec else 1\n",
    "hidden_dim_enc = hidden_dim * n_directions if bidirec else hidden_dim\n",
    "\n",
    "enc = Encoder_SEQ(embedding_dim=embedding_dim, hidden_dim=hidden_dim, vocab_size=tech_dataset.vocab_size, n_layers=n_layers, bidirec=bidirec, device=device).to(device)\n",
    "att = Attention(hidden_dim_enc, hidden_dim).to(device)\n",
    "dec = AttnDecoder_SEQ(embedding_dim=embedding_dim, vocab_size=tech_dataset.vocab_size, hidden_dim=hidden_dim, hidden_dim_enc=hidden_dim_enc, attention=att, n_layers=n_layers, device=device, max_len=tech_dataset.seq_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = SEQ2SEQ(device=device, dataset=tech_dataset, enc=enc, dec=dec, max_len=tech_dataset.seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "o, h = enc(x)\n",
    "display(o.shape)\n",
    "display(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "next_input = torch.from_numpy(np.tile([tech_dataset.vocab_w2i['<SOS>']], batch_size)).to(device)\n",
    "inputs = next_input.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embedded = dec.dropout(dec.embedding(inputs))\n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = dec.attention(h, o)\n",
    "a = a.unsqueeze(1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weighted = torch.bmm(a, o)\n",
    "weighted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gru_input = torch.cat((embedded, weighted), dim=2)\n",
    "gru_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "o, h = dec.gru(gru_input, h)\n",
    "display(o.shape)\n",
    "display(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dec.fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p = dec.fc_out(torch.cat((embedded.squeeze(1), weighted.squeeze(1), o.squeeze(1)), dim=1))\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dec(next_input, h, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embedded = enc.dropout(enc.embedding(x))\n",
    "h_init = enc.initHidden(len(x))\n",
    "o, h = enc.gru(embedded, h_init)\n",
    "# o, h = enc(x)\n",
    "\n",
    "h = h.view(n_layers, n_directions, batch_size, hidden_dim)\n",
    "\n",
    "print(f\"Output: {o.shape}\\nHidden: {h.shape}\")\n",
    "display(o[0, -1, :])\n",
    "display(h[-1].view(1, batch_size, -1))\n",
    "\n",
    "print(f\"Decoder input: {h[-1].view(batch_size, -1).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Bi-directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bidirec = True\n",
    "hidden_dim_dec = hidden_dim\n",
    "n_directions = 2 if bidirec else 1\n",
    "hidden_dim_enc = hidden_dim * n_directions if bidirec else hidden_dim\n",
    "\n",
    "enc = Encoder_SEQ(embedding_dim=embedding_dim, hidden_dim=hidden_dim, vocab_size=tech_dataset.vocab_size, n_layers=n_layers, bidirec=bidirec, device=device).to(device)\n",
    "att = Attention(hidden_dim_enc, hidden_dim).to(device)\n",
    "dec = AttnDecoder_SEQ(embedding_dim=embedding_dim, vocab_size=tech_dataset.vocab_size, hidden_dim=hidden_dim, hidden_dim_enc=hidden_dim_enc, attention=att, n_layers=n_layers, device=device, max_len=tech_dataset.seq_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedded = enc.dropout(enc.embedding(x))\n",
    "h_init = enc.initHidden(len(x))\n",
    "o, h = enc.gru(embedded, h_init)\n",
    "# o, h = enc(x)\n",
    "print(f\"Output: {o.shape}\\nHidden: {h.shape}\\n\\n\")\n",
    "\n",
    "h = h.view(n_layers, n_directions, batch_size, hidden_dim)\n",
    "\n",
    "print(\"Forward path\")\n",
    "display(o[0, -1, :hidden_dim])\n",
    "display(h[-1, 0, 0, :])\n",
    "print(\"Backward path\")\n",
    "display(o[0, 0, hidden_dim:])\n",
    "display(h[-1, -1, 0, :])\n",
    "\n",
    "print(f\"Decoder input: {h[-1].view(batch_size, -1).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hh = torch.cat([h,h], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hh_ = torch.permute(hh, (1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hh_.reshape(2,-1)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
