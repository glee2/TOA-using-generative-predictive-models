{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T15:09:57.625253Z",
     "start_time": "2022-10-24T15:09:57.611795Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "sys.path.append(\"/share/tml_package\")\n",
    "from tml import utils\n",
    "from scipy import io\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data import TechDataset, CVSampler\n",
    "from model import Encoder_SEQ, SEQ2SEQ, Attention, AttnDecoder_SEQ\n",
    "from train_utils import run_epoch, EarlyStopping, perf_eval\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import cleantext\n",
    "from cleantext.sklearn import CleanTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Patent classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TOKEN_SOS = '<SOS>'\n",
    "TOKEN_EOS = '<EOS>'\n",
    "TOKEN_PAD = '<PAD>'\n",
    "tokens = [TOKEN_SOS, TOKEN_EOS, TOKEN_PAD]\n",
    "regex = re.compile(\"[0-9a-zA-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_root = \"/home2/glee/Tech_Gen/data/\"\n",
    "rawdata = pd.read_csv(os.path.join(data_root, \"collection_final.csv\"))\n",
    "rawdata_dropna = rawdata.dropna(axis=0, subset=['main ipc', 'sub ipc'])[['number','main ipc', 'sub ipc']]\n",
    "cols_year = ['<1976']+list(np.arange(1976,2018).astype(str))\n",
    "n_TC = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipc, num = np.unique(rawdata_dropna['main ipc'].apply(lambda x: x.split(' ')[0]), return_counts=True)\n",
    "ipc_vocab_size = pd.concat([pd.Series(ipc[np.argsort(num)[::-1]]), pd.Series(num[np.argsort(num)[::-1]])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Level 1\")\n",
    "display(np.unique(rawdata_dropna['main ipc'].apply(lambda x: x.split(' ')[0][:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Level 2\")\n",
    "display(np.unique(rawdata_dropna['main ipc'].apply(lambda x: x.split(' ')[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Level 3\")\n",
    "display(np.unique(rawdata_dropna['main ipc'].apply(lambda x: x.replace(' ',''))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ipc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipc_vocab_size.iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# target_ipc = \"A61K\"\n",
    "target_ipc = \"G01N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "main_ipcs = [x for x in pd.unique(rawdata_dropna['main ipc']) if target_ipc in x]\n",
    "rawdata_ipc = rawdata_dropna.loc[rawdata_dropna['main ipc'].isin(main_ipcs)]\n",
    "data = rawdata_ipc[['number']].copy(deep=True)\n",
    "data['main_ipc'] = rawdata_ipc['main ipc'].apply(lambda x: regex.findall(x)[0])\n",
    "data['sub_ipc'] = rawdata_ipc['sub ipc'].apply(lambda x: [regex.findall(xx)[0] for xx in x.split(';')])\n",
    "\n",
    "rawdata_tc = rawdata.loc[rawdata_ipc.index][['year']+cols_year]\n",
    "data['TC'+str(n_TC)] = rawdata_tc.apply(lambda x: x[np.arange(x['year']+1 if x['year']<2017 else 2017, x['year']+n_TC+1 if x['year']+n_TC<2018 else 2018).astype(str)].sum(), axis=1)\n",
    "\n",
    "data = data.set_index('number')\n",
    "# main_ipcs = [regex.findall(x)[0] for x in main_ipcs]\n",
    "main_ipcs = [target_ipc]\n",
    "sub_ipcs = list(np.unique(np.concatenate(list(data['sub_ipc'].values))))\n",
    "all_ipcs = list(np.union1d(main_ipcs, sub_ipcs))\n",
    "seq_len = data['sub_ipc'].apply(lambda x: len(x)).max() + 3\n",
    "\n",
    "vocab_w2i = {all_ipcs[i]: i for i in range(len(all_ipcs))}\n",
    "vocab_w2i.update({tokens[i]: len(all_ipcs)+i for i in range(len(tokens))})\n",
    "vocab_i2w = {i: all_ipcs[i] for i in range(len(all_ipcs))}\n",
    "vocab_i2w.update({len(all_ipcs)+i: tokens[i] for i in range(len(tokens))})\n",
    "vocab_size = len(vocab_w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aa = {'r': 1, 'y': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aa['r'] = aa['r'] / sum(aa.values())\n",
    "aa['y'] = 1 - aa['r']\n",
    "display(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "regex = re.compile(\"[0-9a-zA-Z\\/]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data['main_ipc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.unique(rawdata_ipc['main ipc'].apply(lambda x: \"\".join(regex.findall(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rawdata_ipc['main ipc'].apply(lambda x: regex.findall(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aaa = rawdata_ipc['sub ipc'].apply(lambda x: [\"\".join(regex.findall(xx)) for xx in x.split(';')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aaa.iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rawdata_ipc['main ipc'].apply(lambda x: \"\".join(regex.findall(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data['sub_ipc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rawdata_ipc['main ipc'].apply(lambda x: regex.findall(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sub_ipcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab_i2w.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(index=data.index)\n",
    "X_df['main'] = data['main_ipc'].apply(lambda x: vocab_w2i[x])\n",
    "X_df['sub'] = data['sub_ipc'].apply(lambda x: [vocab_w2i[xx] for xx in x])\n",
    "main_sub_combined = X_df.apply(lambda x: [x['main']]+x['sub'], axis=1)\n",
    "X_df['seq'] = main_sub_combined.apply(lambda x: np.concatenate([[vocab_w2i['<SOS>']]+x+[vocab_w2i['<EOS>']], np.zeros(seq_len-(len(x)+2))+vocab_w2i['<PAD>']]).astype(int))\n",
    "\n",
    "# xaxis = np.concatenate([np.tile([i], X_df['sub'].apply(lambda x: len(x)).values[i]) for i in range(len(X_df))])\n",
    "# X = np.zeros((len(self.data), len(self.all_ipcs))) # (#samples, #ipcs)\n",
    "# X[tuple(np.arange(len(X_df))), tuple(X_df['main'].values)] += 10\n",
    "# X[tuple(xaxis), tuple(np.concatenate(X_df['sub'].values))] += 1\n",
    "\n",
    "# X = np.zeros((len(data), seq_len, len(all_ipcs)+len(tokens)))\n",
    "# X[tuple(np.sort(np.tile(np.arange(len(X_df)), seq_len))), tuple(np.tile(np.arange(seq_len), len(X_df))), tuple(np.concatenate(X_df['seq'].values))] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target_ipc = \"A23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"TRANSFORM one-by-one\")\n",
    "tstart = time.time()\n",
    "tech_dataset = TechDataset(device=device, data_dir=data_root, do_transform=True, params={'target_ipc': target_ipc})\n",
    "data_loader = DataLoader(tech_dataset, batch_size=batch_size)\n",
    "tend = time.time()\n",
    "print(f\"{tend-tstart} sec Elapsed\")\n",
    "\n",
    "print(\"TRANSFORM as a whole\")\n",
    "tstart = time.time()\n",
    "tech_dataset = TechDataset(device=device, data_dir=data_root, do_transform=False, params={'target_ipc': target_ipc})\n",
    "data_loader = DataLoader(tech_dataset, batch_size=batch_size)\n",
    "tend = time.time()\n",
    "print(f\"{tend-tstart} sec Elapsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tech_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data\n",
    "importlib.reload(data)\n",
    "from data import CVSampler, TechDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tstart = time.time()\n",
    "xx, yy = next(iter(data_loader))\n",
    "tend = time.time()\n",
    "print(f\"{tend-tstart} sec Elapsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "enc = Encoder_SEQ(embedding_dim=128, vocab_size=vocab_size, hidden_dim=32, n_layers=1, device=device, padding_idx=tech_dataset.vocab_w2i['<PAD>'])\n",
    "enc = enc.to(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "enc(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dec = Decoder_SEQ(embedding_dim=128, vocab_size=vocab_size, hidden_dim=32, n_layers=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "next_input = torch.from_numpy(np.tile(vocab_w2i[TOKEN_SOS], 32)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dec.initHidden(len(next_input)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "o,h = dec(next_input, hidden=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq2seq = SEQ2SEQ(device=device, dataset=tech_dataset, enc=enc, dec=dec, max_len=tech_dataset.seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "outputs, zz = seq2seq(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.unique([tech_dataset.vocab_i2w[i] for i in outputs[:,0,:].argmax(1).detach().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loss_fn(outputs.transpose(0,1).transpose(1,2), xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trues = xx.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(seq2seq.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_losses = []\n",
    "\n",
    "outputs, z = seq2seq(xx) # outputs shape: (seq_len, batch_size, vocab_size)\n",
    "preds = outputs.transpose(0,1).transpose(1,2) # preds shape: (batch_size, vocab_size, seq_len), regard seq_len as additional dimension\n",
    "trues = xx.clone()\n",
    "loss = loss_fn(preds, trues)\n",
    "batch_losses.append(loss.item())\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# if batch % 10 == 0 or batch == len(dataloader)-1:\n",
    "#     loss, current = loss.item(), batch*len(X)\n",
    "#     if batch == len(dataloader)-1:\n",
    "#         current = size\n",
    "#     print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\", end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.from_numpy(np.tile([0], 32)).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.from_numpy(np.tile(np.arange(30), (32,1))).squeeze(0).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=10, verbose=True, path=\"../models/ES_checkpoint.ckpt\")\n",
    "for ep in range(max_epochs):\n",
    "    print(f\"Epoch {ep+1}\\n\"+str(\"-\"*30))\n",
    "    train_loss = run_epoch(data_loader, seq2seq, loss_fn, mode='train', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sampler = CVSampler(tech_dataset, n_folds=1, test_ratio=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cv_idx = sampler.get_idx_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"#Samples\\nTrain: {len(cv_idx[0]['train'])}, Validation: {len(cv_idx[0]['val'])}, Test: {len(cv_idx[0]['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data\n",
    "importlib.reload(data)\n",
    "from data import TechDataset, CVSampler\n",
    "\n",
    "import model\n",
    "importlib.reload(model)\n",
    "from model import Encoder_SEQ, Decoder_SEQ, SEQ2SEQ, Attention, AttnDecoder_SEQ\n",
    "\n",
    "import train_utils\n",
    "importlib.reload(train_utils)\n",
    "from train_utils import run_epoch, EarlyStopping, perf_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patent claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T14:37:07.902481Z",
     "start_time": "2022-10-24T14:37:06.757127Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/USPTO-2m/train.csv\", nrows=100)\n",
    "claims = data['claims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T14:37:28.281056Z",
     "start_time": "2022-10-24T14:37:28.275340Z"
    }
   },
   "outputs": [],
   "source": [
    "claims = data['claims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T15:18:01.907282Z",
     "start_time": "2022-10-24T15:18:01.890942Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_cleaning(text_list=None):\n",
    "    if not isinstance(text_list, pd.core.series.Series): text_list = pd.Series(text_list)\n",
    "    \n",
    "    basic_cleaner = CleanTransformer(\n",
    "                    lower=True, no_line_breaks=True, normalize_whitespace=True,\n",
    "                    no_punct=True, strip_lines=True,\n",
    "                    no_currency_symbols=True, replace_with_currency_symbol=\"\",\n",
    "                    no_numbers=True, replace_with_number=\"\",\n",
    "                    no_digits=True, replace_with_digit=\"\")\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Split claims and remove claim separator\n",
    "    cleaned = text_list.apply(lambda text: [claim.replace(\"<CLAIM SEP>\", \"\") for claim in text.split(\"<CLAIM SEP>\")])\n",
    "    # Basic text cleaning\n",
    "    cleaned = cleaned.apply(basic_cleaner.transform)\n",
    "    # Remove stopwords\n",
    "    cleaned = cleaned.apply(lambda text: [np.array([word for word in claim.split() if word not in stop_words]) for claim in text])\n",
    "    # Stemming\n",
    "    cleaned = cleaned.apply(lambda text: [[stemmer.stem(word) for word in claim] for claim in text])\n",
    "    # Remove duplicates and sorting\n",
    "    cleaned = cleaned.apply(lambda text: [list(np.array(claim)[np.sort(np.unique(claim, return_index=True)[1])]) for claim in text])\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T15:16:51.208539Z",
     "start_time": "2022-10-24T15:16:49.594674Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = text_cleaning(text_list=claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T01:41:40.556704Z",
     "start_time": "2022-10-25T01:41:40.494878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [[system, compris, volum, materi, first, forma...\n",
       "1     [[garden, glove, protect, fingertip, fingernai...\n",
       "2     [[hockey, helmet, receiv, head, wearer, crown,...\n",
       "3     [[toilet, recreat, vehicl, compris, bowl, flus...\n",
       "4     [[combin, mount, surfac, water, valv, bodi, he...\n",
       "                            ...                        \n",
       "95    [[method, compris, thermal, coupl, second, tec...\n",
       "96    [[circuit, compris, compar, oper, receiv, inpu...\n",
       "97    [[heat, transfer, system, compris, hollow, sup...\n",
       "98    [[method, determin, condit, damper, remot, loc...\n",
       "99    [[method, monitor, amount, refriger, system, e...\n",
       "Name: claims, Length: 100, dtype: object"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T01:42:36.580816Z",
     "start_time": "2022-10-25T01:42:36.561514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generate new sample from latent vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_type = 'sequence'\n",
    "n_folds = 1\n",
    "learning_rate = 5e-3\n",
    "batch_size = 1\n",
    "max_epochs = 2\n",
    "n_gpus = 1\n",
    "embedding_dim = 128\n",
    "hidden_dim = 32\n",
    "n_layers = 3\n",
    "bidirec = None\n",
    "\n",
    "data_dir = \"/home2/glee/Tech_Gen/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target_ipc = 'G01N'\n",
    "train_params = {'target_ipc': target_ipc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Load dataset...\")\n",
    "tstart = time.time()\n",
    "tech_dataset = TechDataset(device=device, data_dir=data_dir, do_transform=False, params=train_params)\n",
    "data_loader = DataLoader(tech_dataset, batch_size=batch_size)\n",
    "tend = time.time()\n",
    "print(f\"{np.round(tend-tstart,4)} sec elapsed for loading patents for class [{train_params['target_ipc']}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xs, ys = next(iter(data_loader))\n",
    "x = xs[0].unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One-directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bidirec = True\n",
    "# hidden_dim_enc = 2 if bidirec else 1\n",
    "hidden_dim_dec = hidden_dim\n",
    "n_directions = 2 if bidirec else 1\n",
    "hidden_dim_enc = hidden_dim * n_directions if bidirec else hidden_dim\n",
    "\n",
    "enc = Encoder_SEQ(embedding_dim=embedding_dim, hidden_dim=hidden_dim, vocab_size=tech_dataset.vocab_size, n_layers=n_layers, bidirec=bidirec, device=device).to(device)\n",
    "att = Attention(hidden_dim_enc, hidden_dim).to(device)\n",
    "dec = AttnDecoder_SEQ(embedding_dim=embedding_dim, vocab_size=tech_dataset.vocab_size, hidden_dim=hidden_dim, hidden_dim_enc=hidden_dim_enc, attention=att, n_layers=n_layers, device=device, max_len=tech_dataset.seq_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = SEQ2SEQ(device=device, dataset=tech_dataset, enc=enc, dec=dec, max_len=tech_dataset.seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "o, h = enc(x)\n",
    "display(o.shape)\n",
    "display(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "next_input = torch.from_numpy(np.tile([tech_dataset.vocab_w2i['<SOS>']], batch_size)).to(device)\n",
    "inputs = next_input.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embedded = dec.dropout(dec.embedding(inputs))\n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = dec.attention(h, o)\n",
    "a = a.unsqueeze(1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weighted = torch.bmm(a, o)\n",
    "weighted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gru_input = torch.cat((embedded, weighted), dim=2)\n",
    "gru_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "o, h = dec.gru(gru_input, h)\n",
    "display(o.shape)\n",
    "display(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dec.fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p = dec.fc_out(torch.cat((embedded.squeeze(1), weighted.squeeze(1), o.squeeze(1)), dim=1))\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dec(next_input, h, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embedded = enc.dropout(enc.embedding(x))\n",
    "h_init = enc.initHidden(len(x))\n",
    "o, h = enc.gru(embedded, h_init)\n",
    "# o, h = enc(x)\n",
    "\n",
    "h = h.view(n_layers, n_directions, batch_size, hidden_dim)\n",
    "\n",
    "print(f\"Output: {o.shape}\\nHidden: {h.shape}\")\n",
    "display(o[0, -1, :])\n",
    "display(h[-1].view(1, batch_size, -1))\n",
    "\n",
    "print(f\"Decoder input: {h[-1].view(batch_size, -1).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Bi-directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bidirec = True\n",
    "hidden_dim_dec = hidden_dim\n",
    "n_directions = 2 if bidirec else 1\n",
    "hidden_dim_enc = hidden_dim * n_directions if bidirec else hidden_dim\n",
    "\n",
    "enc = Encoder_SEQ(embedding_dim=embedding_dim, hidden_dim=hidden_dim, vocab_size=tech_dataset.vocab_size, n_layers=n_layers, bidirec=bidirec, device=device).to(device)\n",
    "att = Attention(hidden_dim_enc, hidden_dim).to(device)\n",
    "dec = AttnDecoder_SEQ(embedding_dim=embedding_dim, vocab_size=tech_dataset.vocab_size, hidden_dim=hidden_dim, hidden_dim_enc=hidden_dim_enc, attention=att, n_layers=n_layers, device=device, max_len=tech_dataset.seq_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedded = enc.dropout(enc.embedding(x))\n",
    "h_init = enc.initHidden(len(x))\n",
    "o, h = enc.gru(embedded, h_init)\n",
    "# o, h = enc(x)\n",
    "print(f\"Output: {o.shape}\\nHidden: {h.shape}\\n\\n\")\n",
    "\n",
    "h = h.view(n_layers, n_directions, batch_size, hidden_dim)\n",
    "\n",
    "print(\"Forward path\")\n",
    "display(o[0, -1, :hidden_dim])\n",
    "display(h[-1, 0, 0, :])\n",
    "print(\"Backward path\")\n",
    "display(o[0, 0, hidden_dim:])\n",
    "display(h[-1, -1, 0, :])\n",
    "\n",
    "print(f\"Decoder input: {h[-1].view(batch_size, -1).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hh = torch.cat([h,h], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hh_ = torch.permute(hh, (1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hh_.reshape(2,-1)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
