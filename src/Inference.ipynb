{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T04:52:41.169609Z",
     "start_time": "2023-02-15T04:51:00.326000Z"
    }
   },
   "outputs": [],
   "source": [
    "root_dir = '/home2/glee/dissertation/1_tech_gen_impact/class2class/Tech_Gen/'\n",
    "master_dir = '/home2/glee/dissertation/1_tech_gen_impact/master/Tech_Gen/'\n",
    "import sys\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "import copy\n",
    "import gc\n",
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "import multiprocess as mp\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "sys.path.append(\"/share/tml_package\")\n",
    "from tml import utils\n",
    "from scipy import io\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import DataParallel as DP\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset, Dataset\n",
    "from accelerate import Accelerator\n",
    "import pytorch_model_summary\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import RandomSampler, TPESampler\n",
    "from optuna.integration import SkoptSampler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import matthews_corrcoef, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from data import TechDataset, CVSampler\n",
    "from models import Transformer, Predictor\n",
    "from train_utils import EarlyStopping, perf_eval, objective_cv, build_model, train_model, validate_model_mp\n",
    "from utils import token2class, DotDict, to_device\n",
    "\n",
    "from cleantext.sklearn import CleanTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T04:52:41.201542Z",
     "start_time": "2023-02-15T04:52:41.175879Z"
    }
   },
   "outputs": [],
   "source": [
    "analysis_date = \"2023-04-22_15:39\"\n",
    "args = argparse.Namespace(\n",
    "    data_type=\"class+claim\",\n",
    "    data_file=\"collection_[semiconductor,silicon,chip][H01L,H10][2015-2017].csv\",\n",
    "    target_ipc=\"H01L/H10\",\n",
    "    pred_type=\"classification\",\n",
    "    n_TC = 5,\n",
    "    use_pretrained_tokenizer=False,\n",
    "    do_train=None,\n",
    "    do_tune=None,\n",
    "    n_folds=None,\n",
    "    batch_size=512,\n",
    "    max_epochs=20,\n",
    "    use_accelerator=None,\n",
    "    do_save=False,\n",
    "    n_gpus=4,\n",
    "    light=True,\n",
    "    config_file=os.path.join(root_dir, \"configs\", \"USED_configs\", \"[CONFIGS]\"+analysis_date+\".json\"),\n",
    "#     config_file=None,\n",
    "    eval_train_set=False)\n",
    "\n",
    "data_dir = os.path.join(master_dir, \"data\")\n",
    "model_dir = os.path.join(root_dir, \"models\")\n",
    "result_dir = os.path.join(root_dir, \"results\")\n",
    "config_dir = os.path.join(root_dir, \"configs\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "if args.config_file is not None:\n",
    "    config_file = args.config_file\n",
    "    configs = DotDict().load(config_file)\n",
    "    org_config_keys = {key: list(configs[key].keys()) for key in configs.keys()}\n",
    "else:\n",
    "    config_file = os.path.join(config_dir, \"configs_light.json\") if args.light else os.path.join(config_dir, \"configs.json\")\n",
    "    configs = DotDict().load(config_file)\n",
    "    org_config_keys = {key: list(configs[key].keys()) for key in configs.keys()}\n",
    "\n",
    "    instant_configs = {key: value for (key, value) in vars(args).items() if value is not None} # if any argument passed when main.py executed\n",
    "    instant_configs_for_update = {configkey: {key: value for (key,value) in instant_configs.items() if key in org_config_keys[configkey]} for configkey in org_config_keys.keys()}\n",
    "    for key, value in configs.items():\n",
    "        value.update(instant_configs_for_update[key])\n",
    "\n",
    "regex_ipc = re.compile('[A-Z](?![\\\\D])')\n",
    "if regex_ipc.match(configs.data.target_ipc) is None:\n",
    "    configs.data.update({\"target_ipc\": \"ALL\"})\n",
    "elif len(configs.data.target_ipc) > 5:\n",
    "    configs.data.update({\"target_ipc\": configs.data.target_ipc[:4]})\n",
    "\n",
    "if configs.model.model_type == \"enc-pred-dec\":\n",
    "    configs.train.loss_weights[\"recon\"] = configs.train.loss_weights[\"recon\"] / sum(configs.train.loss_weights.values())\n",
    "    configs.train.loss_weights[\"y\"] = 1 - configs.train.loss_weights[\"recon\"]\n",
    "elif configs.model.model_type == \"enc-pred\":\n",
    "    configs.train.loss_weights = {\"recon\": 0, \"y\": 1}\n",
    "elif configs.model.model_type == \"enc-dec\":\n",
    "    configs.train.loss_weights = {\"recon\": 1, \"y\": 0}\n",
    "\n",
    "if configs.train.use_accelerator:\n",
    "    accelerator = Accelerator()\n",
    "    device_ids = list(range(torch.cuda.device_count()))\n",
    "    device = accelerator.device\n",
    "    configs.train.update({\"accelerator\": accelerator})\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        device_ids = list(range(torch.cuda.device_count()))\n",
    "        gpu_usages = [np.sum([float(usage.split(\"uses\")[-1].replace(\" \",\"\").replace(\"MB\",\"\")) for usage in torch.cuda.list_gpu_processes(id).split(\"GPU memory\") if not usage==\"\" and \"no processes are running\" not in usage]) for id in device_ids]\n",
    "        device_ids = np.argsort(gpu_usages)[:configs.train.n_gpus]\n",
    "        device_ids = list(map(lambda x: torch.device('cuda', x),list(device_ids)))\n",
    "        device = device_ids[0] # main device\n",
    "        torch.cuda.set_device(device)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        device_ids = []\n",
    "\n",
    "configs.data.update({\"root_dir\": root_dir,\n",
    "                        \"data_dir\": data_dir,\n",
    "                        \"model_dir\": model_dir,\n",
    "                        \"result_dir\": result_dir,\n",
    "                        \"pretrained_enc\": configs.model.pretrained_enc,\n",
    "                        \"pretrained_dec\": configs.model.pretrained_dec,\n",
    "                        \"data_nrows\": None})\n",
    "configs.train.update({\"device\": device,\n",
    "                        \"device_ids\": device_ids,\n",
    "                        \"root_dir\": root_dir,\n",
    "                        \"data_dir\": data_dir,\n",
    "                        \"model_dir\": model_dir,\n",
    "                        \"use_keywords\": configs.data.use_keywords,\n",
    "                        \"early_stop_patience\": int(0.3*configs.train.max_epochs)})\n",
    "configs.model.update({\"device\": device,\n",
    "                        \"device_ids\": device_ids,\n",
    "                        \"n_directions\": 2 if configs.model.bidirec else 1,\n",
    "                        \"use_accelerator\": configs.train.use_accelerator})\n",
    "\n",
    "## Set hyperparameters for model training (To be TUNED)\n",
    "if configs.train.do_train and configs.train.do_tune:\n",
    "    n_layers = configs.model.n_layers = None\n",
    "    d_embedding = configs.model.d_embedding = None\n",
    "    d_enc_hidden = configs.model.d_enc_hidden = None\n",
    "    d_pred_hidden = configs.model.d_pred_hidden = None\n",
    "    learning_rate = configs.train.learning_rate = None\n",
    "    batch_size = configs.train.batch_size = None\n",
    "    config_name = \"HPARAM_TUNING\"\n",
    "    final_model_path = None\n",
    "else:\n",
    "    n_layers = configs.model.n_layers\n",
    "    d_embedding = configs.model.d_embedding\n",
    "    d_enc_hidden = configs.model.d_enc_hidden\n",
    "    d_pred_hidden = configs.model.d_pred_hidden\n",
    "    d_latent = configs.model.d_enc_hidden * configs.model.n_directions\n",
    "\n",
    "    key_components = {\"data\": [\"target_ipc\", \"vocab_size\"], \"model\": [\"n_layers\", \"d_enc_hidden\", \"d_pred_hidden\", \"d_latent\", \"d_embedding\", \"d_ff\", \"n_head\", \"d_head\"], \"train\": [\"learning_rate\", \"batch_size\", \"max_epochs\"]}\n",
    "    config_name = \"\"\n",
    "    for key in key_components.keys():\n",
    "        for component in key_components[key]:\n",
    "            config_name += \"[\"+str(configs[key][component])+component+\"]\"\n",
    "    final_model_path = os.path.join(model_dir, f\"[Final_model]{config_name}.ckpt\")\n",
    "\n",
    "configs.train.update({\"config_name\": config_name,\n",
    "                        \"final_model_path\": final_model_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Dataset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T04:53:55.403544Z",
     "start_time": "2023-02-15T04:52:41.241617Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pickled dataset...\n",
      "Pickled dataset loaded\n",
      "0.7179 sec elapsed for loading patents for class [H01L]\n"
     ]
    }
   ],
   "source": [
    "tstart = time.time()\n",
    "org_config_keys_temp = copy.copy(org_config_keys[\"data\"])\n",
    "org_config_keys_temp.pop(org_config_keys_temp.index(\"data_file\"))\n",
    "org_config_keys_temp.pop(org_config_keys_temp.index(\"max_seq_len_claim\"))\n",
    "org_config_keys_temp.pop(org_config_keys_temp.index(\"max_seq_len_class\"))\n",
    "dataset_config_name = \"-\".join([str(key)+\"=\"+str(value) for (key,value) in configs.data.items() if key in org_config_keys_temp])\n",
    "dataset_path = os.path.join(data_dir, \"pickled_dataset\", \"[tech_dataset]\"+dataset_config_name+\".pickle\")\n",
    "if os.path.exists(dataset_path) and args.do_save is False:\n",
    "    print(\"Load pickled dataset...\")\n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        tech_dataset = pickle.load(f)   # Load pickled dataset if dataset with same configuration already saved\n",
    "        if tech_dataset.pretrained_enc != configs.data.pretrained_enc or tech_dataset.pretrained_dec != configs.data.pretrained_dec:\n",
    "            tech_dataset.pretrained_enc = configs.data.pretrained_enc\n",
    "            tech_dataset.pretrained_dec = configs.data.pretrained_dec\n",
    "            tech_dataset.tokenizers = tech_dataset.get_tokenizers()\n",
    "        for tk in tech_dataset.tokenizers.values():\n",
    "            if \"vocab_size\" not in dir(tk):\n",
    "                tk.vocab_size = tk.get_vocab_size()\n",
    "    print(\"Pickled dataset loaded\")\n",
    "else:\n",
    "    print(\"Make dataset...\")\n",
    "    if args.debug:\n",
    "        configs.data.update({\"data_nrows\": 1000})\n",
    "        dataset_path += \".debug\"\n",
    "    tech_dataset = TechDataset(configs.data)\n",
    "    if not args.debug:\n",
    "        rawdata_for_save = copy.deepcopy(tech_dataset.rawdata)\n",
    "        with open(dataset_path, \"wb\") as f:\n",
    "            tech_dataset.rawdata = None\n",
    "            pickle.dump(tech_dataset, f)\n",
    "        tech_dataset.rawdata = rawdata_for_save\n",
    "tend = time.time()\n",
    "print(f\"{np.round(tend-tstart,4)} sec elapsed for loading patents for class [{configs.data.target_ipc}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs.model.update({\"tokenizers\": tech_dataset.tokenizers,\n",
    "                    \"n_enc_seq_claim\": tech_dataset.max_seq_len_claim,\n",
    "                    \"n_dec_seq_claim\": tech_dataset.max_seq_len_claim,\n",
    "                    \"n_enc_seq_class\": tech_dataset.max_seq_len_class,\n",
    "                    \"n_dec_seq_class\": tech_dataset.max_seq_len_class,\n",
    "                    \"n_outputs\": 1 if configs.data.pred_type==\"regression\" else tech_dataset.n_outputs,\n",
    "                    \"i_padding\": tech_dataset.tokenizers[\"class_enc\"].pad_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home2/glee/dissertation/1_tech_gen_impact/class2class/Tech_Gen/models/[Final_model][H01Ltarget_ipc][1500vocab_size][2n_layers][16d_enc_hidden][16d_pred_hidden][128d_latent][128d_embedding][16d_ff][4n_head][32d_head][0.0005learning_rate][512batch_size][100max_epochs].ckpt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded\n"
     ]
    }
   ],
   "source": [
    "final_model = build_model(configs.model, tokenizers=tech_dataset.tokenizers)\n",
    "if os.path.exists(final_model_path):\n",
    "    best_states = torch.load(final_model_path)\n",
    "else:\n",
    "    raise Exception(\"Model need to be trained first\")\n",
    "converted_states = OrderedDict()\n",
    "for k, v in best_states.items():\n",
    "    if 'module' not in k:\n",
    "        k = 'module.'+k\n",
    "    else:\n",
    "        k = k.replace('features.module.', 'module.features.')\n",
    "    converted_states[k] = v\n",
    "final_model.load_state_dict(converted_states)\n",
    "\n",
    "del best_states\n",
    "del converted_states\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Model successfully loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy predictor\n",
    "temp_path = os.path.join(model_dir, \"temp\", \"temp.ckpt\")\n",
    "predictor = Predictor(final_model.module.config).to(final_model.module.device)\n",
    "torch.save(final_model.module.predictor.state_dict(), temp_path)\n",
    "predictor.load_state_dict(torch.load(temp_path, map_location=final_model.module.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = os.path.join(root_dir, \"results\")\n",
    "\n",
    "used_train_data = pd.read_excel(os.path.join(result_path, \"[DATASET]\"+analysis_date+\".xlsx\"), sheet_name=\"TRAIN_dataset\")\n",
    "used_test_data = pd.read_excel(os.path.join(result_path, \"[DATASET]\"+analysis_date+\".xlsx\"), sheet_name=\"TEST_dataset\")\n",
    "used_train_index = tech_dataset.data.index.get_indexer(pd.Index(used_train_data[\"number\"]))\n",
    "used_test_index = tech_dataset.data.index.get_indexer(pd.Index(used_test_data[\"number\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_train_dataset = Subset(tech_dataset, used_train_index)\n",
    "train_loader = DataLoader(used_train_dataset, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2542/2542 [01:13<00:00, 34.46it/s]\n"
     ]
    }
   ],
   "source": [
    "zs, ys, preds = [], [], []\n",
    "newzs = []\n",
    "for batch_data in tqdm(train_loader):\n",
    "    batch_data = to_device(batch_data, final_model.module.device)\n",
    "    y = batch_data[\"targets\"].cpu().detach().numpy()\n",
    "    \n",
    "    enc_outputs, z, mu, logvar = final_model.module.encode(batch_data[\"text_inputs\"])\n",
    "    pred_outputs = final_model.module.predictor(z)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    zs.append(z.cpu().detach().numpy())\n",
    "    ys.append(y)\n",
    "    preds.append(pred_outputs.argmax(1).cpu().detach().numpy())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "                                                \n",
    "zs = np.concatenate(zs)\n",
    "ys = np.concatenate(ys)\n",
    "preds = np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAIN set\n",
    "idx = 99\n",
    "\n",
    "input_class = torch.tensor(tech_dataset.tokenizers[\"class_enc\"].encode(tech_dataset.X_class[used_train_index][idx])).unsqueeze(0)\n",
    "input_claim = tech_dataset.tokenize(tech_dataset.tokenizers[\"claim_enc\"], tech_dataset.X_claim[used_train_index][idx])\n",
    "input_claim = {k: v.unsqueeze(0) for k, v in input_claim.items()}\n",
    "batch_input = {\"class\": torch.tensor(input_class), \"claim\": input_claim}\n",
    "input_inf = to_device(batch_input, final_model.module.device)\n",
    "\n",
    "output_class = torch.tensor(tech_dataset.tokenizers[\"class_dec\"].encode(tech_dataset.X_class[used_train_index][idx])).unsqueeze(0)\n",
    "batch_output = {\"text_outputs\": torch.tensor(output_class)}\n",
    "output_inf = to_device(batch_output, final_model.module.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST set\n",
    "idx = 11\n",
    "\n",
    "input_class = torch.tensor(tech_dataset.tokenizers[\"class_enc\"].encode(tech_dataset.X_class[used_test_index][idx])).unsqueeze(0)\n",
    "input_claim = tech_dataset.tokenize(tech_dataset.tokenizers[\"claim_enc\"], tech_dataset.X_claim[used_test_index][idx])\n",
    "input_claim = {k: v.unsqueeze(0) for k, v in input_claim.items()}\n",
    "batch_input = {\"class\": torch.tensor(input_class), \"claim\": input_claim}\n",
    "input_inf = to_device(batch_input, final_model.module.device)\n",
    "\n",
    "output_class = torch.tensor(tech_dataset.tokenizers[\"class_dec\"].encode(tech_dataset.X_class[used_test_index][idx])).unsqueeze(0)\n",
    "batch_output = {\"text_outputs\": torch.tensor(output_class)}\n",
    "output_inf = to_device(batch_output, final_model.module.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class:\n",
      " ['H04N9/00', 'H01L25/00', 'H01L27/00', 'H04N3/00', 'H04N5/00'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "near_mean_idx = np.argsort(np.sum(abs(zs - np.mean(zs, axis=0)), axis=1))[:2500]\n",
    "near_mean_idx_ = np.union1d(near_mean_idx, np.random.choice(np.where(ys==1)[0], 50))\n",
    "enc_outputs, z, mu, logvar = final_model.module.encode(input_inf)\n",
    "org_z = copy.deepcopy(z.view(1,-1).cpu().detach().numpy())\n",
    "pred_outputs = final_model.module.predict(z)\n",
    "org_y = copy.deepcopy(pred_outputs.argmax(1).cpu().detach().numpy())\n",
    "dec_inputs = None\n",
    "\n",
    "if visualize:\n",
    "    zs_for_tsne = np.concatenate([zs[near_mean_idx_], org_z])\n",
    "    ys_for_tsne = np.concatenate([ys[near_mean_idx_], org_y])\n",
    "    tsne = TSNE(early_exaggeration=10, learning_rate=\"auto\", n_iter=500, init=\"random\", verbose=0, metric=\"cosine\", square_distances=True)\n",
    "    z_tsne = tsne.fit_transform(zs_for_tsne)\n",
    "    plt.scatter(z_tsne[:-1,0], z_tsne[:-1,1], c=ys_for_tsne[:-1], cmap=\"bwr\")\n",
    "    plt.scatter(z_tsne[-1,0], z_tsne[-1,1], c=\"k\", marker=\"X\")\n",
    "    plt.text(z_tsne[-1,0]+0.5, z_tsne[-1,1]+0.5, \"origin\", weight=\"bold\")\n",
    "    plt.show()\n",
    "\n",
    "tokenizer = tech_dataset.tokenizers[\"class_dec\"]\n",
    "\n",
    "org_text = tokenizer.decode_batch(input_class.cpu().detach().numpy())[0]\n",
    "org_text = org_text[org_text.index(tokenizer.sos_token)+1:org_text.index(tokenizer.eos_token)]\n",
    "print(\"Original class:\\n\",org_text,\"\\n\")\n",
    "\n",
    "dec_outputs = final_model.module.decode(z, enc_outputs, dec_inputs=None)\n",
    "dec_outputs = dec_outputs.argmax(-1)\n",
    "\n",
    "gen_text = tokenizer.decode_batch(dec_outputs.cpu().detach().numpy())[0]\n",
    "gen_text = gen_text[gen_text.index(tokenizer.sos_token)+1:gen_text.index(tokenizer.eos_token)]\n",
    "print(f\"Generated class:\\n\", gen_text,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Estimated prob. for L1 forward citations (Iter 0): 0.4325 (0.5675)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 0):\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 1\n",
      "Estimated prob. for L1 forward citations (Iter 1): 0.5071 (0.4929)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 1):\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 2\n",
      "Estimated prob. for L1 forward citations (Iter 2): 0.5283 (0.4717)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 2):\n",
      " ['H04N3/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 3\n",
      "Estimated prob. for L1 forward citations (Iter 3): 0.8297 (0.1703)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 3):\n",
      " ['H04N3/00', 'G06T3/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 4\n",
      "Estimated prob. for L1 forward citations (Iter 4): 0.9035 (0.0965)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 4):\n",
      " ['H04N3/00', 'G06T3/00', 'H01L27/00', 'H04N3/00', 'H04N5/00'] \n",
      "\n",
      "Iteration 5\n",
      "Estimated prob. for L1 forward citations (Iter 5): 0.9236 (0.0764)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 5):\n",
      " ['H04N3/00', 'G06T3/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 6\n",
      "Estimated prob. for L1 forward citations (Iter 6): 0.9352 (0.0648)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 6):\n",
      " ['H04N3/00', 'G06T3/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 7\n",
      "Estimated prob. for L1 forward citations (Iter 7): 0.9439 (0.0561)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 7):\n",
      " ['H04N3/00', 'G06T3/00', 'H01L27/00', 'H04N3/00', 'H04N5/00'] \n",
      "\n",
      "Iteration 8\n",
      "Estimated prob. for L1 forward citations (Iter 8): 0.9504 (0.0496)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 8):\n",
      " ['H04N3/00', 'G06T3/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 9\n",
      "Estimated prob. for L1 forward citations (Iter 9): 0.9558 (0.0442)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 9):\n",
      " ['H04N3/00', 'G06T3/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 10\n",
      "Estimated prob. for L1 forward citations (Iter 10): 0.9598 (0.0402)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 10):\n",
      " ['H04N3/00', 'G06T3/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 11\n",
      "Estimated prob. for L1 forward citations (Iter 11): 0.9632 (0.0368)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 11):\n",
      " ['H04N3/00', 'G06T3/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 12\n",
      "Estimated prob. for L1 forward citations (Iter 12): 0.966 (0.034)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 12):\n",
      " ['H04N3/00', 'G06T3/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 13\n",
      "Estimated prob. for L1 forward citations (Iter 13): 0.9683 (0.0317)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 13):\n",
      " ['H04N3/00', 'G06T3/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 14\n",
      "Estimated prob. for L1 forward citations (Iter 14): 0.9705 (0.0295)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 14):\n",
      " ['H04N3/00', 'G06T3/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 15\n",
      "Estimated prob. for L1 forward citations (Iter 15): 0.9722 (0.0278)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 15):\n",
      " ['H04N3/00', 'G06T3/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 16\n",
      "Estimated prob. for L1 forward citations (Iter 16): 0.9738 (0.0262)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 16):\n",
      " ['H04N3/00', 'G06T3/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 17\n",
      "Estimated prob. for L1 forward citations (Iter 17): 0.9752 (0.0248)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 17):\n",
      " ['H04N3/00', 'G06T3/00', 'H04N3/00', 'H03M1/00', 'H04N5/00'] \n",
      "\n",
      "Iteration 18\n",
      "Estimated prob. for L1 forward citations (Iter 18): 0.9765 (0.0235)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 18):\n",
      " ['H04N3/00', 'G06T3/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Iteration 19\n",
      "Estimated prob. for L1 forward citations (Iter 19): 0.9777 (0.0223)\n",
      "Original class:\n",
      " ['H04N9/00', 'H01L27/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n",
      "Generated class (Iter 19):\n",
      " ['H04N3/00', 'G06T3/00', 'H04N3/00', 'H04N5/00', 'H04N9/00'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# n_iter = 20\n",
    "# step_size = 20\n",
    "\n",
    "# for i in range(n_iter):\n",
    "#     print(f\"Iteration {i}\")\n",
    "#     pred_outputs = final_model.module.predict(z)\n",
    "#     z.retain_grad()\n",
    "#     FC_estimated = pred_outputs[0,1] # estimated forward citations\n",
    "#     FC_estimated_inv = pred_outputs[0,0].item()\n",
    "#     if i % 1 == 0:\n",
    "#         print(f\"Estimated prob. for L1 forward citations (Iter {i}): {np.round(np.exp(FC_estimated.item()), 4)} ({np.round(np.exp(FC_estimated_inv),4)})\")\n",
    "#     FC_estimated.backward(retain_graph=True)\n",
    "        \n",
    "#     grad_for_update = (step_size * z.grad)\n",
    "# #     if i % 1 == 0:\n",
    "# #         print(f\"sum of gradient (Iter {i}): {grad_for_update.sum()}\")\n",
    "#     z_ = z + grad_for_update\n",
    "    \n",
    "#     if visualize:\n",
    "#         curr_z = copy.deepcopy(z_.view(1,-1).cpu().detach().numpy())\n",
    "#         curr_y = copy.deepcopy(pred_outputs.argmax(1).cpu().detach().numpy())\n",
    "\n",
    "#         zs_for_tsne = np.concatenate([zs[near_mean_idx_], org_z, curr_z])\n",
    "#         ys_for_tsne = np.concatenate([ys[near_mean_idx_], org_y, curr_y])\n",
    "\n",
    "#         z_tsne = tsne.fit_transform(zs_for_tsne)\n",
    "#         plt.scatter(z_tsne[:-1,0], z_tsne[:-1,1], c=ys_for_tsne[:-1], cmap=\"bwr\")\n",
    "#         plt.scatter(z_tsne[-2,0], z_tsne[-2,1], c=\"k\", marker=\"X\")\n",
    "#         plt.text(z_tsne[-2,0]+0.5, z_tsne[-2,1]+0.5, \"origin\", weight=\"bold\")\n",
    "#         plt.scatter(z_tsne[-1,0], z_tsne[-1,1], c=\"c\", marker=\"x\")\n",
    "#         plt.text(z_tsne[-1,0]-4, z_tsne[-1,1]-2.5, f\"Iter_{i}\", c=\"c\")\n",
    "#         plt.show()\n",
    "    \n",
    "#     z.grad.zero_()\n",
    "#     dec_outputs = final_model.module.decode(z_, enc_outputs, dec_inputs=None)\n",
    "#     dec_outputs = dec_outputs.argmax(-1)\n",
    "        \n",
    "#     tokenizer = tech_dataset.tokenizers[\"class_dec\"]\n",
    "#     gen_text = tokenizer.decode_batch(dec_outputs.cpu().detach().numpy())[0]\n",
    "#     gen_text = gen_text[gen_text.index(tokenizer.sos_token)+1:gen_text.index(tokenizer.eos_token)]\n",
    "#     print(\"Original class:\\n\",org_text,\"\\n\")\n",
    "\n",
    "#     if i % 1 == 0:\n",
    "#         print(f\"Generated class (Iter {i}):\\n\", gen_text,\"\\n\")\n",
    "    \n",
    "#     z = z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< Iteration 0 >>\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "\n",
      "<< Iteration 1 >>\n",
      "Estimated prob. for L1 (L2): 0.356 (0.644)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "\n",
      "<< Iteration 2 >>\n",
      "Estimated prob. for L1 (L2): 0.6018 (0.3982)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'G11C11', 'H01L27', 'H01L29'] \n",
      "\n",
      "<< Iteration 3 >>\n",
      "Estimated prob. for L1 (L2): 0.7649 (0.2351)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'G01R33', 'H01L27', 'H01L29'] \n",
      "\n",
      "<< Iteration 4 >>\n",
      "Estimated prob. for L1 (L2): 0.8727 (0.1273)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'G01R33', 'H01L27', 'H01L29'] \n",
      "\n",
      "<< Iteration 5 >>\n",
      "Estimated prob. for L1 (L2): 0.9007 (0.0993)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'G01R33', 'H01L27', 'H01L29'] \n",
      "\n",
      "<< Iteration 6 >>\n",
      "Estimated prob. for L1 (L2): 0.9163 (0.0837)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'G01R33', 'H01L27', 'H01L29', 'H01L43'] \n",
      "\n",
      "<< Iteration 7 >>\n",
      "Estimated prob. for L1 (L2): 0.9287 (0.0713)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'G01R33', 'H01L27', 'H01L29'] \n",
      "\n",
      "<< Iteration 8 >>\n",
      "Estimated prob. for L1 (L2): 0.9379 (0.0621)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'G01R33', 'H01L27', 'H01L29'] \n",
      "\n",
      "<< Iteration 9 >>\n",
      "Estimated prob. for L1 (L2): 0.9451 (0.0549)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'G01R33', 'H01L27', 'H01L29'] \n",
      "\n",
      "<< Iteration 10 >>\n",
      "Estimated prob. for L1 (L2): 0.95 (0.05)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'G01R33', 'H01L27', 'H01L29', 'H01L43'] \n",
      "\n",
      "<< Iteration 11 >>\n",
      "Estimated prob. for L1 (L2): 0.9545 (0.0455)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'G01R33', 'H01L27', 'H01L43'] \n",
      "\n",
      "<< Iteration 12 >>\n",
      "Estimated prob. for L1 (L2): 0.958 (0.042)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'G01R33', 'H01L27', 'H01L29'] \n",
      "\n",
      "<< Iteration 13 >>\n",
      "Estimated prob. for L1 (L2): 0.9612 (0.0388)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'G01R33', 'H01L27', 'H01L29'] \n",
      "\n",
      "<< Iteration 14 >>\n",
      "Estimated prob. for L1 (L2): 0.9637 (0.0363)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'G01R33', 'H01L27', 'H01L29'] \n",
      "\n",
      "<< Iteration 15 >>\n",
      "Estimated prob. for L1 (L2): 0.9662 (0.0338)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H01L29', 'G01R33', 'H01L27', 'H01L43'] \n",
      "\n",
      "<< Iteration 16 >>\n",
      "Estimated prob. for L1 (L2): 0.9679 (0.0321)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H04N03', 'G01R33', 'H01L27', 'H01L29', 'H01L43'] \n",
      "\n",
      "<< Iteration 17 >>\n",
      "Estimated prob. for L1 (L2): 0.9697 (0.0303)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H04N03', 'G01R33', 'H01L27', 'H01L43'] \n",
      "\n",
      "<< Iteration 18 >>\n",
      "Estimated prob. for L1 (L2): 0.9715 (0.0285)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H04N03', 'G01R33', 'H01L27', 'H01L29', 'H01L43'] \n",
      "\n",
      "<< Iteration 19 >>\n",
      "Estimated prob. for L1 (L2): 0.9729 (0.0271)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H04N03', 'G01R33', 'H01L27', 'H01L29'] \n",
      "\n",
      "<< Iteration 20 >>\n",
      "Estimated prob. for L1 (L2): 0.974 (0.026)\n",
      "Original class:\n",
      " ['H01L29', 'H01L23', 'H01L27', 'H01L29'] \n",
      "Generated class:\n",
      " ['H04N03', 'G01R33', 'H01L27', 'H01L43'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 63\n",
    "    \n",
    "input_class = torch.tensor(tech_dataset.tokenizers[\"class_enc\"].encode(tech_dataset.X_class[used_test_index][idx])).unsqueeze(0)\n",
    "input_claim = tech_dataset.tokenize(tech_dataset.tokenizers[\"claim_enc\"], tech_dataset.X_claim[used_test_index][idx])\n",
    "input_claim = {k: v.unsqueeze(0) for k, v in input_claim.items()}\n",
    "batch_input = {\"class\": torch.tensor(input_class), \"claim\": input_claim}\n",
    "input_inf = to_device(batch_input, final_model.module.device)\n",
    "\n",
    "output_class = torch.tensor(tech_dataset.tokenizers[\"class_dec\"].encode(tech_dataset.X_class[used_test_index][idx])).unsqueeze(0)\n",
    "batch_output = {\"text_outputs\": torch.tensor(output_class)}\n",
    "output_inf = to_device(batch_output, final_model.module.device)\n",
    "\n",
    "near_mean_idx = np.argsort(np.sum(abs(zs - np.mean(zs, axis=0)), axis=1))[:2500]\n",
    "near_mean_idx_ = np.union1d(near_mean_idx, np.random.choice(np.where(ys==1)[0], 50))\n",
    "enc_outputs, z, mu, logvar = final_model.module.encode(input_inf)\n",
    "org_z = copy.deepcopy(z.view(1,-1).cpu().detach().numpy())\n",
    "pred_outputs = final_model.module.predict(z)\n",
    "org_y = copy.deepcopy(pred_outputs.argmax(1).cpu().detach().numpy())\n",
    "dec_inputs = None\n",
    "\n",
    "if visualize:\n",
    "    zs_for_tsne = np.concatenate([zs[near_mean_idx_], org_z])\n",
    "    ys_for_tsne = np.concatenate([ys[near_mean_idx_], org_y])\n",
    "    tsne = TSNE(early_exaggeration=10, learning_rate=\"auto\", n_iter=500, init=\"random\", verbose=0, metric=\"cosine\", square_distances=True)\n",
    "    z_tsne = tsne.fit_transform(zs_for_tsne)\n",
    "    plt.scatter(z_tsne[:-1,0], z_tsne[:-1,1], c=ys_for_tsne[:-1], cmap=\"bwr\")\n",
    "    plt.scatter(z_tsne[-1,0], z_tsne[-1,1], c=\"k\", marker=\"X\")\n",
    "    plt.text(z_tsne[-1,0]+0.5, z_tsne[-1,1]+0.5, \"origin\", weight=\"bold\")\n",
    "    plt.show()\n",
    "\n",
    "tokenizer = tech_dataset.tokenizers[\"class_dec\"]\n",
    "\n",
    "print(f\"<< Iteration {0} >>\")\n",
    "\n",
    "org_text = tokenizer.decode_batch(input_class.cpu().detach().numpy())[0]\n",
    "org_text = org_text[org_text.index(tokenizer.sos_token)+1:org_text.index(tokenizer.eos_token)]\n",
    "org_text = [t.split(\"/\")[0] for t in org_text]\n",
    "org_text = [t[:4]+\"0\"+t[4:] if (len(t[4:])<2) else t for t in org_text]\n",
    "\n",
    "dec_outputs = final_model.module.decode(z, enc_outputs, dec_inputs=None)\n",
    "dec_outputs = dec_outputs.argmax(-1)\n",
    "\n",
    "gen_text = tokenizer.decode_batch(dec_outputs.cpu().detach().numpy())[0]\n",
    "gen_text = gen_text[gen_text.index(tokenizer.sos_token)+1:gen_text.index(tokenizer.eos_token)]\n",
    "gen_text = [gen_text[0]] + list(np.array(gen_text[1:])[np.unique(gen_text[1:], return_index=True)[1]])\n",
    "gen_text = [t.split(\"/\")[0] for t in gen_text]\n",
    "gen_text = [t[:4]+\"0\"+t[4:] if (len(t[4:])<2) else t for t in gen_text]\n",
    "print(\"Original class:\\n\",org_text,\"\\nGenerated class:\\n\", gen_text,\"\\n\")\n",
    "\n",
    "n_iter = 20\n",
    "step_size = 20\n",
    "\n",
    "for i in range(n_iter):\n",
    "    print(f\"<< Iteration {i+1} >>\")\n",
    "    pred_outputs = final_model.module.predict(z)\n",
    "    z.retain_grad()\n",
    "    FC_estimated = pred_outputs[0,1] # estimated forward citations\n",
    "    FC_estimated_inv = pred_outputs[0,0].item()\n",
    "    if i % 1 == 0:\n",
    "        print(f\"Estimated prob. for L1 (L2): {np.round(np.exp(FC_estimated.item()), 4)} ({np.round(np.exp(FC_estimated_inv),4)})\")\n",
    "    FC_estimated.backward(retain_graph=True)\n",
    "        \n",
    "    grad_for_update = (step_size * z.grad)\n",
    "#     if i % 1 == 0:\n",
    "#         print(f\"sum of gradient (Iter {i}): {grad_for_update.sum()}\")\n",
    "    z_ = z + grad_for_update\n",
    "    \n",
    "    if visualize:\n",
    "        curr_z = copy.deepcopy(z_.view(1,-1).cpu().detach().numpy())\n",
    "        curr_y = copy.deepcopy(pred_outputs.argmax(1).cpu().detach().numpy())\n",
    "\n",
    "        zs_for_tsne = np.concatenate([zs[near_mean_idx_], org_z, curr_z])\n",
    "        ys_for_tsne = np.concatenate([ys[near_mean_idx_], org_y, curr_y])\n",
    "\n",
    "        z_tsne = tsne.fit_transform(zs_for_tsne)\n",
    "        plt.scatter(z_tsne[:-1,0], z_tsne[:-1,1], c=ys_for_tsne[:-1], cmap=\"bwr\")\n",
    "        plt.scatter(z_tsne[-2,0], z_tsne[-2,1], c=\"k\", marker=\"X\")\n",
    "        plt.text(z_tsne[-2,0]+0.5, z_tsne[-2,1]+0.5, \"origin\", weight=\"bold\")\n",
    "        plt.scatter(z_tsne[-1,0], z_tsne[-1,1], c=\"c\", marker=\"x\")\n",
    "        plt.text(z_tsne[-1,0]-4, z_tsne[-1,1]-2.5, f\"Iter_{i}\", c=\"c\")\n",
    "        plt.show()\n",
    "    \n",
    "    z.grad.zero_()\n",
    "    dec_outputs = final_model.module.decode(z_, enc_outputs, dec_inputs=None)\n",
    "    dec_outputs = dec_outputs.argmax(-1)\n",
    "        \n",
    "    tokenizer = tech_dataset.tokenizers[\"class_dec\"]\n",
    "    gen_text = tokenizer.decode_batch(dec_outputs.cpu().detach().numpy())[0]\n",
    "    gen_text = gen_text[gen_text.index(tokenizer.sos_token)+1:gen_text.index(tokenizer.eos_token)]\n",
    "    gen_text = [gen_text[0]] + list(np.array(gen_text[1:])[np.unique(gen_text[1:], return_index=True)[1]])\n",
    "    gen_text = [t.split(\"/\")[0] for t in gen_text]\n",
    "    gen_text = [t[:4]+\"0\"+t[4:] if (len(t[4:])<2) else t for t in gen_text]\n",
    "    print(\"Original class:\\n\",org_text,\"\\nGenerated class:\\n\", gen_text,\"\\n\")\n",
    "    \n",
    "    z = z_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
